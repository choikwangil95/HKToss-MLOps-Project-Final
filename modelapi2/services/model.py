import numpy as np
import re
from konlpy.tag import Okt
import numpy as np
from sqlalchemy.orm import Session
from collections import Counter

from schemas.model import SimilarNewsItem
import asyncio
from fastapi.responses import StreamingResponse
import json
import redis
from concurrent.futures import ThreadPoolExecutor
import requests
from db.postgresql import get_db
from models.custom import (
    NewsModel_v2,
    NewsModel_v2_Metadata,
    NewsModel_v2_External,
    NewsModel_v2_Topic,
)
import shap
import json

import ast
import pandas as pd
import time
from datetime import datetime


async def get_news_embeddings(article_list, request):
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ONNX ëª¨ë¸ì´ ë°°ì¹˜ ì…ë ¥ì„ ì§€ì›í•  ê²½ìš°, í•œ ë²ˆì— ì¶”ë¡ í•©ë‹ˆë‹¤.
    """
    tokenizer = request.app.state.tokenizer_embedding
    session = request.app.state.session_embedding

    # 1. í† í°í™”
    encoded = [tokenizer.encode(x) for x in article_list]
    input_ids = [e.ids for e in encoded]
    attention_mask = [[1] * len(ids) for ids in input_ids]

    # 2. íŒ¨ë”© (ìµœëŒ€ ê¸¸ì´ ê¸°ì¤€)
    max_len = max(len(ids) for ids in input_ids)
    input_ids_padded = [ids + [0] * (max_len - len(ids)) for ids in input_ids]
    attention_mask_padded = [
        mask + [0] * (max_len - len(mask)) for mask in attention_mask
    ]

    # 3. numpy ë°°ì—´ë¡œ ë³€í™˜
    input_ids_np = np.array(input_ids_padded, dtype=np.int64)
    attention_mask_np = np.array(attention_mask_padded, dtype=np.int64)

    # 4. ONNX ì¶”ë¡ 
    outputs = session.run(
        ["sentence_embedding"],
        {"input_ids": input_ids_np, "attention_mask": attention_mask_np},
    )[
        0
    ]  # shape: (batch_size, hidden_dim)

    # 5. ë°˜í™˜ (List[List[float]])
    return outputs.tolist()


def safe_parse_list(val):
    if isinstance(val, str):
        try:
            return ast.literal_eval(val)
        except Exception:
            return []
    return val if isinstance(val, list) else []


async def get_news_similar_list(payload, request):
    """
    ìœ ì‚¬ ë‰´ìŠ¤ top_k
    """
    article = payload.article
    top_k = payload.top_k

    vectordb = request.app.state.vectordb

    # ê²€ìƒ‰
    results = vectordb.similarity_search_with_score(article, k=100)

    news_similar_list = []
    seen_titles = set()

    for doc, score in results:
        similarity = round(1 - float(score), 2)

        if similarity > 0.9:
            continue  # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ ì œì™¸ (0.9 ì´ìƒ í•„í„°ë§)

        title = doc.metadata.get("title")
        if title in seen_titles:
            continue  # ì´ë¯¸ ì¶”ê°€í•œ titleì´ë©´ ìŠ¤í‚µ
        seen_titles.add(title)

        news_id = doc.metadata.get("news_id")
        wdate = doc.metadata.get("wdate")
        summary = doc.page_content
        url = doc.metadata.get("url")
        image = doc.metadata.get("image")
        stock_list = safe_parse_list(doc.metadata.get("stock_list"))
        industry_list = safe_parse_list(doc.metadata.get("industry_list"))

        if not news_id or not wdate or not summary:
            continue

        news_similar_list.append(
            SimilarNewsItem(
                news_id=news_id,
                wdate=wdate,
                title=title,
                summary=summary,
                url=url,
                image=image,
                stock_list=stock_list,
                industry_list=industry_list,
                similarity=similarity,
            )
        )

    return news_similar_list[:top_k]


# ì „ì—­ Redis ì—°ê²°
redis_conn = redis.Redis(
    host="3.39.99.26",
    port=6379,
    password="q1w2e3r4!@#",
    decode_responses=True,
)

# ì „ì—­ ThreadPoolExecutor
redis_executor = ThreadPoolExecutor(max_workers=10)


# Redis ì „ì†¡ í•¨ìˆ˜
def send_to_redis(data: dict):
    try:
        redis_conn.publish("chat-response", json.dumps(data, ensure_ascii=False))
    except Exception as e:
        print(f"[Redis Error] {type(e).__name__}: {e}")


# ğŸ” ì „ì²´ ì²˜ë¦¬ ë¡œì§ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë¨)
def process_chat_and_publish(chatbot, payload):
    try:
        start = time.time()
        print(f"[{datetime.now()}] ğŸ”µ Background ì‘ì—… ì‹œì‘")

        t1 = time.time()
        messages = chatbot.make_stream_prompt(payload.question, top_k=2)
        print(
            f"[{datetime.now()}] âœ… make_stream_prompt ì™„ë£Œ ({time.time() - t1:.2f}s)"
        )

        t2 = time.time()
        stream = chatbot.get_client().chat.completions.create(
            model="gpt-4.1-mini",
            messages=messages,
            temperature=0.4,
            max_tokens=1024,
            stream=True,
        )
        print(f"[{datetime.now()}] âœ… GPT ìŠ¤íŠ¸ë¦¬ë° ì¤€ë¹„ ì™„ë£Œ ({time.time() - t2:.2f}s)")

        idx = 0
        for chunk in stream:
            delta = chunk.choices[0].delta
            content = getattr(delta, "content", None)
            if content:
                data = {
                    "client_id": payload.client_id,
                    "content": content,
                    "is_last": False,
                    "index": idx,
                }
                send_to_redis(data)
                idx += 1

        # ë§ˆì§€ë§‰ ë©”ì‹œì§€
        data = {
            "client_id": payload.client_id,
            "content": "",
            "is_last": True,
            "index": idx,
        }
        send_to_redis(data)
        print(
            f"[{datetime.now()}] âœ… Redis ì „ì†¡ ì™„ë£Œ (ì´ {idx+1}ê°œ, {time.time() - start:.2f}s ì†Œìš”)"
        )

    except Exception as e:
        print(f"[âŒ Background Error] {type(e).__name__}: {e}")


# âœ… FastAPI ì—”ë“œí¬ì¸íŠ¸ í•¨ìˆ˜
async def get_stream_response(request, payload):
    print(f"[{datetime.now()}] ğŸŸ¢ ìš”ì²­ ìˆ˜ì‹  â†’ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì‹œì‘")
    chatbot = request.app.state.chatbot
    loop = asyncio.get_event_loop()

    # ì „ì²´ ì²˜ë¦¬ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
    loop.run_in_executor(None, process_chat_and_publish, chatbot, payload)

    print(f"[{datetime.now()}] ğŸŸ¢ ì‘ë‹µ ì¦‰ì‹œ ë°˜í™˜")
    return {"message": "ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. Redis ì±„ë„(chat-response)ë¡œ ì „ì†¡ë©ë‹ˆë‹¤."}


async def get_embedding_batch(article_list, request):
    """ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”© ì²˜ë¦¬"""
    batch_size = 20

    all_embeddings = []
    for i in range(0, len(article_list), batch_size):
        batch = article_list[i : i + batch_size]
        try:
            batch_news_embeddings = await get_news_embeddings(batch, request)
            batch_embeddings = batch_news_embeddings if batch_news_embeddings else []
            all_embeddings.extend(batch_embeddings)
            print(f"ë°°ì¹˜ {i}-{i+len(batch)}: {len(batch_embeddings)}ê°œ ì„ë² ë”© ì„±ê³µ")

        except Exception as e:
            print(f"ë°°ì¹˜ {i}-{i+len(batch)} ì‹¤íŒ¨: {str(e)}")
            all_embeddings.extend([None] * len(batch))

    return all_embeddings


async def get_news_metadata_df(news_ids: list) -> pd.DataFrame:
    """
    ë‰´ìŠ¤ ID ëª©ë¡ì„ ë°›ì•„ ê° ë‰´ìŠ¤ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜
    :param news_ids: ë‰´ìŠ¤ ID ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['20250513_0092', '20250618_28735495'])
    :return: ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° DataFrame
    """
    API_BASE_URL = "http://3.37.207.16:8000"
    METADATA_ENDPOINT = "/news/v2/{news_id}/metadata"

    records = []

    # news_idsê°€ ìœ íš¨í•œ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
    if not isinstance(news_ids, list) or not all(isinstance(i, str) for i in news_ids):
        print("ì˜ëª»ëœ news_ids í˜•ì‹: ë°˜ë“œì‹œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤")
        return pd.DataFrame()

    for news_id in news_ids:
        # URL ì •ìƒ ìƒì„± í™•ì¸
        url = API_BASE_URL + METADATA_ENDPOINT.format(news_id=news_id)
        try:
            response = requests.get(url)
            response.raise_for_status()  # 4xx/5xx ì˜¤ë¥˜ ì‹œ ì˜ˆì™¸ ë°œìƒ
            meta = response.json()
            records.append(meta)
            print(f"{news_id} ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì„±ê³µ")
        except requests.exceptions.HTTPError as e:
            print(f"{news_id} ì²˜ë¦¬ ì¤‘ HTTP ì˜¤ë¥˜ ({e.response.status_code}): {e}")
        except Exception as e:
            print(f"{news_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    return pd.DataFrame(records)


async def get_news_recommended(payload, request):
    """
    ë‰´ìŠ¤ í›„ë³´êµ° ì¶”ì²œ
    """
    news_clicked_ids = payload.news_clicked_ids
    news_candidate_ids = payload.news_candidate_ids

    # ì„ë² ë”©
    news_clicked_metadata = await get_news_metadata_df(news_clicked_ids)
    news_candidate_medatata = await get_news_metadata_df(news_candidate_ids)

    news_clicked_summaries = news_clicked_metadata["summary"].tolist()
    news_candidate_summaries = news_candidate_medatata["summary"].tolist()

    news_clicked_embeddings = await get_embedding_batch(news_clicked_summaries, request)
    news_candidate_embeddings = await get_embedding_batch(
        news_candidate_summaries, request
    )

    # ì„ë² ë”© ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬
    clicked_embeddings_np = np.array(news_clicked_embeddings, dtype=np.float32)
    candi_embeddings_np = np.array(news_candidate_embeddings, dtype=np.float32)

    # ì…ë ¥ ë°ì´í„° í˜•ì‹ ì¡°ì •
    num_clicked = len(news_clicked_embeddings)
    num_candidates = len(news_candidate_embeddings)

    clicked_input = clicked_embeddings_np[:num_clicked].reshape(1, num_clicked, 768)
    candidate_input = candi_embeddings_np[:num_candidates].reshape(
        1, num_candidates, 768
    )

    # ì¶”ë¡  Step 1
    top_k = 5  # í´ë¦­ ë‰´ìŠ¤ ë³„ í›„ë³´ ë‰´ìŠ¤ ìœ ì‚¬ í™•ë¥  top_k

    # ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
    inputs = {"clicked": clicked_input, "candidates": candidate_input}
    model_recommend = request.app.state.model_recommend
    outputs = model_recommend.run(None, inputs)[0]

    scores = outputs  # [1, 5]
    top_indices = np.argsort(-scores, axis=2)[:, :, :top_k]
    flattened = top_indices.squeeze(0).flatten()

    # ìœ ë‹ˆí¬ í›„ë³´êµ° ë° ë“±ì¥ ë¹ˆë„ ê³„ì‚°
    unique_candidates = np.unique(flattened)
    counts = Counter(flattened)

    # [0, 2, 6, 4, ...]
    candidate_indices = unique_candidates.tolist()

    # ì‹¤ì œ ë‰´ìŠ¤ ID ë§¤í•‘
    top_k_news_ids = [news_candidate_ids[i] for i in candidate_indices]

    # ìµœì¢… ë°˜í™˜
    # ['20250513_0089', '20250513_0148', '20250513_0085', ... ]
    return top_k_news_ids


async def get_news_recommended_ranked(payload, request, db):
    """
    ë‰´ìŠ¤ í›„ë³´êµ° ì¶”ì²œ
    """
    model_recommend_ranker = request.app.state.model_recommend_ranker
    user_id = payload.user_id

    # ê¸°ë³¸ê°’
    user_data = {"userPnl": 0, "asset": 0, "investScore": 1}

    is_user_exist = user_id not in [None, "", "None"]
    if is_user_exist:
        # 1ì°¨ API ìš”ì²­ (3.39.99.26)
        try:
            url1 = f"http://3.39.99.26:8080/api/v1/userinfo/{user_id}"
            response = requests.get(url1, timeout=1)
            response.raise_for_status()
            data = response.json()["data"]

            if isinstance(data, dict) and data:
                user_data = data
            else:
                raise ValueError(f"âœ… ì‚¬ìš©ì {user_id} ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ (1ì°¨): {url1}")
        except Exception as e:
            print(f"âŒ 1ì°¨ ì‚¬ìš©ì API ì‹¤íŒ¨: {str(e)}")

            # 2ì°¨ API ìš”ì²­ (3.37.207.16)
            try:
                url2 = f"http://3.37.207.16:8000/users/{user_id}"
                response = requests.get(url2, timeout=1)
                response.raise_for_status()
                user_data = response.json()

                print(f"âœ… ì‚¬ìš©ì {user_id} ì •ë³´ ì¡°íšŒ ì„±ê³µ (2ì°¨): {url2}")
            except Exception as e:
                print(f"âŒ 2ì°¨ ì‚¬ìš©ì API ì‹¤íŒ¨: {str(e)}")
                print(f"âš ï¸ ê¸°ë³¸ ì‚¬ìš©ì ë°ì´í„°ë¡œ ëŒ€ì²´: {user_data}")

        user_data = {
            "userPnl": user_data.get("userPnl") or user_data.get("user_pnl", 0),
            "asset": user_data.get("asset", 0),
            "investScore": user_data.get("investScore")
            or user_data.get("invest_score", 1),
            "memberStocks": user_data.get("memberStocks")
            or user_data.get("member_stocks", []),
        }

    # ë‰´ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    news_ids = payload.news_ids

    # 1. ê°ê° í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¡°íšŒ
    news_list = db.query(NewsModel_v2).filter(NewsModel_v2.news_id.in_(news_ids)).all()
    metadata_list = (
        db.query(NewsModel_v2_Metadata)
        .filter(NewsModel_v2_Metadata.news_id.in_(news_ids))
        .all()
    )
    external_list = (
        db.query(NewsModel_v2_External)
        .filter(NewsModel_v2_External.news_id.in_(news_ids))
        .all()
    )
    topic_list = (
        db.query(NewsModel_v2_Topic)
        .filter(NewsModel_v2_Topic.news_id.in_(news_ids))
        .all()
    )

    # 2. ê° í…Œì´ë¸” ê²°ê³¼ë¥¼ news_id ê¸°ì¤€ dictë¡œ ë§¤í•‘
    news_map = {row.news_id: row.__dict__ for row in news_list}
    metadata_map = {row.news_id: row.__dict__ for row in metadata_list}
    external_map = {row.news_id: row.__dict__ for row in external_list}
    topic_map = {row.news_id: row.__dict__ for row in topic_list}

    # 3. SQLAlchemy ë‚´ë¶€ í‚¤ ì œê±° (_sa_instance_state)
    def clean_sqlalchemy_dict(d):
        return {k: v for k, v in d.items() if k != "_sa_instance_state"}

    # 4. ë³‘í•©
    merged_results = []

    for news_id in news_ids:
        row = {}

        if news_id in news_map:
            row.update(clean_sqlalchemy_dict(news_map[news_id]))
        if news_id in metadata_map:
            row.update(clean_sqlalchemy_dict(metadata_map[news_id]))
        if news_id in external_map:
            row.update(clean_sqlalchemy_dict(external_map[news_id]))
        if news_id in topic_map:
            row.update(clean_sqlalchemy_dict(topic_map[news_id]))

        # [2] ì‚¬ìš©ì ë°ì´í„° ë³‘í•©
        row.update(user_data)  # user_dataëŠ” ê° rowì— ë™ì¼í•˜ê²Œ ì ìš©ëœë‹¤ê³  ê°€ì •

        # [3] main_topic ê³„ì‚°
        topic_columns = [f"topic_{i}" for i in range(1, 10)]
        topic_values = [row.get(col) for col in topic_columns]

        if any(pd.notna(val) for val in topic_values):
            try:
                max_topic_index = int(np.argmax(topic_values)) + 1  # 1ë¶€í„° ì‹œì‘
                row["main_topic"] = max_topic_index
            except Exception:
                row["main_topic"] = None
        else:
            row["main_topic"] = None

        # [4] is_same_stock ê³„ì‚°
        try:
            news_stocks = {
                stock["stock_id"] for stock in row.get("stock_list_view", [])
            }
            user_stocks = {stock["stockCode"] for stock in row.get("memberStocks", [])}
            row["is_same_stock"] = 1 if news_stocks & user_stocks else 0
        except Exception:
            row["is_same_stock"] = 0

        # [5] ë³‘í•© ê²°ê³¼ ì €ì¥
        merged_results.append(row)

    columns_to_keep = [
        "userPnl",
        "asset",
        "investScore",
        "topic_1",
        "topic_2",
        "topic_3",
        "topic_4",
        "topic_5",
        "topic_6",
        "topic_7",
        "topic_8",
        "topic_9",
        "main_topic",
        "is_same_stock",
    ]

    # ë³‘í•©ëœ ê²°ê³¼ì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê¸°
    filtered_results = [
        {k: row.get(k, None) for k in columns_to_keep} for row in merged_results
    ]

    df_input = pd.DataFrame(filtered_results)

    try:
        # í´ë¦­ í™•ë¥  ì˜ˆì¸¡ (í´ë˜ìŠ¤ 1ì— ëŒ€í•œ í™•ë¥ )
        proba = model_recommend_ranker.predict_proba(df_input)[:, 1]

        # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ë³‘í•©
        df_pred = pd.DataFrame(merged_results)
        df_pred["click_score"] = proba
        df_pred = df_pred[
            [
                "news_id",
                "wdate",
                "title",
                "summary",
                "image",
                "press",
                "url",
                "click_score",
                "stock_list",
            ]
        ]

        # 7. í™•ë¥  ê¸°ì¤€ ì •ë ¬
        df_pred = df_pred.sort_values("click_score", ascending=False).reset_index(
            drop=True
        )

        # 5. SHAP value ê³„ì‚°
        explainer = shap.TreeExplainer(model_recommend_ranker)
        shap_values = explainer.shap_values(df_input)  # shape: [n_samples, n_features]

        # 6. ìƒìœ„ 3ê°œ ì¤‘ìš” í”¼ì²˜ëª… ì¶”ì¶œ
        feature_kor_map = {
            "userPnl": "íˆ¬ì ìˆ˜ìµë¥  ê³ ë ¤",
            "asset": "ìì‚° ë³´ìœ ëŸ‰ ê³ ë ¤",
            "investScore": "íˆ¬ì ì„±í–¥ ê³ ë ¤",
            "topic_1": "ê¸€ë¡œë²Œ ì‹œì¥ ë™í–¥",
            "topic_2": "ì¦ê¶Œì‚¬ ì£¼ê°€ ì „ë§",
            "topic_3": "ê¸°ì—… ì‹¤ì  ê°œì„ ",
            "topic_4": "ì¦ê¶Œì‚¬ ì‚¬ì—… ë¶„ì„",
            "topic_5": "ëŒ€ê¸°ì—… ì‚¬ì—… ì „ëµ",
            "topic_6": "ê¸°ì—… ê¸°ìˆ  ê°œë°œ",
            "topic_7": "ë°˜ë„ì²´ ë° AI",
            "topic_8": "ê¸ˆìœµ ì„œë¹„ìŠ¤",
            "topic_9": "ì£¼ì£¼ ë° ê²½ì˜ ì´ìŠˆ",
            "is_same_stock": "ê´€ì‹¬ ìœ ì‚¬ ì¢…ëª©",
        }

        top_features_list = []
        feature_names = df_input.columns.tolist()

        for i, shap_row in enumerate(shap_values):
            abs_values = [abs(val) for val in shap_row]
            top_indices = sorted(range(len(abs_values)), key=lambda i: -abs_values[i])[
                :3
            ]
            top_feature_names = [feature_names[i] for i in top_indices]

            main_topic_value = df_input.iloc[i]["main_topic"]
            converted = []

            for f in top_feature_names:
                if f == "main_topic" and 1 <= int(main_topic_value) <= 9:
                    topic_key = f"topic_{int(main_topic_value)}"
                    converted.append(feature_kor_map.get(topic_key, topic_key))
                else:
                    converted.append(feature_kor_map.get(f, f))

            unique_converted = list(
                dict.fromkeys(converted)
            )  # âœ… ìˆœì„œ ìœ ì§€ + ì¤‘ë³µ ì œê±°
            top_features_list.append(unique_converted)

        # 7. SHAP ìƒìœ„ í”¼ì²˜ ì»¬ëŸ¼ ì¶”ê°€
        df_pred["recommend_reasons"] = top_features_list

        news_recommend_ranked_list = df_pred[:10].to_dict(orient="records")

        return news_recommend_ranked_list

    except Exception as e:
        print(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []
