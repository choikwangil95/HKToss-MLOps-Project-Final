from tokenizers import Tokenizer
from pathlib import Path
import onnxruntime as ort
from langchain_chroma import Chroma
from langchain.embeddings.base import Embeddings
import numpy as np
import joblib
import os
import requests
import openai
from dotenv import load_dotenv
import pickle
import re

load_dotenv()


def get_embedding_tokenizer():
    """
    ONNX NER ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    base_path = Path("models/kr_sbert_mean_onnx")

    tokenizer = Tokenizer.from_file(str(base_path / "tokenizer.json"))
    session = ort.InferenceSession(str(base_path / "kr_sbert.onnx"))

    return tokenizer, session


def get_vectordb():
    """
    vectordb ë¡œë”©
    """

    class OnnxEmbedder(Embeddings):
        def __init__(self, model_path: str, tokenizer_path: str):
            self.session = ort.InferenceSession(model_path)
            self.tokenizer = Tokenizer.from_file(tokenizer_path)

        def _embed(self, text: str):
            encoding = self.tokenizer.encode(text)
            input_ids = np.array([encoding.ids], dtype=np.int64)
            attention_mask = np.array([[1] * len(encoding.ids)], dtype=np.int64)

            outputs = self.session.run(
                None, {"input_ids": input_ids, "attention_mask": attention_mask}
            )

            raw_vector = outputs[0][0]
            norm_vector = raw_vector / (np.linalg.norm(raw_vector) + 1e-10)
            return norm_vector.tolist()

        def embed_documents(self, texts: list[str]) -> list[list[float]]:
            return [self._embed(text) for text in texts]

        def embed_query(self, text: str) -> list[float]:
            return self._embed(text)

    model_base_path = Path("models")

    embedding = OnnxEmbedder(
        model_path=str(model_base_path / "kr_sbert_mean_onnx/kr_sbert.onnx"),
        tokenizer_path=str(model_base_path / "kr_sbert_mean_onnx/tokenizer.json"),
    )

    db_base_path = Path("db")

    vectordb = Chroma(
        persist_directory=str(db_base_path / "chroma_store"),
        embedding_function=embedding,
    )

    return vectordb


class NewsTossChatbot:
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY í•„ìš”)
        self.client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def get_client(self):
        return self.client

    def search_similar_news(self, query_text, top_k=2):
        # Step 1: ì²« ë²ˆì§¸ APIë¡œ query_text ê¸°ë°˜ ê°€ì¥ ìœ ì‚¬í•œ ë‰´ìŠ¤ 1ê°œ ì°¾ê¸°
        first_url = "http://15.165.211.100:8000/news/similar"
        response = requests.post(first_url, json={"article": query_text, "top_k": 2})
        response.raise_for_status()
        top_news = response.json()["similar_news_list"]
        # news_id = top_news["news_id"]
        similar_news = top_news.copy()

        # Step 2: í•´ë‹¹ news_idë¥¼ ë‘ ë²ˆì§¸ APIì— ë„£ì–´ì„œ ìœ ì‚¬ ë‰´ìŠ¤ top_kê°œ ê°€ì ¸ì˜¤ê¸°
        # second_url = f"http://3.37.207.16:8000/news/v2/{news_id}/similar?top_n=2"
        # response = requests.get(second_url)
        # response.raise_for_status()
        # similar_news = response.json()

        return similar_news

    def build_prompt(self, context, question, has_news=True):
        if has_news:
            system_prompt = """
                ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì£¼ì‹ íˆ¬ìì— ë„ì›€ì„ ì£¼ëŠ” ì „ë¬¸ AI ì±—ë´‡, 'ë‰´ìŠ¤í† ìŠ¤'ì…ë‹ˆë‹¤.  
                ì‚¬ìš©ìì˜ ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ì•„ë˜ ê¸°ì¤€ì„ ì² ì €íˆ ì§€í‚¤ë©° ê²½ì œ, ê¸ˆìœµ, ì£¼ì‹ê³¼ ì—°ê´€ì§€ì–´ ë‹µë³€í•˜ì„¸ìš”. 
                ---

                ## [ì§ˆë¬¸ ë¶„ë¥˜ ê¸°ì¤€]

                ### 1. ì •ì²´ì„± ê´€ë ¨ ì§ˆë¬¸
                - ì˜ˆì‹œ: "ë„ˆ ëˆ„êµ¬ì•¼", "ì •ì²´ê°€ ë­ì•¼", "ë‹ˆ ì—­í• ì€ ë­ì•¼", "ë„ˆ ë­í•˜ëŠ” ì• ì•¼"
                - **ì´ ê²½ìš°, ìœ ì‚¬ ë‰´ìŠ¤ ì¹´ë“œ ì—†ì´ ì•„ë˜ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”.**

                ì €ëŠ” ë‹¹ì‹ ì˜ ì£¼ì‹ íˆ¬ìì— ë„ì›€ì„ ì£¼ëŠ” ì±—ë´‡ 'ë‰´ìŠ¤í† ìŠ¤'ì…ë‹ˆë‹¤. ğŸ˜„


                ### 2. ê³¼ê±° ìœ ì‚¬ ë‰´ìŠ¤ ì§ˆë¬¸(ê²½ì œÂ·ì‚°ì—…Â·ì£¼ì‹Â·ì •ì±… ë“±)
                - ì˜ˆì‹œ: "í˜„ëŒ€ ë¡œí…œì´ ì¤‘ë™ì˜ ì „ìŸìœ¼ë¡œ ì¸í•´ ë°©ì‚° ì‚°ì—… ìˆ˜í˜œë¥¼ ì…ì—ˆë‹¤ê³  í•˜ëŠ”ë°, ê³¼ê±° ìœ ì‚¬ ì‚¬ê±´ ë‰´ìŠ¤ë¥¼ ì•Œë ¤ì¤˜."
                - ê³¼ê±° ë‰´ìŠ¤ë¥¼ ì•Œë ¤ë‹¬ë¼ëŠ” ì§ˆë¬¸ì—ëŠ” ê¼­ ì•„ë˜ì™€ ê°™ì´ ë‹µí•˜ì„¸ìš”.
                - ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë‰´ìŠ¤ ì¹´ë“œ HTMLì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
                - ê°€ê¸‰ì  2ê°œì˜ ë‰´ìŠ¤ ì¹´ë“œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”.

                [ë‰´ìŠ¤ ì¹´ë“œ ì¶œë ¥ í˜•ì‹]

                - ** ì£¼ì˜: ì½”ë“œë¸”ë¡(```html ... ```) ì‚¬ìš© ì ˆëŒ€ ê¸ˆì§€!  
                - HTML íƒœê·¸ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤ì œ ë Œë”ë§ë˜ë„ë¡ í•´ì•¼ í•¨.**
                - HTML ì¶œë ¥ ì˜ˆì‹œ:

                <div style="margin-bottom: 24px;">
                <h3 style="margin: 0 0 8px 0; font-size: 22px;">
                    ìœ ì‚¬ ì‚¬ê±´ ë‰´ìŠ¤ 1
                </h3>

                <a href="https://n.news.naver.com/mnews/article/015/0005063326" target="_blank" style="text-decoration: none; color: #0070f3;">
                    <strong>í•˜ì´ë¸Œ ìƒì¥ ë•Œ 4000ì–µ ë”°ë¡œ ì±™ê¸´ ë°©ì‹œí˜â€¦ë‹¹êµ­, ì œì¬ ì—¬ë¶€ ê²€í† </strong>
                </a>

                <img src="https://imgnews.pstatic.net/image/015/2024/11/29/0005063326_001_20241129155613852.jpg?type=w200"
                    alt="ë‰´ìŠ¤ ì´ë¯¸ì§€"
                    style="width: 200px; border-radius: 8px; margin-bottom: 12px;">
           
                    <p><strong>ğŸ“Šìœ ì‚¬ë„</strong>: 0.58</p>
                    <p><strong>ğŸ—“ï¸ë‚ ì§œ</strong>: 2024-11-29</p>
                    <p><strong>ğŸ“„ìš”ì•½</strong>: ë°©ì‹œí˜ í•˜ì´ë¸Œ ì˜ì¥ì€ 2020ë…„ í•˜ì´ë¸Œ ìƒì¥ ì „ ìŠ¤í‹±ì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸ ë“±ê³¼ ì£¼ì£¼ ê°„ ê³„ì•½ì„ ë§ºê³ ...</p>
 

                [ìœ ì‚¬ ì§ˆë¬¸ ì¶”ì²œ] 
                
                - ìœ ì‚¬ ì§ˆë¬¸ ì¶”ì²œì€ ë‰´ìŠ¤ ì¹´ë“œ ë‹µë³€ ì‹œì—ë§Œ ì¶”ê°€í•˜ê³ , ì •ì²´ì„± ê´€ë ¨ ì§ˆë¬¸ì´ë‚˜ ê·¸ ì™¸ ì§ˆë¬¸ì—ëŠ” ë…¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.
                - ë‰´ìŠ¤ì¹´ë“œ ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ **ì˜ë¬¸ë¬¸ í˜•íƒœì˜ ìœ ì‚¬ ì§ˆë¬¸** 2~3ê°œë¥¼ ì¶œë ¥í•˜ê³ , ë§ˆë¬´ë¦¬ ë©˜íŠ¸ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.  
                - **ë§ˆí¬ë‹¤ìš´ ëŒ€ì‹  ì•„ë˜ HTML êµ¬ì¡°ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.**

                <br />
                <h3 style="margin-top: 10px; font-size: 21px;">ì•„ë˜ì™€ ê°™ì€ ì§ˆë¬¸ë„ í•¨ê»˜ ì°¸ê³ í•´ë³´ì„¸ìš”!</h3>
                <br />
                    <p>â–¸ í•˜ì´ë¸Œ ìƒì¥ ë‹¹ì‹œ ë°©ì‹œí˜ ì˜ì¥ì˜ ê³„ì•½ ë‚´ìš©ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?</p>
                    <p>â–¸ ê³¼ê±° IPO ì£¼ê´€ì‚¬ ì„ ì • ê³¼ì •ì—ì„œ ì–´ë–¤ ì´ìŠˆë“¤ì´ ìˆì—ˆë‚˜ìš”?</p>
                    <p>â–¸ IPO ì‹¤íŒ¨ ì‹œ ì§€ë¶„ ë°˜í™˜ ì¡°ê±´ì´ ì ìš©ëœ ì‚¬ë¡€ê°€ ìˆë‚˜ìš”?</p>
                </ul>
                <br />
                <p style="margin-top: 12px;">ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ ì£¼ì„¸ìš” ğŸ˜‰</p>
                </div>


                ### 3. ê·¸ ì™¸, ê²½ì œÂ·ê¸ˆìœµ ìš©ì–´, íˆ¬ì ì „ëµ, ì£¼ì‹ ê´€ë ¨ ì¼ë°˜ ì§ˆë¬¸ ë“±
                - ì£¼ì‹ íˆ¬ìì— ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ, GPTì˜ ì „ë¬¸ ì§€ì‹ì„ í™œìš©í•´ ììœ ë¡­ê²Œ ë‹µë³€í•˜ì„¸ìš”.
                - ì‚¬ìš©ìê°€ ì½ê¸° í¸í•˜ê²Œë” ì ì ˆí•œ ë¬¸ë‹¨ ë‚˜ëˆ„ê¸°, ì¤„ë°”ê¿ˆ ë“± ë‹¤ì–‘í•œ ì„œì‹ì„ ì´ìš©í•˜ì„¸ìš”.
                - ì˜ˆì‹œ: ìš©ì–´ í•´ì„¤, íˆ¬ì ì „ëµ ì„¤ëª…, ê¸ˆìœµ ìƒí’ˆ ë¹„êµ, ì‹œì¥ ë¶„ì„, ì¬ë¬´ì§€í‘œ í•´ì„, íˆ¬ì íŒ ë“±
                - ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ, ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‘ì„±í•˜ì„¸ìš”.
                - **ë‹¨, ì‹œì‚¬ ì´ìŠˆ/ì‚¬ê±´ì´ ì•„ë‹ˆë¼ë©´ ë‰´ìŠ¤ì¹´ë“œ í˜•ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**

                ---

                ## [ê¸ˆì§€ì‚¬í•­]
                - í—ˆìœ„ ì •ë³´, ë¯¸ë˜ ì˜ˆì¸¡, ê°œì¸ ì˜ê²¬, íˆ¬ì ê¶Œìœ ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
                - í•­ìƒ ì¤‘ë¦½ì ì´ê³  ì •ë³´ ì¤‘ì‹¬ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

            """

            user_prompt = f"""## [ì œê³µëœ ìœ ì‚¬ ë‰´ìŠ¤ ì¹´ë“œ]
                    {context}

                    ## [ì‚¬ìš©ì ì§ˆë¬¸]
                    {question}"""

            return [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]

        else:
            system_prompt = """ë‹¹ì‹ ì€ ê³¼ê±° ë‰´ìŠ¤ ê¸°ë°˜ AI ì±—ë´‡ì…ë‹ˆë‹¤.  
                    ìœ ì‚¬ ë‰´ìŠ¤ê°€ ì—†ì„ ê²½ìš°, ì•„ë˜ 3ê°€ì§€ ì˜ˆì‹œ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•´ ì •ì¤‘íˆ ì•ˆë‚´í•˜ì„¸ìš”.  
                    **ì ˆëŒ€ ì¢…í•© ì •ë³´, ë°°ê²½ ì„¤ëª…, ê°œì¸ ì˜ê²¬ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.**

                    - "ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´, ìµœì‹  ì´ìŠˆì™€ ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•´ë“œë¦´ê²Œìš”!"
                    - "ì§ì ‘ì ì¸ ì—°ê´€ ì‚¬ë¡€ë¥¼ í™•ì¸í•˜ë ¤ë©´, ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”. â˜ºï¸"
                    - "ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´, ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
                """
            user_prompt = f"""## [ì‚¬ìš©ì ì§ˆë¬¸]{question}"""

            return [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]

    def make_stream_prompt(self, question, top_k=2):
        similar_news = self.search_similar_news(question, top_k=top_k)
        # filtered_news = [row for row in similar_news if row.get("similarity", 0) >= 0.1]
        filtered_news = similar_news.copy()

        retrieved_infos = []
        for row in filtered_news:
            info = (
                f"{row['title']} ({row['url']})\n"
                f"<img src=\"{row['image']}\" alt=\"ë‰´ìŠ¤ ì´ë¯¸ì§€\">\n"
                f"{row['summary']}\n"
                f"{row['wdate'][:10]}\n"
                f"(ìœ ì‚¬ë„: {0.4 + row.get('similarity', 0):.2f})"
            )
            retrieved_infos.append(info)

        context = "\n\n".join(retrieved_infos)
        return self.build_prompt(context, question, has_news=bool(filtered_news))

    def answer(self, question, top_k=2):
        messages = self.make_stream_prompt(question, top_k)

        response = self.client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=messages,
            temperature=0.4,
            max_tokens=1024,
        )

        return response.choices[0].message.content


def get_recommend_model():
    model_base_path = Path("models")

    # ONNX ì„¸ì…˜ ìƒì„±
    model_recommend = ort.InferenceSession(
        str(model_base_path / "two_tower_model.onnx")
    )

    return model_recommend


def get_recommend_ranker_model():
    model_base_path = Path("models")

    # ONNX ì„¸ì…˜ ìƒì„±
    model_recommend_ranker = joblib.load(str(model_base_path / "lgbm_model2.pkl"))

    return model_recommend_ranker
