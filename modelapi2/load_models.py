from tokenizers import Tokenizer
from pathlib import Path
import onnxruntime as ort
from langchain_chroma import Chroma
from langchain.embeddings.base import Embeddings
import numpy as np
import joblib
import os
import requests
import openai
from dotenv import load_dotenv
import pickle
import re

load_dotenv()


def get_embedding_tokenizer():
    """
    ONNX NER ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    base_path = Path("models/kr_sbert_mean_onnx")

    tokenizer = Tokenizer.from_file(str(base_path / "tokenizer.json"))
    session = ort.InferenceSession(str(base_path / "kr_sbert.onnx"))

    return tokenizer, session


def get_vectordb():
    """
    vectordb ë¡œë”©
    """

    class OnnxEmbedder(Embeddings):
        def __init__(self, model_path: str, tokenizer_path: str):
            self.session = ort.InferenceSession(model_path)
            self.tokenizer = Tokenizer.from_file(tokenizer_path)

        def _embed(self, text: str):
            encoding = self.tokenizer.encode(text)
            input_ids = np.array([encoding.ids], dtype=np.int64)
            attention_mask = np.array([[1] * len(encoding.ids)], dtype=np.int64)

            outputs = self.session.run(
                None, {"input_ids": input_ids, "attention_mask": attention_mask}
            )

            raw_vector = outputs[0][0]
            norm_vector = raw_vector / (np.linalg.norm(raw_vector) + 1e-10)
            return norm_vector.tolist()

        def embed_documents(self, texts: list[str]) -> list[list[float]]:
            return [self._embed(text) for text in texts]

        def embed_query(self, text: str) -> list[float]:
            return self._embed(text)

    model_base_path = Path("models")

    embedding = OnnxEmbedder(
        model_path=str(model_base_path / "kr_sbert_mean_onnx/kr_sbert.onnx"),
        tokenizer_path=str(model_base_path / "kr_sbert_mean_onnx/tokenizer.json"),
    )

    db_base_path = Path("db")

    vectordb = Chroma(
        persist_directory=str(db_base_path / "chroma_store"),
        embedding_function=embedding,
    )

    return vectordb


class NewsTossChatbot:
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY í•„ìš”)
        self.client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def get_client(self):
        return self.client

    def search_similar_news(self, query_text, top_k=2):
        # Step 1: ì²« ë²ˆì§¸ APIë¡œ query_text ê¸°ë°˜ ê°€ì¥ ìœ ì‚¬í•œ ë‰´ìŠ¤ 1ê°œ ì°¾ê¸°
        first_url = "http://15.164.44.39:8000/news/similar"
        response = requests.post(first_url, json={"article": query_text, "top_k": 2})
        response.raise_for_status()
        top_news = response.json()["similar_news_list"]
        # news_id = top_news["news_id"]
        similar_news = top_news.copy()

        # Step 2: í•´ë‹¹ news_idë¥¼ ë‘ ë²ˆì§¸ APIì— ë„£ì–´ì„œ ìœ ì‚¬ ë‰´ìŠ¤ top_kê°œ ê°€ì ¸ì˜¤ê¸°
        # second_url = f"http://3.37.207.16:8000/news/v2/{news_id}/similar?top_n=2"
        # response = requests.get(second_url)
        # response.raise_for_status()
        # similar_news = response.json()

        return similar_news

    def build_prompt(self, context, question):
        system_prompt = """
            ë‹¹ì‹ ì€ ì£¼ì‹ íˆ¬ìì— ë„ì›€ì„ ì£¼ëŠ” ì „ë¬¸ AI ì±—ë´‡, 'ë‰´ìŠ¤í† ìŠ¤'ì…ë‹ˆë‹¤.  
            ì•„ë˜ 3ê°€ì§€ ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ë‹µë³€ ê¸°ì¤€ì„ ì² ì €íˆ ì§€í‚¤ë©° ê²½ì œ, ê¸ˆìœµ, ì£¼ì‹ê³¼ ì—°ê´€ì§€ì–´ ë‹µë³€í•˜ì„¸ìš”. 
            ---

            ## [ì§ˆë¬¸ ìœ í˜• ë° ë‹µë³€ ê·œì¹™]

            ### 1. ì •ì²´ì„± ê´€ë ¨ ì§ˆë¬¸
            - ì˜ˆì‹œ: "ë„ˆ ëˆ„êµ¬ì•¼", "ì •ì²´ê°€ ë­ì•¼", "ë‹ˆ ì—­í• ì€ ë­ì•¼", "ë„ˆ ë­í•˜ëŠ” ì• ì•¼"
            - ì •ì²´ê°€ ë­”ì§€, ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ë¬»ëŠ” ê²½ìš°, ìœ ì‚¬ ë‰´ìŠ¤ ì¹´ë“œ ì—†ì´ ì•„ë˜ì™€ ê°™ì´ ì¶œë ¥í•˜ì„¸ìš”.
            "ì €ëŠ” ë‹¹ì‹ ì˜ ì£¼ì‹ íˆ¬ìì— ë„ì›€ì„ ì£¼ëŠ” ì±—ë´‡ 'ë‰´ìŠ¤í† ìŠ¤'ì…ë‹ˆë‹¤. ğŸ˜„ <br>
            1. ìº˜ë¦°ë”ë¥¼ í™•ì¸í•˜ê³ , ì•ìœ¼ë¡œ ìˆì„ ì¼ì •ê³¼ ê´€ë ¨ëœ ê³¼ê±° ìœ ì‚¬ ë‰´ìŠ¤ë¥¼ ë¬¼ì–´ë³´ì„¸ìš”! <br>
            2. ê²½ì œ, ê¸ˆìœµ ìš©ì–´ë‚˜ ì£¼ì‹ íˆ¬ì ê´€ë ¨ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"

            ### 2. ê³¼ê±° ìœ ì‚¬ ë‰´ìŠ¤ ì§ˆë¬¸(ê²½ì œÂ·ì‚°ì—…Â·ì£¼ì‹Â·ì •ì±… ë“±)
            - ê³¼ê±° ë‰´ìŠ¤ë¥¼ ì•Œë ¤ë‹¬ë¼ëŠ” ì§ˆë¬¸ì—ëŠ” ê¼­ ì•„ë˜ì™€ ê°™ì´ ë‹µí•˜ì„¸ìš”.
            - ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë‰´ìŠ¤ ì¹´ë“œ HTMLì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
            - ê°€ê¸‰ì  2ê°œì˜ ë‰´ìŠ¤ ì¹´ë“œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”.
            - ì•„ë˜ ì¶œë ¥ ì˜ˆì‹œ ì† ë‰´ìŠ¤ ì¹´ë“œ ë‚´ìš©ì€ ì°¸ê³ ë§Œ í•˜ë˜, ë™ì¼í•˜ê²Œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
            - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ê¸°ì—…ëª…/ì¢…ëª©ëª…ì„ ì°¾ê³ , ê·¸ì™€ ì—°ê´€ëœ ìœ ì‚¬í•œ ë‰´ìŠ¤ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ì„¸ìš”.
                - ê¸°ì—…ëª…/ì¢…ëª©ëª…ì´ ì—†ë‹¤ë©´, ë™ì¼ ì‚°ì—…êµ°ì—ì„œ ìœ ì‚¬ ë‰´ìŠ¤ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ì„¸ìš”.
            - contextì— ì—†ëŠ” ë‰´ìŠ¤, ë‚ ì§œ, ìš”ì•½, ì œëª©, ì´ë¯¸ì§€ëŠ” ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.

            [ë‰´ìŠ¤ ì¹´ë“œ ì¶œë ¥ í˜•ì‹]
            - ** ì£¼ì˜: ì½”ë“œë¸”ë¡(```html ... ```) ì‚¬ìš© ì ˆëŒ€ ê¸ˆì§€!  
            - HTML íƒœê·¸ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤ì œ ë Œë”ë§ë˜ë„ë¡ í•´ì•¼ í•¨.**
            - ë‰´ìŠ¤ ì¹´ë“œ ì œëª© ìœ—ì¤„ì— ìœ ì‚¬ ë‰´ìŠ¤ 1ï¸âƒ£, ìœ ì‚¬ ë‰´ìŠ¤ 2ï¸âƒ£ ë¥¼ í‘œì‹œí•˜ì„¸ìš”.
            - HTML ì¶œë ¥ ì˜ˆì‹œ:

            
            <h3 style="margin: 0 0 8px 0; font-size: 20px !important;">
                <strong style="font-size: 20px !important;">ìœ ì‚¬ ë‰´ìŠ¤ 1ï¸âƒ£</strong><br>
                <a href="https://n.news.naver.com/mnews/article/015/0005063326" target="_blank" style="text-decoration: underline; color: #0070f3;">
                <strong>í•˜ì´ë¸Œ ìƒì¥ ë•Œ 4000ì–µ ë”°ë¡œ ì±™ê¸´ ë°©ì‹œí˜â€¦ë‹¹êµ­, ì œì¬ ì—¬ë¶€ ê²€í† </strong>
                </a>
            </h3>

            <img src="https://imgnews.pstatic.net/image/015/2024/11/29/0005063326_001_20241129155613852.jpg?type=w200"
                alt="ë‰´ìŠ¤ ì´ë¯¸ì§€"
                style="width: 200px; border-radius: 8px; margin-bottom: 12px;">
    
                <p><strong>ğŸ“Šìœ ì‚¬ë„</strong>: 0.58</p>
                <p><strong>ğŸ—“ï¸ë‚ ì§œ</strong>: 2024-11-29</p>
                <p><strong>ğŸ“„ìš”ì•½</strong>: ë°©ì‹œí˜ í•˜ì´ë¸Œ ì˜ì¥ì€ 2020ë…„ í•˜ì´ë¸Œ ìƒì¥ ì „ ìŠ¤í‹±ì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸ ë“±ê³¼ ì£¼ì£¼ ê°„ ê³„ì•½ì„ ë§ºê³ ...</p>
                <br>


            [ìœ ì‚¬ ì§ˆë¬¸ ì¶”ì²œ]    
            - ìœ ì‚¬ ì§ˆë¬¸ ì¶”ì²œì€ ë‰´ìŠ¤ ì¹´ë“œ ë‹µë³€ ì‹œì—ë§Œ ì¶”ê°€í•˜ê³ , ì •ì²´ì„± ê´€ë ¨ ì§ˆë¬¸ì´ë‚˜ ê·¸ ì™¸ ì§ˆë¬¸ì—ëŠ” ë…¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.
            - ë‰´ìŠ¤ì¹´ë“œ ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ **ì˜ë¬¸ë¬¸ í˜•íƒœì˜ ìœ ì‚¬ ì§ˆë¬¸** 2~3ê°œë¥¼ ì¶œë ¥í•˜ê³ , ë§ˆë¬´ë¦¬ ë©˜íŠ¸ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.  
            - **ë§ˆí¬ë‹¤ìš´ ëŒ€ì‹  ì•„ë˜ HTML êµ¬ì¡°ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.**

            <br />
            <h3 style="margin-top: 10px; font-size: 20px !important;">ì•„ë˜ì™€ ê°™ì€ ì§ˆë¬¸ë„ í•¨ê»˜ ì°¸ê³ í•´ë³´ì„¸ìš”!</h3>
            <br />
                <p>â–¸ í•˜ì´ë¸Œ ìƒì¥ ë‹¹ì‹œ ë°©ì‹œí˜ ì˜ì¥ì˜ ê³„ì•½ ë‚´ìš©ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?</p>
                <p>â–¸ ê³¼ê±° IPO ì£¼ê´€ì‚¬ ì„ ì • ê³¼ì •ì—ì„œ ì–´ë–¤ ì´ìŠˆë“¤ì´ ìˆì—ˆë‚˜ìš”?</p>
                <p>â–¸ IPO ì‹¤íŒ¨ ì‹œ ì§€ë¶„ ë°˜í™˜ ì¡°ê±´ì´ ì ìš©ëœ ì‚¬ë¡€ê°€ ìˆë‚˜ìš”?</p>
            </ul>
            <br />
            <p style="margin-top: 12px;">ì§ˆë¬¸ì— "íšŒì‚¬ ì´ë¦„"ê³¼ "íŠ¹ì • ì‚¬ê±´/ì´ìŠˆ"ë¥¼ í¬í•¨í•˜ë©´ ë‹µë³€ ì •í™•ë„ê°€ ì˜¬ë¼ê°€ìš”!<br>
            ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ ì£¼ì„¸ìš”ğŸ˜‰</p>


            ### 3. ê·¸ ì™¸, ê²½ì œÂ·ê¸ˆìœµ ìš©ì–´, íˆ¬ì ì „ëµ, ì£¼ì‹ ê´€ë ¨ ì¼ë°˜ ì§ˆë¬¸ ë“±
            - ì£¼ì‹ íˆ¬ìì— ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ, GPTì˜ ì „ë¬¸ ì§€ì‹ì„ í™œìš©í•´ ììœ ë¡­ê²Œ ë‹µë³€í•˜ì„¸ìš”.
            - ì‚¬ìš©ìê°€ ì½ê¸° í¸í•˜ê²Œë” **ë¬¸ë‹¨ ë‚˜ëˆ„ê¸°**, **ì¤„ë°”ê¿ˆ**, **ê°•ì¡° í‘œì‹œ** ë“±ì„ ì ê·¹ì ìœ¼ë¡œ ì´ìš©í•˜ì„¸ìš”.
            - ë‹µë³€ì„ HTML í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ :
                - ì¤‘ìš”í•œ ë¶€ë¶„ì€ <strong> íƒœê·¸ë¡œ ê°•ì¡°
                - ë¦¬ìŠ¤íŠ¸ëŠ” <ul>, <li> íƒœê·¸ë¡œ ê°•ì¡°
                ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” <p> íƒœê·¸ë¡œ êµ¬ë¶„
            - ì˜ˆì‹œ: ìš©ì–´ í•´ì„¤, íˆ¬ì ì „ëµ ì„¤ëª…, ê¸ˆìœµ ìƒí’ˆ ë¹„êµ, ì‹œì¥ ë¶„ì„, ì¬ë¬´ì§€í‘œ í•´ì„, íˆ¬ì íŒ ë“±
            - ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ, ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‘ì„±í•˜ì„¸ìš”.
            - **ë‹¨, ì‹œì‚¬ ì´ìŠˆ/ì‚¬ê±´ì´ ì•„ë‹ˆë¼ë©´ ë‰´ìŠ¤ì¹´ë“œ í˜•ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**

            ---

            ## [ê¸ˆì§€ì‚¬í•­]
            - í—ˆìœ„ ì •ë³´, ë¯¸ë˜ ì˜ˆì¸¡, ê°œì¸ ì˜ê²¬, íˆ¬ì ê¶Œìœ ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
            - í•­ìƒ ì¤‘ë¦½ì ì´ê³  ì •ë³´ ì¤‘ì‹¬ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

        """

        user_prompt = f"""## [ì œê³µëœ ìœ ì‚¬ ë‰´ìŠ¤ ì¹´ë“œ]
                {context}

                ## [ì‚¬ìš©ì ì§ˆë¬¸]
                {question}"""

        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

    def make_stream_prompt(self, question, top_k=2):
        similar_news = self.search_similar_news(question, top_k=top_k)
        # filtered_news = [row for row in similar_news if row.get("similarity", 0) >= 0.1]
        # filtered_news = similar_news.copy()
        retrieved_infos = []
        for row in similar_news:
            info = (
                f"{row['title']} ({row['url']})\n"
                f"<img src=\"{row['image']}\" alt=\"ë‰´ìŠ¤ ì´ë¯¸ì§€\">\n"
                f"{row['summary']}\n"
                f"{row['wdate'][:10]}\n"
                f"(ìœ ì‚¬ë„: {0.5 + row.get('similarity', 0):.2f})"
            )
            retrieved_infos.append(info)

        context = "\n\n".join(retrieved_infos)
        return self.build_prompt(context, question)

    def answer(self, question, top_k=2):
        messages = self.make_stream_prompt(question, top_k)

        response = self.client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=messages,
            temperature=0.4,
            max_tokens=1024,
        )

        return response.choices[0].message.content


def get_recommend_model():
    model_base_path = Path("models")

    # ONNX ì„¸ì…˜ ìƒì„±
    model_recommend = ort.InferenceSession(
        str(model_base_path / "two_tower_model.onnx")
    )

    return model_recommend


def get_recommend_ranker_model():
    model_base_path = Path("models")

    # ONNX ì„¸ì…˜ ìƒì„±
    model_recommend_ranker = joblib.load(str(model_base_path / "lgbm_model2.pkl"))

    return model_recommend_ranker
