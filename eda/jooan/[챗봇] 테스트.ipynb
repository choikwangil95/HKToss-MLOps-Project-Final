{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0a3b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp39-cp39-win_amd64.whl.metadata (5.0 kB)\n",
      "Downloading psycopg2_binary-2.9.10-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.5/1.2 MB 246.8 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.5/1.2 MB 246.8 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 0.8/1.2 MB 325.8 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 1.0/1.2 MB 449.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 440.6 kB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718990cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL 연결 성공!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "# PostgreSQL 연결 테스트\n",
    "conn = psycopg2.connect(\n",
    "    host=\"3.37.207.16\",\n",
    "    port=5432,\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"password\",\n",
    ")\n",
    "print(\"PostgreSQL 연결 성공!\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092eb7ea",
   "metadata": {},
   "source": [
    "## 회귀모델 사용 ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae26b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 1. 임베딩 모델 및 벡터DB 로드\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    ")\n",
    "vector_db = FAISS.load_local(\n",
    "    \"vectorstore/news_db\", embedding_model, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# 2. 이미 훈련된 회귀모델 로드\n",
    "reg_model = joblib.load(\"models/regression_model.pkl\")  # 경로는 실제 파일 위치로\n",
    "\n",
    "# 3. 쿼리 입력 및 top-N 검색\n",
    "query = \"미국 금리 인하 시 국내 증시가 어떤 흐름을 보일까?\"\n",
    "top_k = 5\n",
    "results_with_scores = vector_db.similarity_search_with_score(query, k=top_k)\n",
    "retrieved_docs = [doc for doc, _ in results_with_scores]\n",
    "\n",
    "# 4. 각 문서에서 회귀모델 입력 피처 추출\n",
    "def extract_features(doc):\n",
    "    # 예시: 문서 메타데이터에서 피처 추출\n",
    "    meta = doc.metadata\n",
    "    return [\n",
    "        meta.get(\"impact_score\", 0),\n",
    "        meta.get(\"d_minus_14_close\", 0),\n",
    "        meta.get(\"d_minus_14_volume\", 0),\n",
    "        # 필요한 추가 피처...\n",
    "    ]\n",
    "\n",
    "features = np.array([extract_features(doc) for doc in retrieved_docs])\n",
    "\n",
    "# 5. 회귀모델로 유사도 예측 및 리랭킹\n",
    "predicted_scores = reg_model.predict(features)\n",
    "reranked_indices = np.argsort(predicted_scores)[::-1]\n",
    "reranked_docs = [retrieved_docs[i] for i in reranked_indices]\n",
    "\n",
    "# 6. 챗봇 컨텍스트 구성 및 응답 생성\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "# 이후 llm_chain 등으로 답변 생성\n",
    "\n",
    "print(\"최종 리랭킹 결과:\")\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content[:100]} ...\")\n",
    "\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "당신은 주식 투자자를 위한 뉴스 기반 정보 어시스턴트 챗봇 '뉴스토스'입니다.\n",
    "당신의 임무는 실시간 뉴스와 과거 유사사건 뉴스 데이터를 바탕으로,\n",
    "- 사용자의 투자 판단에 도움이 되는 정보를 제공하고,\n",
    "- 과거 유사사건, 해당 시기의 주가 흐름, 관련 리포트의 핵심 내용을 구체적으로 인용하며,\n",
    "- 미래 전망 질문에는 과거 사례를 근거로 신중하게 의견을 제시하는 것입니다.\n",
    "\n",
    "답변 작성 시 반드시 다음을 지켜주세요:\n",
    "1. 답변 내용 중 포함되는 과거 유사사건의 날짜, 사건명, 당시 주가 흐름(상승/하락/횡보 등), 주요 리포트 내용은 구체적으로 인용하세요.\n",
    "2. 미래 전망 질문에는 과거 유사사건을 근거로 논리적인 전망을 제시하세요.\n",
    "3. 답변 마지막에는 '⭐️투자 판단은 본인의 책임입니다.⭐️'라는 안내문을 추가하세요.\n",
    "4. 답변은 반드시 한글로, 명확하고 간결하게 작성하세요.\n",
    "5. 제공된 검색 결과(유사도 높은 과거 뉴스, 주가 데이터, 리포트 등)만 근거로 사용하세요. 근거가 없으면 '근거가 없는데 답변해도 될까? 이건 너의 소중한 돈이 걸린 문제야'라고 하세요.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "검색 결과: {context}\n",
    "질문: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=prompt_template\n",
    ")\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"...\",  # 실제 모델 경로\n",
    "    n_gpu_layers=0,\n",
    "    n_ctx=2048,\n",
    "    n_threads=4,\n",
    "    verbose=True,\n",
    "    n_batch=512,\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "response = llm_chain.run({\"context\": context, \"question\": query})\n",
    "\n",
    "print(\"🧠 챗봇 응답:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de7b0a",
   "metadata": {},
   "source": [
    "## 회귀모델 사용 X, Cross-encoder 사용 ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23687393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 1. 임베딩 모델 및 벡터DB 로드\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    ")\n",
    "vector_db = FAISS.load_local(\n",
    "    \"vectorstore/news_db\", embedding_model, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# 2. 사용자 질문 임베딩 및 Top-K 검색\n",
    "query = \"미국 금리 인상 시 국내 증시에 어떤 영향을 줄까?\"\n",
    "top_k = 10\n",
    "results_with_scores = vector_db.similarity_search_with_score(query, k=top_k)\n",
    "retrieved_docs = [doc for doc, _ in results_with_scores]\n",
    "\n",
    "# 3. Cross-Encoder 리랭커 준비 (예시: BAAI/bge-reranker-v2-m3)\n",
    "reranker_model_name = \"BAAI/bge-reranker-v2-m3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    reranker_model_name\n",
    ").to(\"cpu\")\n",
    "reranker_model.eval()\n",
    "\n",
    "# 4. Cross-Encoder로 쿼리-문서 쌍 리랭킹\n",
    "pairs = [(query, doc.page_content) for doc in retrieved_docs]\n",
    "inputs = tokenizer(\n",
    "    [f\"{q} [SEP] {d}\" for q, d in pairs],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "with torch.no_grad():\n",
    "    scores = reranker_model(**inputs).logits.squeeze().cpu().numpy()\n",
    "reranked_indices = np.argsort(scores)[::-1]\n",
    "reranked_docs = [retrieved_docs[i] for i in reranked_indices]\n",
    "\n",
    "# 5. LLM 프롬프트 및 답변 생성 (예시)\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/app/ml_models/quantized_llama/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\",\n",
    "    n_gpu_layers=0,\n",
    "    n_ctx=2048,\n",
    "    n_threads=4,\n",
    "    verbose=True,\n",
    "    n_batch=512,\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "당신은 주식 투자자를 위한 뉴스 기반 정보 어시스턴트 챗봇 '뉴스토스'입니다.\n",
    "당신의 임무는 실시간 뉴스와 과거 유사사건 뉴스 데이터를 바탕으로,\n",
    "- 사용자의 투자 판단에 도움이 되는 정보를 제공하고,\n",
    "- 과거 유사사건, 해당 시기의 주가 흐름, 관련 리포트의 핵심 내용을 구체적으로 인용하며,\n",
    "- 미래 전망 질문에는 과거 사례를 근거로 신중하게 의견을 제시하는 것입니다.\n",
    "\n",
    "답변 작성 시 반드시 다음을 지켜주세요:\n",
    "1. 답변 내용 중 포함되는 과거 유사사건의 날짜, 사건명, 당시 주가 흐름(상승/하락/횡보 등), 주요 리포트 내용은 구체적으로 인용하세요.\n",
    "2. 미래 전망 질문에는 과거 유사사건을 근거로 논리적인 전망을 제시하세요.\n",
    "3. 답변 마지막에는 '⭐️투자 판단은 본인의 책임입니다.⭐️'라는 안내문을 추가하세요.\n",
    "4. 답변은 반드시 한글로, 명확하고 간결하게 작성하세요.\n",
    "5. 제공된 검색 결과(유사도 높은 과거 뉴스, 주가 데이터, 리포트 등)만 근거로 사용하세요. 근거가 없으면 '근거가 없는데 답변해도 될까? 이건 너의 소중한 돈이 걸린 문제야'라고 하세요.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "검색 결과: {context}\n",
    "질문: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=prompt_template\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "context = \"\\n\\n\".join(\n",
    "    [doc.page_content for doc in reranked_docs[:5]]\n",
    ")  # 최종 top-5만 사용\n",
    "response = llm_chain.run({\"context\": context, \"question\": query})\n",
    "\n",
    "print(\"🧠 챗봇 응답:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec7c59",
   "metadata": {},
   "source": [
    "## rag_pipeline.py (벡터 DB & Cross-encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05525649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from llama_cpp import Llama\n",
    "from typing import List, Dict\n",
    "\n",
    "class CSVDummyVectorDB:\n",
    "    def __init__(self, csv_path: str):\n",
    "        # CSV 파일 로드 (embedding 컬럼은 문자열 \"[0.1, 0.2, ...]\" 형태)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.texts = df['content'].tolist()\n",
    "        # embedding 컬럼 문자열을 리스트로 변환 후 numpy 배열 생성\n",
    "        self.embeddings = np.vstack(df['embedding'].apply(eval).to_numpy()).astype('float32')\n",
    "        # FAISS 인덱스 생성 및 벡터 정규화\n",
    "        faiss.normalize_L2(self.embeddings)\n",
    "        self.index = faiss.IndexFlatIP(self.embeddings.shape[1])\n",
    "        self.index.add(self.embeddings)\n",
    "\n",
    "    def search(self, query_vec: np.ndarray, top_k: int = 10) -> List[str]:\n",
    "        faiss.normalize_L2(query_vec)\n",
    "        scores, indices = self.index.search(query_vec, top_k)\n",
    "        return [self.texts[i] for i in indices[0]]\n",
    "\n",
    "class NewsTossChatbot:\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: str = \"/app/db/news_v2_vector_202506122113.csv\",\n",
    "        embedding_model_name: str = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\",\n",
    "        reranker_model_name: str = \"BAAI/bge-reranker-v2-m3\",\n",
    "        llm_model_path: str = \"/app/models/quantized_llama3/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf\",\n",
    "        k: int = 10,\n",
    "        rerank_top_n: int = 5\n",
    "    ):\n",
    "        # 1. 임베딩 모델 초기화\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        # 2. CSV 기반 벡터 DB 초기화\n",
    "        self.vector_db = CSVDummyVectorDB(csv_path)\n",
    "        # 3. Cross-Encoder 리랭커 초기화\n",
    "        self.reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "        self.reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name).to(\"cpu\")\n",
    "        self.reranker_model.eval()\n",
    "        # GPU 사용 시(예시, 주석 해제):\n",
    "        # self.reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name).to(\"cuda\")\n",
    "        # 4. LLaMA3 LLM 초기화\n",
    "        self.llm = Llama(\n",
    "            model_path=llm_model_path,\n",
    "            n_ctx=2048,\n",
    "            n_threads=4,      # CPU 코어 수\n",
    "            n_gpu_layers=0,   # CPU만 사용\n",
    "            verbose=False\n",
    "        )\n",
    "        # GPU 사용 시(예시, 주석 해제):\n",
    "        # self.llm = Llama(\n",
    "        #     model_path=llm_model_path,\n",
    "        #     n_ctx=2048,\n",
    "        #     n_threads=4,\n",
    "        #     n_gpu_layers=8,  # GPU 레이어 수 (환경에 맞게 조정)\n",
    "        #     verbose=True\n",
    "        # )\n",
    "        # 5. 프롬프트 템플릿\n",
    "        self.prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        당신은 주식 투자자를 위한 뉴스 기반 정보 어시스턴트 챗봇 '뉴스토스'입니다.\n",
    "        당신의 임무는 실시간 뉴스와 과거 유사사건 뉴스 데이터를 바탕으로,\n",
    "        - 사용자의 투자 판단에 도움이 되는 정보를 제공하고,\n",
    "        - 과거 유사사건, 해당 시기의 주가 흐름, 관련 리포트의 핵심 내용을 구체적으로 인용하며,\n",
    "        - 미래 전망 질문에는 과거 사례를 근거로 신중하게 의견을 제시하는 것입니다.\n",
    "\n",
    "        답변 작성 시 반드시 다음을 지켜주세요:\n",
    "        1. 답변 내용 중 포함되는 과거 유사사건의 날짜, 사건명, 당시 주가 흐름(상승/하락/횡보 등), 주요 리포트 내용은 구체적으로 인용하세요.\n",
    "        2. 미래 전망 질문에는 과거 유사사건을 근거로 논리적인 전망을 제시하세요.\n",
    "        3. 답변 마지막에는 '⭐️투자 판단은 본인의 책임입니다.⭐️'라는 안내문을 추가하세요.\n",
    "        4. 답변은 반드시 한글로, 명확하고 간결하게 작성하세요.\n",
    "        5. 제공된 검색 결과(유사도 높은 과거 뉴스, 주가 데이터, 리포트 등)만 근거로 사용하세요. 근거가 없으면 '근거가 없는데 답변해도 될까? 이건 너의 소중한 돈이 걸린 문제야'라고 하세요.\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        검색 결과: {context}\n",
    "        질문: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "        self.k = k\n",
    "        self.rerank_top_n = rerank_top_n\n",
    "\n",
    "    def retrieve_docs(self, query: str) -> List[str]:\n",
    "        \"\"\"CSV+FAISS 기반 벡터DB에서 Top-K 뉴스 검색\"\"\"\n",
    "        query_emb = self.embedding_model.encode([query]).astype('float32')\n",
    "        docs = self.vector_db.search(query_emb, top_k=self.k)\n",
    "        return docs\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[str]) -> List[str]:\n",
    "        \"\"\"Cross-Encoder로 리랭킹 후 상위 N개 뉴스 반환\"\"\"\n",
    "        pairs = [(query, doc) for doc in docs]\n",
    "        inputs = self.reranker_tokenizer(\n",
    "            [f\"{q} [SEP] {d}\" for q, d in pairs],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        # CPU 기준\n",
    "        with torch.no_grad():\n",
    "            scores = self.reranker_model(**inputs).logits.squeeze().cpu().numpy()\n",
    "        # GPU 사용 시(예시, 주석 해제):\n",
    "        # with torch.no_grad():\n",
    "        #     scores = self.reranker_model(**inputs.to(\"cuda\")).logits.squeeze().cpu().numpy()\n",
    "        reranked = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)\n",
    "        return [doc for _, doc in reranked[:self.rerank_top_n]]\n",
    "\n",
    "    def generate_answer(self, context: str, question: str) -> str:\n",
    "        \"\"\"LLM 직접 추론\"\"\"\n",
    "        full_prompt = self.prompt_template.format(context=context, question=question)\n",
    "        output = self.llm(full_prompt, max_tokens=512, stop=[\"<|eot_id|>\"])\n",
    "        return output['choices'][0]['text']\n",
    "\n",
    "    def answer(self, query: str) -> Dict[str, str]:\n",
    "        # 1. 뉴스 검색\n",
    "        docs = self.retrieve_docs(query)\n",
    "        # 2. 리랭킹\n",
    "        reranked_docs = self.rerank_docs(query, docs)\n",
    "        # 3. 컨텍스트 생성\n",
    "        context = \"\\n\\n\".join(reranked_docs)\n",
    "        # 4. 답변 생성\n",
    "        answer = self.generate_answer(context, query)\n",
    "        return {\"answer\": answer, \"reranked_docs\": reranked_docs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66af2a0",
   "metadata": {},
   "source": [
    "### 테스트!!~!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1b5e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.7.1-cp312-cp312-win_amd64.whl (216.1 MB)\n",
      "   ---------------------------------------- 0.0/216.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 4.5/216.1 MB 22.3 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 8.9/216.1 MB 22.1 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 13.1/216.1 MB 21.6 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 17.8/216.1 MB 22.0 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 22.5/216.1 MB 22.3 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 27.5/216.1 MB 22.4 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 31.5/216.1 MB 22.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 35.4/216.1 MB 21.8 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 40.1/216.1 MB 21.8 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 45.1/216.1 MB 22.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 49.8/216.1 MB 22.2 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 55.1/216.1 MB 22.3 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 60.0/216.1 MB 22.6 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 65.5/216.1 MB 22.8 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 70.5/216.1 MB 22.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 75.8/216.1 MB 23.0 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 81.3/216.1 MB 23.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 86.5/216.1 MB 23.4 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 91.5/216.1 MB 23.4 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 96.7/216.1 MB 23.5 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 101.7/216.1 MB 23.5 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 104.9/216.1 MB 23.6 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 108.3/216.1 MB 22.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 113.0/216.1 MB 22.9 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 117.2/216.1 MB 22.8 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 121.6/216.1 MB 22.8 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 124.3/216.1 MB 22.4 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 129.0/216.1 MB 22.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 133.7/216.1 MB 22.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 137.4/216.1 MB 22.4 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 142.9/216.1 MB 22.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 147.1/216.1 MB 22.3 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 151.0/216.1 MB 22.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 155.2/216.1 MB 22.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 159.9/216.1 MB 22.2 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 164.6/216.1 MB 22.2 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 169.3/216.1 MB 22.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 173.5/216.1 MB 22.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 178.0/216.1 MB 22.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 182.5/216.1 MB 22.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 187.2/216.1 MB 22.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 191.1/216.1 MB 22.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 195.3/216.1 MB 22.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 199.2/216.1 MB 22.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 204.5/216.1 MB 22.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 210.5/216.1 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  215.7/216.1 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 216.1/216.1 MB 21.6 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 5.2/6.3 MB 26.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 24.1 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.14.0 torch-2.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b4d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.1/10.5 MB 19.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 13.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.5 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 16.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 23.1 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed huggingface-hub-0.33.0 safetensors-0.5.3 sentence-transformers-4.1.0 tokenizers-0.21.1 transformers-4.52.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "104f52ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl (15.0 MB)\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 4.7/15.0 MB 23.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.4/15.0 MB 23.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.4/15.0 MB 23.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.0/15.0 MB 23.0 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76200df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n",
      "     ---------------------------------------- 0.0/67.9 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 4.5/67.9 MB 22.4 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 9.7/67.9 MB 24.2 MB/s eta 0:00:03\n",
      "     -------- ------------------------------ 14.7/67.9 MB 24.3 MB/s eta 0:00:03\n",
      "     ----------- --------------------------- 19.9/67.9 MB 24.7 MB/s eta 0:00:02\n",
      "     -------------- ------------------------ 25.7/67.9 MB 25.1 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 30.9/67.9 MB 24.9 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 35.7/67.9 MB 24.9 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 40.9/67.9 MB 24.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 46.4/67.9 MB 24.8 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 51.1/67.9 MB 24.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 56.1/67.9 MB 24.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 61.1/67.9 MB 24.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 65.8/67.9 MB 24.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 67.9/67.9 MB 23.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from llama-cpp-python) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [20 lines of output]\n",
      "      \u001b[32m*** \u001b[1mscikit-build-core 0.11.4\u001b[0m using \u001b[34mCMake 4.0.2\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "      \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "      2025-06-12 21:56:36,530 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "      loading initial cache file C:\\Users\\user\\AppData\\Local\\Temp\\tmp04rfoju9\\build\\CMakeInit.txt\n",
      "      -- Building for: NMake Makefiles\n",
      "      CMake Error at CMakeLists.txt:3 (project):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n",
      "      CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      \u001b[31m\n",
      "      \u001b[1m***\u001b[0m \u001b[31mCMake configuration failed\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6546507",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 1. CSV+FAISS 벡터DB 클래스\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCSVDummyVectorDB\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# 1. CSV+FAISS 벡터DB 클래스\n",
    "class CSVDummyVectorDB:\n",
    "    def __init__(self, csv_path: str):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.texts = df['content'].tolist()\n",
    "        self.embeddings = np.vstack(df['embedding'].apply(eval).to_numpy()).astype('float32')\n",
    "        faiss.normalize_L2(self.embeddings)\n",
    "        self.index = faiss.IndexFlatIP(self.embeddings.shape[1])\n",
    "        self.index.add(self.embeddings)\n",
    "\n",
    "    def search(self, query_vec: np.ndarray, top_k: int = 10):\n",
    "        faiss.normalize_L2(query_vec)\n",
    "        scores, indices = self.index.search(query_vec, top_k)\n",
    "        return [self.texts[i] for i in indices[0]]\n",
    "\n",
    "# 2. 주요 파라미터 (로컬 환경에 맞게 수정)\n",
    "csv_path = \"./news_v2_vector_202506122113.csv\"  # 노트북 파일 위치\n",
    "embedding_model_name = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    "reranker_model_name = \"BAAI/bge-reranker-v2-m3\"\n",
    "llm_model_path = \"./llama-3-Korean-Bllossom-8B-Q4_K_M.gguf\"  # 로컬에 다운받은 모델 경로\n",
    "\n",
    "# 3. 모델 로딩\n",
    "embedding_model = SentenceTransformer(embedding_model_name)\n",
    "vector_db = CSVDummyVectorDB(csv_path)\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name).to(\"cpu\")\n",
    "reranker_model.eval()\n",
    "llm = Llama(\n",
    "    model_path=llm_model_path,\n",
    "    n_ctx=2048,\n",
    "    n_threads=4,\n",
    "    n_gpu_layers=0,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 4. 프롬프트 템플릿\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "당신은 주식 투자자를 위한 뉴스 기반 정보 어시스턴트 챗봇 '뉴스토스'입니다.\n",
    "당신의 임무는 실시간 뉴스와 과거 유사사건 뉴스 데이터를 바탕으로,\n",
    "- 사용자의 투자 판단에 도움이 되는 정보를 제공하고,\n",
    "- 과거 유사사건, 해당 시기의 주가 흐름, 관련 리포트의 핵심 내용을 구체적으로 인용하며,\n",
    "- 미래 전망 질문에는 과거 사례를 근거로 신중하게 의견을 제시하는 것입니다.\n",
    "\n",
    "답변 작성 시 반드시 다음을 지켜주세요:\n",
    "1. 답변 내용 중 포함되는 과거 유사사건의 날짜, 사건명, 당시 주가 흐름(상승/하락/횡보 등), 주요 리포트 내용은 구체적으로 인용하세요.\n",
    "2. 미래 전망 질문에는 과거 유사사건을 근거로 논리적인 전망을 제시하세요.\n",
    "3. 답변 마지막에는 '⭐️투자 판단은 본인의 책임입니다.⭐️'라는 안내문을 추가하세요.\n",
    "4. 답변은 반드시 한글로, 명확하고 간결하게 작성하세요.\n",
    "5. 제공된 검색 결과(유사도 높은 과거 뉴스, 주가 데이터, 리포트 등)만 근거로 사용하세요. 근거가 없으면 '근거가 없는데 답변해도 될까? 이건 너의 소중한 돈이 걸린 문제야'라고 하세요.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "검색 결과: {context}\n",
    "질문: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "# 5. 전체 파이프라인 함수\n",
    "def rag_answer(question, top_k=10, rerank_n=5):\n",
    "    # 1. 임베딩\n",
    "    query_emb = embedding_model.encode([question]).astype('float32')\n",
    "    # 2. FAISS 검색\n",
    "    docs = vector_db.search(query_emb, top_k=top_k)\n",
    "    # 3. Cross-Encoder 리랭킹\n",
    "    pairs = [(question, doc) for doc in docs]\n",
    "    inputs = reranker_tokenizer(\n",
    "        [f\"{q} [SEP] {d}\" for q, d in pairs],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze().cpu().numpy()\n",
    "    reranked = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)\n",
    "    reranked_docs = [doc for _, doc in reranked[:rerank_n]]\n",
    "    # 4. LLM 답변 생성\n",
    "    context = \"\\n\\n\".join(reranked_docs)\n",
    "    full_prompt = prompt_template.format(context=context, question=question)\n",
    "    output = llm(full_prompt, max_tokens=512, stop=[\"<|eot_id|>\"])\n",
    "    return {\n",
    "        \"answer\": output['choices'][0]['text'],\n",
    "        \"reranked_docs\": reranked_docs\n",
    "    }\n",
    "\n",
    "# 6. 사용 예시\n",
    "question = \"금리인하했는데 주가에 미치는 영향은?\"\n",
    "result = rag_answer(question)\n",
    "print(\"답변:\", result[\"answer\"])\n",
    "print(\"참고 뉴스:\", result[\"reranked_docs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64cef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
