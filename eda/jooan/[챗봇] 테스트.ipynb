{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0a3b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp39-cp39-win_amd64.whl.metadata (5.0 kB)\n",
      "Downloading psycopg2_binary-2.9.10-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.5/1.2 MB 246.8 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.5/1.2 MB 246.8 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 0.8/1.2 MB 325.8 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 1.0/1.2 MB 449.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 440.6 kB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718990cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL ì—°ê²° ì„±ê³µ!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "# PostgreSQL ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "conn = psycopg2.connect(\n",
    "    host=\"3.37.207.16\",\n",
    "    port=5432,\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"password\",\n",
    ")\n",
    "print(\"PostgreSQL ì—°ê²° ì„±ê³µ!\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092eb7ea",
   "metadata": {},
   "source": [
    "## íšŒê·€ëª¨ë¸ ì‚¬ìš© ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae26b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 1. ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„°DB ë¡œë“œ\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    ")\n",
    "vector_db = FAISS.load_local(\n",
    "    \"vectorstore/news_db\", embedding_model, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# 2. ì´ë¯¸ í›ˆë ¨ëœ íšŒê·€ëª¨ë¸ ë¡œë“œ\n",
    "reg_model = joblib.load(\"models/regression_model.pkl\")  # ê²½ë¡œëŠ” ì‹¤ì œ íŒŒì¼ ìœ„ì¹˜ë¡œ\n",
    "\n",
    "# 3. ì¿¼ë¦¬ ì…ë ¥ ë° top-N ê²€ìƒ‰\n",
    "query = \"ë¯¸êµ­ ê¸ˆë¦¬ ì¸í•˜ ì‹œ êµ­ë‚´ ì¦ì‹œê°€ ì–´ë–¤ íë¦„ì„ ë³´ì¼ê¹Œ?\"\n",
    "top_k = 5\n",
    "results_with_scores = vector_db.similarity_search_with_score(query, k=top_k)\n",
    "retrieved_docs = [doc for doc, _ in results_with_scores]\n",
    "\n",
    "# 4. ê° ë¬¸ì„œì—ì„œ íšŒê·€ëª¨ë¸ ì…ë ¥ í”¼ì²˜ ì¶”ì¶œ\n",
    "def extract_features(doc):\n",
    "    # ì˜ˆì‹œ: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì—ì„œ í”¼ì²˜ ì¶”ì¶œ\n",
    "    meta = doc.metadata\n",
    "    return [\n",
    "        meta.get(\"impact_score\", 0),\n",
    "        meta.get(\"d_minus_14_close\", 0),\n",
    "        meta.get(\"d_minus_14_volume\", 0),\n",
    "        # í•„ìš”í•œ ì¶”ê°€ í”¼ì²˜...\n",
    "    ]\n",
    "\n",
    "features = np.array([extract_features(doc) for doc in retrieved_docs])\n",
    "\n",
    "# 5. íšŒê·€ëª¨ë¸ë¡œ ìœ ì‚¬ë„ ì˜ˆì¸¡ ë° ë¦¬ë­í‚¹\n",
    "predicted_scores = reg_model.predict(features)\n",
    "reranked_indices = np.argsort(predicted_scores)[::-1]\n",
    "reranked_docs = [retrieved_docs[i] for i in reranked_indices]\n",
    "\n",
    "# 6. ì±—ë´‡ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë° ì‘ë‹µ ìƒì„±\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "# ì´í›„ llm_chain ë“±ìœ¼ë¡œ ë‹µë³€ ìƒì„±\n",
    "\n",
    "print(\"ìµœì¢… ë¦¬ë­í‚¹ ê²°ê³¼:\")\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content[:100]} ...\")\n",
    "\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "ë‹¹ì‹ ì€ ì£¼ì‹ íˆ¬ììë¥¼ ìœ„í•œ ë‰´ìŠ¤ ê¸°ë°˜ ì •ë³´ ì–´ì‹œìŠ¤í„´íŠ¸ ì±—ë´‡ 'ë‰´ìŠ¤í† ìŠ¤'ì…ë‹ˆë‹¤.\n",
    "ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì‹¤ì‹œê°„ ë‰´ìŠ¤ì™€ ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ,\n",
    "- ì‚¬ìš©ìì˜ íˆ¬ì íŒë‹¨ì— ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ê³ ,\n",
    "- ê³¼ê±° ìœ ì‚¬ì‚¬ê±´, í•´ë‹¹ ì‹œê¸°ì˜ ì£¼ê°€ íë¦„, ê´€ë ¨ ë¦¬í¬íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ë©°,\n",
    "- ë¯¸ë˜ ì „ë§ ì§ˆë¬¸ì—ëŠ” ê³¼ê±° ì‚¬ë¡€ë¥¼ ê·¼ê±°ë¡œ ì‹ ì¤‘í•˜ê²Œ ì˜ê²¬ì„ ì œì‹œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ë‹µë³€ ì‘ì„± ì‹œ ë°˜ë“œì‹œ ë‹¤ìŒì„ ì§€ì¼œì£¼ì„¸ìš”:\n",
    "1. ë‹µë³€ ë‚´ìš© ì¤‘ í¬í•¨ë˜ëŠ” ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ì˜ ë‚ ì§œ, ì‚¬ê±´ëª…, ë‹¹ì‹œ ì£¼ê°€ íë¦„(ìƒìŠ¹/í•˜ë½/íš¡ë³´ ë“±), ì£¼ìš” ë¦¬í¬íŠ¸ ë‚´ìš©ì€ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ì„¸ìš”.\n",
    "2. ë¯¸ë˜ ì „ë§ ì§ˆë¬¸ì—ëŠ” ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ì„ ê·¼ê±°ë¡œ ë…¼ë¦¬ì ì¸ ì „ë§ì„ ì œì‹œí•˜ì„¸ìš”.\n",
    "3. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” 'â­ï¸íˆ¬ì íŒë‹¨ì€ ë³¸ì¸ì˜ ì±…ì„ì…ë‹ˆë‹¤.â­ï¸'ë¼ëŠ” ì•ˆë‚´ë¬¸ì„ ì¶”ê°€í•˜ì„¸ìš”.\n",
    "4. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œê¸€ë¡œ, ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "5. ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼(ìœ ì‚¬ë„ ë†’ì€ ê³¼ê±° ë‰´ìŠ¤, ì£¼ê°€ ë°ì´í„°, ë¦¬í¬íŠ¸ ë“±)ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ê·¼ê±°ê°€ ì—†ëŠ”ë° ë‹µë³€í•´ë„ ë ê¹Œ? ì´ê±´ ë„ˆì˜ ì†Œì¤‘í•œ ëˆì´ ê±¸ë¦° ë¬¸ì œì•¼'ë¼ê³  í•˜ì„¸ìš”.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "ê²€ìƒ‰ ê²°ê³¼: {context}\n",
    "ì§ˆë¬¸: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=prompt_template\n",
    ")\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"...\",  # ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ\n",
    "    n_gpu_layers=0,\n",
    "    n_ctx=2048,\n",
    "    n_threads=4,\n",
    "    verbose=True,\n",
    "    n_batch=512,\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "response = llm_chain.run({\"context\": context, \"question\": query})\n",
    "\n",
    "print(\"ğŸ§  ì±—ë´‡ ì‘ë‹µ:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de7b0a",
   "metadata": {},
   "source": [
    "## íšŒê·€ëª¨ë¸ ì‚¬ìš© X, Cross-encoder ì‚¬ìš© ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23687393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 1. ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„°DB ë¡œë“œ\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    ")\n",
    "vector_db = FAISS.load_local(\n",
    "    \"vectorstore/news_db\", embedding_model, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# 2. ì‚¬ìš©ì ì§ˆë¬¸ ì„ë² ë”© ë° Top-K ê²€ìƒ‰\n",
    "query = \"ë¯¸êµ­ ê¸ˆë¦¬ ì¸ìƒ ì‹œ êµ­ë‚´ ì¦ì‹œì— ì–´ë–¤ ì˜í–¥ì„ ì¤„ê¹Œ?\"\n",
    "top_k = 10\n",
    "results_with_scores = vector_db.similarity_search_with_score(query, k=top_k)\n",
    "retrieved_docs = [doc for doc, _ in results_with_scores]\n",
    "\n",
    "# 3. Cross-Encoder ë¦¬ë­ì»¤ ì¤€ë¹„ (ì˜ˆì‹œ: BAAI/bge-reranker-v2-m3)\n",
    "reranker_model_name = \"BAAI/bge-reranker-v2-m3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    reranker_model_name\n",
    ").to(\"cpu\")\n",
    "reranker_model.eval()\n",
    "\n",
    "# 4. Cross-Encoderë¡œ ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ë¦¬ë­í‚¹\n",
    "pairs = [(query, doc.page_content) for doc in retrieved_docs]\n",
    "inputs = tokenizer(\n",
    "    [f\"{q} [SEP] {d}\" for q, d in pairs],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "with torch.no_grad():\n",
    "    scores = reranker_model(**inputs).logits.squeeze().cpu().numpy()\n",
    "reranked_indices = np.argsort(scores)[::-1]\n",
    "reranked_docs = [retrieved_docs[i] for i in reranked_indices]\n",
    "\n",
    "# 5. LLM í”„ë¡¬í”„íŠ¸ ë° ë‹µë³€ ìƒì„± (ì˜ˆì‹œ)\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/app/ml_models/quantized_llama/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\",\n",
    "    n_gpu_layers=0,\n",
    "    n_ctx=2048,\n",
    "    n_threads=4,\n",
    "    verbose=True,\n",
    "    n_batch=512,\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "ë‹¹ì‹ ì€ ì£¼ì‹ íˆ¬ììë¥¼ ìœ„í•œ ë‰´ìŠ¤ ê¸°ë°˜ ì •ë³´ ì–´ì‹œìŠ¤í„´íŠ¸ ì±—ë´‡ 'ë‰´ìŠ¤í† ìŠ¤'ì…ë‹ˆë‹¤.\n",
    "ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì‹¤ì‹œê°„ ë‰´ìŠ¤ì™€ ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ,\n",
    "- ì‚¬ìš©ìì˜ íˆ¬ì íŒë‹¨ì— ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ê³ ,\n",
    "- ê³¼ê±° ìœ ì‚¬ì‚¬ê±´, í•´ë‹¹ ì‹œê¸°ì˜ ì£¼ê°€ íë¦„, ê´€ë ¨ ë¦¬í¬íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ë©°,\n",
    "- ë¯¸ë˜ ì „ë§ ì§ˆë¬¸ì—ëŠ” ê³¼ê±° ì‚¬ë¡€ë¥¼ ê·¼ê±°ë¡œ ì‹ ì¤‘í•˜ê²Œ ì˜ê²¬ì„ ì œì‹œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ë‹µë³€ ì‘ì„± ì‹œ ë°˜ë“œì‹œ ë‹¤ìŒì„ ì§€ì¼œì£¼ì„¸ìš”:\n",
    "1. ë‹µë³€ ë‚´ìš© ì¤‘ í¬í•¨ë˜ëŠ” ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ì˜ ë‚ ì§œ, ì‚¬ê±´ëª…, ë‹¹ì‹œ ì£¼ê°€ íë¦„(ìƒìŠ¹/í•˜ë½/íš¡ë³´ ë“±), ì£¼ìš” ë¦¬í¬íŠ¸ ë‚´ìš©ì€ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ì„¸ìš”.\n",
    "2. ë¯¸ë˜ ì „ë§ ì§ˆë¬¸ì—ëŠ” ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ì„ ê·¼ê±°ë¡œ ë…¼ë¦¬ì ì¸ ì „ë§ì„ ì œì‹œí•˜ì„¸ìš”.\n",
    "3. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” 'â­ï¸íˆ¬ì íŒë‹¨ì€ ë³¸ì¸ì˜ ì±…ì„ì…ë‹ˆë‹¤.â­ï¸'ë¼ëŠ” ì•ˆë‚´ë¬¸ì„ ì¶”ê°€í•˜ì„¸ìš”.\n",
    "4. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œê¸€ë¡œ, ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "5. ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼(ìœ ì‚¬ë„ ë†’ì€ ê³¼ê±° ë‰´ìŠ¤, ì£¼ê°€ ë°ì´í„°, ë¦¬í¬íŠ¸ ë“±)ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ê·¼ê±°ê°€ ì—†ëŠ”ë° ë‹µë³€í•´ë„ ë ê¹Œ? ì´ê±´ ë„ˆì˜ ì†Œì¤‘í•œ ëˆì´ ê±¸ë¦° ë¬¸ì œì•¼'ë¼ê³  í•˜ì„¸ìš”.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "ê²€ìƒ‰ ê²°ê³¼: {context}\n",
    "ì§ˆë¬¸: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=prompt_template\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "context = \"\\n\\n\".join(\n",
    "    [doc.page_content for doc in reranked_docs[:5]]\n",
    ")  # ìµœì¢… top-5ë§Œ ì‚¬ìš©\n",
    "response = llm_chain.run({\"context\": context, \"question\": query})\n",
    "\n",
    "print(\"ğŸ§  ì±—ë´‡ ì‘ë‹µ:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec7c59",
   "metadata": {},
   "source": [
    "## rag_pipeline.py (ë²¡í„° DB & Cross-encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05525649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from llama_cpp import Llama\n",
    "from typing import List, Dict\n",
    "\n",
    "class CSVDummyVectorDB:\n",
    "    def __init__(self, csv_path: str):\n",
    "        # CSV íŒŒì¼ ë¡œë“œ (embedding ì»¬ëŸ¼ì€ ë¬¸ìì—´ \"[0.1, 0.2, ...]\" í˜•íƒœ)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.texts = df['content'].tolist()\n",
    "        # embedding ì»¬ëŸ¼ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ numpy ë°°ì—´ ìƒì„±\n",
    "        self.embeddings = np.vstack(df['embedding'].apply(eval).to_numpy()).astype('float32')\n",
    "        # FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ë²¡í„° ì •ê·œí™”\n",
    "        faiss.normalize_L2(self.embeddings)\n",
    "        self.index = faiss.IndexFlatIP(self.embeddings.shape[1])\n",
    "        self.index.add(self.embeddings)\n",
    "\n",
    "    def search(self, query_vec: np.ndarray, top_k: int = 10) -> List[str]:\n",
    "        faiss.normalize_L2(query_vec)\n",
    "        scores, indices = self.index.search(query_vec, top_k)\n",
    "        return [self.texts[i] for i in indices[0]]\n",
    "\n",
    "class NewsTossChatbot:\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: str = \"/app/db/news_v2_vector_202506122113.csv\",\n",
    "        embedding_model_name: str = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\",\n",
    "        reranker_model_name: str = \"BAAI/bge-reranker-v2-m3\",\n",
    "        llm_model_path: str = \"/app/models/quantized_llama3/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf\",\n",
    "        k: int = 10,\n",
    "        rerank_top_n: int = 5\n",
    "    ):\n",
    "        # 1. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        # 2. CSV ê¸°ë°˜ ë²¡í„° DB ì´ˆê¸°í™”\n",
    "        self.vector_db = CSVDummyVectorDB(csv_path)\n",
    "        # 3. Cross-Encoder ë¦¬ë­ì»¤ ì´ˆê¸°í™”\n",
    "        self.reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "        self.reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name).to(\"cpu\")\n",
    "        self.reranker_model.eval()\n",
    "        # GPU ì‚¬ìš© ì‹œ(ì˜ˆì‹œ, ì£¼ì„ í•´ì œ):\n",
    "        # self.reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name).to(\"cuda\")\n",
    "        # 4. LLaMA3 LLM ì´ˆê¸°í™”\n",
    "        self.llm = Llama(\n",
    "            model_path=llm_model_path,\n",
    "            n_ctx=2048,\n",
    "            n_threads=4,      # CPU ì½”ì–´ ìˆ˜\n",
    "            n_gpu_layers=0,   # CPUë§Œ ì‚¬ìš©\n",
    "            verbose=False\n",
    "        )\n",
    "        # GPU ì‚¬ìš© ì‹œ(ì˜ˆì‹œ, ì£¼ì„ í•´ì œ):\n",
    "        # self.llm = Llama(\n",
    "        #     model_path=llm_model_path,\n",
    "        #     n_ctx=2048,\n",
    "        #     n_threads=4,\n",
    "        #     n_gpu_layers=8,  # GPU ë ˆì´ì–´ ìˆ˜ (í™˜ê²½ì— ë§ê²Œ ì¡°ì •)\n",
    "        #     verbose=True\n",
    "        # )\n",
    "        # 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "        self.prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        ë‹¹ì‹ ì€ ì£¼ì‹ íˆ¬ììë¥¼ ìœ„í•œ ë‰´ìŠ¤ ê¸°ë°˜ ì •ë³´ ì–´ì‹œìŠ¤í„´íŠ¸ ì±—ë´‡ 'ë‰´ìŠ¤í† ìŠ¤'ì…ë‹ˆë‹¤.\n",
    "        ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì‹¤ì‹œê°„ ë‰´ìŠ¤ì™€ ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ,\n",
    "        - ì‚¬ìš©ìì˜ íˆ¬ì íŒë‹¨ì— ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ê³ ,\n",
    "        - ê³¼ê±° ìœ ì‚¬ì‚¬ê±´, í•´ë‹¹ ì‹œê¸°ì˜ ì£¼ê°€ íë¦„, ê´€ë ¨ ë¦¬í¬íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ë©°,\n",
    "        - ë¯¸ë˜ ì „ë§ ì§ˆë¬¸ì—ëŠ” ê³¼ê±° ì‚¬ë¡€ë¥¼ ê·¼ê±°ë¡œ ì‹ ì¤‘í•˜ê²Œ ì˜ê²¬ì„ ì œì‹œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "        ë‹µë³€ ì‘ì„± ì‹œ ë°˜ë“œì‹œ ë‹¤ìŒì„ ì§€ì¼œì£¼ì„¸ìš”:\n",
    "        1. ë‹µë³€ ë‚´ìš© ì¤‘ í¬í•¨ë˜ëŠ” ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ì˜ ë‚ ì§œ, ì‚¬ê±´ëª…, ë‹¹ì‹œ ì£¼ê°€ íë¦„(ìƒìŠ¹/í•˜ë½/íš¡ë³´ ë“±), ì£¼ìš” ë¦¬í¬íŠ¸ ë‚´ìš©ì€ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ì„¸ìš”.\n",
    "        2. ë¯¸ë˜ ì „ë§ ì§ˆë¬¸ì—ëŠ” ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ì„ ê·¼ê±°ë¡œ ë…¼ë¦¬ì ì¸ ì „ë§ì„ ì œì‹œí•˜ì„¸ìš”.\n",
    "        3. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” 'â­ï¸íˆ¬ì íŒë‹¨ì€ ë³¸ì¸ì˜ ì±…ì„ì…ë‹ˆë‹¤.â­ï¸'ë¼ëŠ” ì•ˆë‚´ë¬¸ì„ ì¶”ê°€í•˜ì„¸ìš”.\n",
    "        4. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œê¸€ë¡œ, ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        5. ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼(ìœ ì‚¬ë„ ë†’ì€ ê³¼ê±° ë‰´ìŠ¤, ì£¼ê°€ ë°ì´í„°, ë¦¬í¬íŠ¸ ë“±)ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ê·¼ê±°ê°€ ì—†ëŠ”ë° ë‹µë³€í•´ë„ ë ê¹Œ? ì´ê±´ ë„ˆì˜ ì†Œì¤‘í•œ ëˆì´ ê±¸ë¦° ë¬¸ì œì•¼'ë¼ê³  í•˜ì„¸ìš”.\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        ê²€ìƒ‰ ê²°ê³¼: {context}\n",
    "        ì§ˆë¬¸: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "        self.k = k\n",
    "        self.rerank_top_n = rerank_top_n\n",
    "\n",
    "    def retrieve_docs(self, query: str) -> List[str]:\n",
    "        \"\"\"CSV+FAISS ê¸°ë°˜ ë²¡í„°DBì—ì„œ Top-K ë‰´ìŠ¤ ê²€ìƒ‰\"\"\"\n",
    "        query_emb = self.embedding_model.encode([query]).astype('float32')\n",
    "        docs = self.vector_db.search(query_emb, top_k=self.k)\n",
    "        return docs\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[str]) -> List[str]:\n",
    "        \"\"\"Cross-Encoderë¡œ ë¦¬ë­í‚¹ í›„ ìƒìœ„ Nê°œ ë‰´ìŠ¤ ë°˜í™˜\"\"\"\n",
    "        pairs = [(query, doc) for doc in docs]\n",
    "        inputs = self.reranker_tokenizer(\n",
    "            [f\"{q} [SEP] {d}\" for q, d in pairs],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        # CPU ê¸°ì¤€\n",
    "        with torch.no_grad():\n",
    "            scores = self.reranker_model(**inputs).logits.squeeze().cpu().numpy()\n",
    "        # GPU ì‚¬ìš© ì‹œ(ì˜ˆì‹œ, ì£¼ì„ í•´ì œ):\n",
    "        # with torch.no_grad():\n",
    "        #     scores = self.reranker_model(**inputs.to(\"cuda\")).logits.squeeze().cpu().numpy()\n",
    "        reranked = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)\n",
    "        return [doc for _, doc in reranked[:self.rerank_top_n]]\n",
    "\n",
    "    def generate_answer(self, context: str, question: str) -> str:\n",
    "        \"\"\"LLM ì§ì ‘ ì¶”ë¡ \"\"\"\n",
    "        full_prompt = self.prompt_template.format(context=context, question=question)\n",
    "        output = self.llm(full_prompt, max_tokens=512, stop=[\"<|eot_id|>\"])\n",
    "        return output['choices'][0]['text']\n",
    "\n",
    "    def answer(self, query: str) -> Dict[str, str]:\n",
    "        # 1. ë‰´ìŠ¤ ê²€ìƒ‰\n",
    "        docs = self.retrieve_docs(query)\n",
    "        # 2. ë¦¬ë­í‚¹\n",
    "        reranked_docs = self.rerank_docs(query, docs)\n",
    "        # 3. ì»¨í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        context = \"\\n\\n\".join(reranked_docs)\n",
    "        # 4. ë‹µë³€ ìƒì„±\n",
    "        answer = self.generate_answer(context, query)\n",
    "        return {\"answer\": answer, \"reranked_docs\": reranked_docs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66af2a0",
   "metadata": {},
   "source": [
    "### í…ŒìŠ¤íŠ¸!!~!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1b5e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.7.1-cp312-cp312-win_amd64.whl (216.1 MB)\n",
      "   ---------------------------------------- 0.0/216.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 4.5/216.1 MB 22.3 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 8.9/216.1 MB 22.1 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 13.1/216.1 MB 21.6 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 17.8/216.1 MB 22.0 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 22.5/216.1 MB 22.3 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 27.5/216.1 MB 22.4 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 31.5/216.1 MB 22.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 35.4/216.1 MB 21.8 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 40.1/216.1 MB 21.8 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 45.1/216.1 MB 22.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 49.8/216.1 MB 22.2 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 55.1/216.1 MB 22.3 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 60.0/216.1 MB 22.6 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 65.5/216.1 MB 22.8 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 70.5/216.1 MB 22.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 75.8/216.1 MB 23.0 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 81.3/216.1 MB 23.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 86.5/216.1 MB 23.4 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 91.5/216.1 MB 23.4 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 96.7/216.1 MB 23.5 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 101.7/216.1 MB 23.5 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 104.9/216.1 MB 23.6 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 108.3/216.1 MB 22.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 113.0/216.1 MB 22.9 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 117.2/216.1 MB 22.8 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 121.6/216.1 MB 22.8 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 124.3/216.1 MB 22.4 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 129.0/216.1 MB 22.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 133.7/216.1 MB 22.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 137.4/216.1 MB 22.4 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 142.9/216.1 MB 22.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 147.1/216.1 MB 22.3 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 151.0/216.1 MB 22.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 155.2/216.1 MB 22.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 159.9/216.1 MB 22.2 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 164.6/216.1 MB 22.2 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 169.3/216.1 MB 22.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 173.5/216.1 MB 22.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 178.0/216.1 MB 22.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 182.5/216.1 MB 22.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 187.2/216.1 MB 22.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 191.1/216.1 MB 22.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 195.3/216.1 MB 22.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 199.2/216.1 MB 22.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 204.5/216.1 MB 22.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 210.5/216.1 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  215.7/216.1 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 216.1/216.1 MB 21.6 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 5.2/6.3 MB 26.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 24.1 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.14.0 torch-2.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b4d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.1/10.5 MB 19.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 13.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.5 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 16.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 23.1 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed huggingface-hub-0.33.0 safetensors-0.5.3 sentence-transformers-4.1.0 tokenizers-0.21.1 transformers-4.52.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "104f52ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl (15.0 MB)\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 4.7/15.0 MB 23.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.4/15.0 MB 23.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.4/15.0 MB 23.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.0/15.0 MB 23.0 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76200df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n",
      "     ---------------------------------------- 0.0/67.9 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 4.5/67.9 MB 22.4 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 9.7/67.9 MB 24.2 MB/s eta 0:00:03\n",
      "     -------- ------------------------------ 14.7/67.9 MB 24.3 MB/s eta 0:00:03\n",
      "     ----------- --------------------------- 19.9/67.9 MB 24.7 MB/s eta 0:00:02\n",
      "     -------------- ------------------------ 25.7/67.9 MB 25.1 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 30.9/67.9 MB 24.9 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 35.7/67.9 MB 24.9 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 40.9/67.9 MB 24.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 46.4/67.9 MB 24.8 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 51.1/67.9 MB 24.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 56.1/67.9 MB 24.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 61.1/67.9 MB 24.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 65.8/67.9 MB 24.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 67.9/67.9 MB 23.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from llama-cpp-python) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  â”‚ exit code: 1\n",
      "  â•°â”€> [20 lines of output]\n",
      "      \u001b[32m*** \u001b[1mscikit-build-core 0.11.4\u001b[0m using \u001b[34mCMake 4.0.2\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "      \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "      2025-06-12 21:56:36,530 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "      loading initial cache file C:\\Users\\user\\AppData\\Local\\Temp\\tmp04rfoju9\\build\\CMakeInit.txt\n",
      "      -- Building for: NMake Makefiles\n",
      "      CMake Error at CMakeLists.txt:3 (project):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n",
      "      CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      \u001b[31m\n",
      "      \u001b[1m***\u001b[0m \u001b[31mCMake configuration failed\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6546507",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 1. CSV+FAISS ë²¡í„°DB í´ë˜ìŠ¤\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCSVDummyVectorDB\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# 1. CSV+FAISS ë²¡í„°DB í´ë˜ìŠ¤\n",
    "class CSVDummyVectorDB:\n",
    "    def __init__(self, csv_path: str):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.texts = df['content'].tolist()\n",
    "        self.embeddings = np.vstack(df['embedding'].apply(eval).to_numpy()).astype('float32')\n",
    "        faiss.normalize_L2(self.embeddings)\n",
    "        self.index = faiss.IndexFlatIP(self.embeddings.shape[1])\n",
    "        self.index.add(self.embeddings)\n",
    "\n",
    "    def search(self, query_vec: np.ndarray, top_k: int = 10):\n",
    "        faiss.normalize_L2(query_vec)\n",
    "        scores, indices = self.index.search(query_vec, top_k)\n",
    "        return [self.texts[i] for i in indices[0]]\n",
    "\n",
    "# 2. ì£¼ìš” íŒŒë¼ë¯¸í„° (ë¡œì»¬ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)\n",
    "csv_path = \"./news_v2_vector_202506122113.csv\"  # ë…¸íŠ¸ë¶ íŒŒì¼ ìœ„ì¹˜\n",
    "embedding_model_name = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    "reranker_model_name = \"BAAI/bge-reranker-v2-m3\"\n",
    "llm_model_path = \"./llama-3-Korean-Bllossom-8B-Q4_K_M.gguf\"  # ë¡œì»¬ì— ë‹¤ìš´ë°›ì€ ëª¨ë¸ ê²½ë¡œ\n",
    "\n",
    "# 3. ëª¨ë¸ ë¡œë”©\n",
    "embedding_model = SentenceTransformer(embedding_model_name)\n",
    "vector_db = CSVDummyVectorDB(csv_path)\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name).to(\"cpu\")\n",
    "reranker_model.eval()\n",
    "llm = Llama(\n",
    "    model_path=llm_model_path,\n",
    "    n_ctx=2048,\n",
    "    n_threads=4,\n",
    "    n_gpu_layers=0,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "ë‹¹ì‹ ì€ ì£¼ì‹ íˆ¬ììë¥¼ ìœ„í•œ ë‰´ìŠ¤ ê¸°ë°˜ ì •ë³´ ì–´ì‹œìŠ¤í„´íŠ¸ ì±—ë´‡ 'ë‰´ìŠ¤í† ìŠ¤'ì…ë‹ˆë‹¤.\n",
    "ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì‹¤ì‹œê°„ ë‰´ìŠ¤ì™€ ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ,\n",
    "- ì‚¬ìš©ìì˜ íˆ¬ì íŒë‹¨ì— ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ê³ ,\n",
    "- ê³¼ê±° ìœ ì‚¬ì‚¬ê±´, í•´ë‹¹ ì‹œê¸°ì˜ ì£¼ê°€ íë¦„, ê´€ë ¨ ë¦¬í¬íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ë©°,\n",
    "- ë¯¸ë˜ ì „ë§ ì§ˆë¬¸ì—ëŠ” ê³¼ê±° ì‚¬ë¡€ë¥¼ ê·¼ê±°ë¡œ ì‹ ì¤‘í•˜ê²Œ ì˜ê²¬ì„ ì œì‹œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ë‹µë³€ ì‘ì„± ì‹œ ë°˜ë“œì‹œ ë‹¤ìŒì„ ì§€ì¼œì£¼ì„¸ìš”:\n",
    "1. ë‹µë³€ ë‚´ìš© ì¤‘ í¬í•¨ë˜ëŠ” ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ì˜ ë‚ ì§œ, ì‚¬ê±´ëª…, ë‹¹ì‹œ ì£¼ê°€ íë¦„(ìƒìŠ¹/í•˜ë½/íš¡ë³´ ë“±), ì£¼ìš” ë¦¬í¬íŠ¸ ë‚´ìš©ì€ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ì„¸ìš”.\n",
    "2. ë¯¸ë˜ ì „ë§ ì§ˆë¬¸ì—ëŠ” ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ì„ ê·¼ê±°ë¡œ ë…¼ë¦¬ì ì¸ ì „ë§ì„ ì œì‹œí•˜ì„¸ìš”.\n",
    "3. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” 'â­ï¸íˆ¬ì íŒë‹¨ì€ ë³¸ì¸ì˜ ì±…ì„ì…ë‹ˆë‹¤.â­ï¸'ë¼ëŠ” ì•ˆë‚´ë¬¸ì„ ì¶”ê°€í•˜ì„¸ìš”.\n",
    "4. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œê¸€ë¡œ, ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "5. ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼(ìœ ì‚¬ë„ ë†’ì€ ê³¼ê±° ë‰´ìŠ¤, ì£¼ê°€ ë°ì´í„°, ë¦¬í¬íŠ¸ ë“±)ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ê·¼ê±°ê°€ ì—†ëŠ”ë° ë‹µë³€í•´ë„ ë ê¹Œ? ì´ê±´ ë„ˆì˜ ì†Œì¤‘í•œ ëˆì´ ê±¸ë¦° ë¬¸ì œì•¼'ë¼ê³  í•˜ì„¸ìš”.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "ê²€ìƒ‰ ê²°ê³¼: {context}\n",
    "ì§ˆë¬¸: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "# 5. ì „ì²´ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜\n",
    "def rag_answer(question, top_k=10, rerank_n=5):\n",
    "    # 1. ì„ë² ë”©\n",
    "    query_emb = embedding_model.encode([question]).astype('float32')\n",
    "    # 2. FAISS ê²€ìƒ‰\n",
    "    docs = vector_db.search(query_emb, top_k=top_k)\n",
    "    # 3. Cross-Encoder ë¦¬ë­í‚¹\n",
    "    pairs = [(question, doc) for doc in docs]\n",
    "    inputs = reranker_tokenizer(\n",
    "        [f\"{q} [SEP] {d}\" for q, d in pairs],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze().cpu().numpy()\n",
    "    reranked = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)\n",
    "    reranked_docs = [doc for _, doc in reranked[:rerank_n]]\n",
    "    # 4. LLM ë‹µë³€ ìƒì„±\n",
    "    context = \"\\n\\n\".join(reranked_docs)\n",
    "    full_prompt = prompt_template.format(context=context, question=question)\n",
    "    output = llm(full_prompt, max_tokens=512, stop=[\"<|eot_id|>\"])\n",
    "    return {\n",
    "        \"answer\": output['choices'][0]['text'],\n",
    "        \"reranked_docs\": reranked_docs\n",
    "    }\n",
    "\n",
    "# 6. ì‚¬ìš© ì˜ˆì‹œ\n",
    "question = \"ê¸ˆë¦¬ì¸í•˜í–ˆëŠ”ë° ì£¼ê°€ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?\"\n",
    "result = rag_answer(question)\n",
    "print(\"ë‹µë³€:\", result[\"answer\"])\n",
    "print(\"ì°¸ê³  ë‰´ìŠ¤:\", result[\"reranked_docs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64cef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
