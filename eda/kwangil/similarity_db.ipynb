{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7424775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_20888\\3322154953.py:19: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "# db.py\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# 환경변수에서 DATABASE_URL 가져오고, 없으면 로컬 기본값 사용\n",
    "DATABASE_URL = os.getenv(\n",
    "    \"DATABASE_URL\", \"postgresql://postgres:password@localhost:5432/postgres\"\n",
    ")\n",
    "\n",
    "# SQLAlchemy 엔진 생성\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# 세션 팩토리\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "# Base 클래스\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04ffb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 연결된 DB URL: postgresql://postgres:***@localhost:5432/postgres\n",
      "[2025-05-23 18:52:00] 20250523_0002 | [단독] 카카오페이, 2500만 회원 쓱·스마일페이 품나…간편결제 시장 빅3 경쟁 후끈\n",
      "[2025-05-23 18:33:00] 20250523_0004 | 골드만삭스 차기 CEO, 이재용·김병주·이창용 만났다\n",
      "[2025-05-23 18:00:00] 20250523_0007 | [단독] 전자결제 강자 카카오페이 쓱·스마일 페이 인수 추진\n",
      "[2025-05-23 17:52:00] 20250523_0010 | 조현준 효성重 지분 4.9% 美 테크펀드 2600억 매각\n",
      "[2025-05-23 17:52:00] 20250523_0011 | 몸집 키우는 카카오…'간편결제 빅3' 흔드나\n"
     ]
    }
   ],
   "source": [
    "from fastapi.models.news import NewsModel_v2\n",
    "# main.py 또는 Jupyter Notebook에서 실행\n",
    "\n",
    "# ✅ DB URL 확인\n",
    "print(\"🔗 연결된 DB URL:\", engine.url)\n",
    "\n",
    "# ✅ 세션 생성\n",
    "db = SessionLocal()\n",
    "\n",
    "# ✅ 뉴스 5건 조회\n",
    "results = db.query(NewsModel_v2).limit(5).all()\n",
    "\n",
    "# ✅ 결과 출력\n",
    "for row in results:\n",
    "    print(f\"[{row.wdate}] {row.news_id} | {row.title}\")\n",
    "\n",
    "# ✅ 세션 종료\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa9f9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84be7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_news_embeddings(article_list, tokenizer, session):\n",
    "    \"\"\"\n",
    "    뉴스 본문 리스트를 임베딩하는 함수입니다.\n",
    "    ONNX 모델이 배치 입력을 지원할 경우, 한 번에 추론합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 토큰화\n",
    "    encoded = [tokenizer.encode(x) for x in article_list]\n",
    "    input_ids = [e.ids for e in encoded]\n",
    "    attention_mask = [[1] * len(ids) for ids in input_ids]\n",
    "\n",
    "    # 2. 패딩 (최대 길이 기준)\n",
    "    max_len = max(len(ids) for ids in input_ids)\n",
    "    input_ids_padded = [ids + [0] * (max_len - len(ids)) for ids in input_ids]\n",
    "    attention_mask_padded = [\n",
    "        mask + [0] * (max_len - len(mask)) for mask in attention_mask\n",
    "    ]\n",
    "\n",
    "    # 3. numpy 배열로 변환\n",
    "    input_ids_np = np.array(input_ids_padded, dtype=np.int64)\n",
    "    attention_mask_np = np.array(attention_mask_padded, dtype=np.int64)\n",
    "\n",
    "    # 4. ONNX 추론\n",
    "    outputs = session.run(\n",
    "        [\"sentence_embedding\"],\n",
    "        {\"input_ids\": input_ids_np, \"attention_mask\": attention_mask_np},\n",
    "    )[\n",
    "        0\n",
    "    ]  # shape: (batch_size, hidden_dim)\n",
    "\n",
    "    # 5. 반환 (List[List[float]])\n",
    "    return outputs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313fd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from requests import Session\n",
    "\n",
    "\n",
    "def scale_ext_grouped(\n",
    "    ext: list, col_names: list, prefix: str, scalers: dict, group_key_map: dict\n",
    "):\n",
    "    grouped_data = {}\n",
    "    grouped_indices = {}\n",
    "    for idx, (col, val) in enumerate(zip(col_names, ext)):\n",
    "        group = group_key_map.get(col, None)\n",
    "        if group:\n",
    "            key = f\"{prefix}_{group}\"\n",
    "            grouped_data.setdefault(key, []).append(val)\n",
    "            grouped_indices.setdefault(key, []).append(idx)\n",
    "\n",
    "    scaled = ext.copy()\n",
    "    for key in grouped_data:\n",
    "        if key in scalers:\n",
    "            try:\n",
    "                values = np.array(grouped_data[key], dtype=np.float32).reshape(1, -1)\n",
    "                # transformed = scalers[key].transform(values)[0]\n",
    "\n",
    "                columns = scalers[key].feature_names_in_  # sklearn >=1.0\n",
    "                values_df = pd.DataFrame(values, columns=columns)\n",
    "                transformed = scalers[key].transform(values_df)[0]\n",
    "\n",
    "                for idx, val in zip(grouped_indices[key], transformed):\n",
    "                    scaled[idx] = val\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {key} 스케일 실패: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(f\"⚠️ {key} 스케일러 없음 → 원본 사용\")\n",
    "\n",
    "    return np.array(scaled, dtype=np.float32)\n",
    "\n",
    "\n",
    "def run_ae(ae_sess, embedding):\n",
    "    input_name = ae_sess.get_inputs()[0].name\n",
    "    output_name = ae_sess.get_outputs()[0].name\n",
    "    return ae_sess.run([output_name], {input_name: embedding.astype(np.float32)})[0]\n",
    "\n",
    "\n",
    "async def compute_similarity(\n",
    "    db: Session,\n",
    "    summary: str,\n",
    "    extA: list,\n",
    "    topicA: list,\n",
    "    similar_summaries: list,\n",
    "    extBs: list,\n",
    "    topicBs: list,\n",
    "    scalers,\n",
    "    ae_sess,\n",
    "    regressor_sess,\n",
    "    embedding_api_func,\n",
    "    ext_col_names: list,\n",
    "    topic_col_names: list,\n",
    "    news_topk_ids: list,\n",
    "):\n",
    "\n",
    "    # group_key_map 생성 (기준 + 유사 뉴스)\n",
    "    group_key_map = {}\n",
    "    for col in ext_col_names + topic_col_names:\n",
    "        if \"date_close\" in col:\n",
    "            group_key_map[col] = \"price_close\"\n",
    "        elif \"date_volume\" in col:\n",
    "            group_key_map[col] = \"volume\"\n",
    "        elif \"date_foreign\" in col:\n",
    "            group_key_map[col] = \"foreign\"\n",
    "        elif \"date_institution\" in col:\n",
    "            group_key_map[col] = \"institution\"\n",
    "        elif \"date_individual\" in col:\n",
    "            group_key_map[col] = \"individual\"\n",
    "        elif col in [\"fx\", \"bond10y\", \"base_rate\"]:\n",
    "            group_key_map[col] = \"macro\"\n",
    "        elif \"토픽\" in col:\n",
    "            group_key_map[col] = \"topic\"\n",
    "\n",
    "    for col in ext_col_names + topic_col_names:\n",
    "        col_sim = f\"similar_{col}\"\n",
    "        if \"date_close\" in col:\n",
    "            group_key_map[col_sim] = \"price_close\"\n",
    "        elif \"date_volume\" in col:\n",
    "            group_key_map[col_sim] = \"volume\"\n",
    "        elif \"date_foreign\" in col:\n",
    "            group_key_map[col_sim] = \"foreign\"\n",
    "        elif \"date_institution\" in col:\n",
    "            group_key_map[col_sim] = \"institution\"\n",
    "        elif \"date_individual\" in col:\n",
    "            group_key_map[col_sim] = \"individual\"\n",
    "        elif col in [\"fx\", \"bond10y\", \"base_rate\"]:\n",
    "            group_key_map[col_sim] = \"macro\"\n",
    "        elif \"토픽\" in col:\n",
    "            group_key_map[col_sim] = \"topic\"\n",
    "\n",
    "    # 텍스트 임베딩 + AE 인코딩\n",
    "    all_texts = [summary] + similar_summaries\n",
    "    embeddings = np.array(await embedding_api_func(all_texts))\n",
    "\n",
    "    embA, embBs = embeddings[0], embeddings[1:]\n",
    "    latentA = run_ae(ae_sess, embA.reshape(1, -1))[0]\n",
    "    latentBs = [run_ae(ae_sess, e.reshape(1, -1))[0] for e in embBs]\n",
    "\n",
    "    # 스케일링\n",
    "    extA_total = extA + topicA\n",
    "    extA_col_names = ext_col_names + topic_col_names\n",
    "    extA_scaled = scale_ext_grouped(\n",
    "        extA_total, extA_col_names, \"extA\", scalers, group_key_map\n",
    "    )\n",
    "\n",
    "    extB_total = [ext + topic for ext, topic in zip(extBs, topicBs)]\n",
    "    extB_col_names = [f\"similar_{col}\" for col in ext_col_names + topic_col_names]\n",
    "    extBs_scaled = [\n",
    "        scale_ext_grouped(extB, extB_col_names, \"extB_similar\", scalers, group_key_map)\n",
    "        for extB in extB_total\n",
    "    ]\n",
    "\n",
    "    # 회귀 예측\n",
    "    inputA_name = regressor_sess.get_inputs()[0].name\n",
    "    inputB_name = regressor_sess.get_inputs()[1].name\n",
    "    output_name = regressor_sess.get_outputs()[0].name\n",
    "\n",
    "    scores = []\n",
    "    for i, (latentB, extB_scaled) in enumerate(zip(latentBs, extBs_scaled)):\n",
    "        if extB_scaled.shape[0] != 42:\n",
    "            raise ValueError(\n",
    "                f\"extB_scaled 길이 이상함! 기대: 42, 실제: {extB_scaled.shape[0]} | index: {i}\"\n",
    "            )\n",
    "\n",
    "        featA = np.concatenate([latentA, extA_scaled]).reshape(1, -1).astype(np.float32)\n",
    "        featB = np.concatenate([latentB, extB_scaled]).reshape(1, -1).astype(np.float32)\n",
    "\n",
    "        score = regressor_sess.run(\n",
    "            [output_name], {inputA_name: featA, inputB_name: featB}\n",
    "        )[0][0][0]\n",
    "        scores.append(score)\n",
    "\n",
    "    # 결과 반환\n",
    "    results = list(zip(similar_summaries, scores, news_topk_ids))\n",
    "    results.sort(key=lambda x: -x[1])  # score 기준 내림차순 정렬\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"news_id\": nid,\n",
    "            \"summary\": summ,\n",
    "            \"wdate\": \"\",\n",
    "            \"score\": float(score),\n",
    "            \"rank\": i + 1,\n",
    "        }\n",
    "        for i, (summ, score, nid) in enumerate(results)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f67e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# fastapi 폴더가 있는 디렉토리 절대경로를 sys.path에 추가\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))  # notebooks의 상위\n",
    "sys.path.append(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06c8e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    "model_emb = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "928f5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import joblib\n",
    "from tokenizers import Tokenizer\n",
    "import onnxruntime as ort\n",
    "\n",
    "from modelapi.load_models import load_scalers_by_group\n",
    "\n",
    "\n",
    "def get_embedding_tokenizer():\n",
    "    \"\"\"\n",
    "    ONNX NER 모델과 토크나이저 로딩\n",
    "    \"\"\"\n",
    "    base_path = Path(\"../../modelapi/models/kr_sbert_mean_onnx\")\n",
    "\n",
    "    tokenizer = Tokenizer.from_file(str(base_path / \"tokenizer.json\"))\n",
    "    session = ort.InferenceSession(str(base_path / \"kr_sbert.onnx\"))\n",
    "\n",
    "    return tokenizer, session\n",
    "\n",
    "\n",
    "def get_similarity_model():\n",
    "    model_dir = \"../../modelapi/models/\"\n",
    "    scaler_dir = os.path.join(model_dir, \"scalers_grouped\")\n",
    "    ae_path = os.path.join(model_dir, \"ae_encoder.onnx\")\n",
    "    regressor_path = os.path.join(model_dir, \"similarity_ranker.onnx\")\n",
    "\n",
    "    # ONNX 모델 로딩\n",
    "    ae_sess = ort.InferenceSession(ae_path)\n",
    "    regressor_sess = ort.InferenceSession(regressor_path)\n",
    "\n",
    "    # 스케일러 로딩\n",
    "    scalers = load_scalers_by_group(scaler_dir)\n",
    "\n",
    "    return scalers, ae_sess, regressor_sess\n",
    "\n",
    "\n",
    "def load_scalers_by_group(folder_path):\n",
    "    scalers = {}\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".joblib\"):\n",
    "            key = filename.replace(\".joblib\", \"\")\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            obj = joblib.load(full_path)\n",
    "\n",
    "            # 버전 정보 포함된 dict일 경우 대응\n",
    "            if isinstance(obj, dict) and \"scaler\" in obj:\n",
    "                scalers[key] = obj[\"scaler\"]\n",
    "            else:\n",
    "                scalers[key] = obj\n",
    "\n",
    "    return scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27be3252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RobustScaler from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from http.client import HTTPException\n",
    "from fastapi.models.news import NewsModel_v2_External, NewsModel_v2_Metadata\n",
    "from modelapi.models.custom import NewsModel_v2_Topic\n",
    "import onnxruntime as ort\n",
    "\n",
    "scalers, ae_sess, regressor_sess = get_similarity_model()\n",
    "tokenizer_embedding, session_embedding = get_embedding_tokenizer()\n",
    "\n",
    "\n",
    "async def get_similar_news(payload, db):\n",
    "    # 로드된 모델 가져오기\n",
    "\n",
    "    async def embedding_api_func(texts):\n",
    "        # embeddings = await get_news_embeddings(texts, tokenizer_embedding, session_embedding)\n",
    "        embeddings = model_emb.encode(texts)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    news_id = payload['news_id']\n",
    "    news_topk_ids = payload['news_topk_ids'] or []\n",
    "\n",
    "    # 공통 외부변수 컬럼 정의 (news_id 제외 전부)\n",
    "    ext_cols = [\n",
    "        col.name\n",
    "        for col in NewsModel_v2_External.__table__.columns\n",
    "        if col.name != \"news_id\"\n",
    "    ]\n",
    "\n",
    "    # 기준 뉴스 정보 조회\n",
    "    ref_news_raw = (\n",
    "        db.query(NewsModel_v2_Metadata)\n",
    "        .filter(NewsModel_v2_Metadata.news_id == news_id)\n",
    "        .first()\n",
    "    )\n",
    "    if not ref_news_raw:\n",
    "        raise HTTPException(\n",
    "            status_code=404, detail=\"기준 뉴스 정보를 찾을 수 없습니다.\"\n",
    "        )\n",
    "    summary = ref_news_raw.summary\n",
    "\n",
    "    ref_news_external = (\n",
    "        db.query(NewsModel_v2_External)\n",
    "        .filter(NewsModel_v2_External.news_id == news_id)\n",
    "        .first()\n",
    "    )\n",
    "    if not ref_news_external:\n",
    "        raise HTTPException(\n",
    "            status_code=404, detail=\"기준 뉴스 외부 변수 정보를 찾을 수 없습니다.\"\n",
    "        )\n",
    "    extA = [getattr(ref_news_external, col) for col in ext_cols]\n",
    "\n",
    "    ref_news_topic = (\n",
    "        db.query(NewsModel_v2_Topic)\n",
    "        .filter(NewsModel_v2_Topic.news_id == news_id)\n",
    "        .first()\n",
    "    )\n",
    "    if not ref_news_topic:\n",
    "        raise HTTPException(\n",
    "            status_code=404, detail=\"기준 뉴스 토픽 정보를 찾을 수 없습니다.\"\n",
    "        )\n",
    "    topic_cols = [\n",
    "        col.name\n",
    "        for col in ref_news_topic.__table__.columns\n",
    "        if col.name.startswith(\"topic_\")\n",
    "    ]\n",
    "    topicA = [getattr(ref_news_topic, col) for col in topic_cols]\n",
    "\n",
    "    extA_total = extA + topicA\n",
    "\n",
    "    # 유사 뉴스 정보 조회\n",
    "    topk_news_raw = (\n",
    "        db.query(NewsModel_v2_Metadata)\n",
    "        .filter(NewsModel_v2_Metadata.news_id.in_(news_topk_ids))\n",
    "        .all()\n",
    "    )\n",
    "    summary_map = {news.news_id: news.summary for news in topk_news_raw}\n",
    "    try:\n",
    "        similar_summaries = [summary_map[nid] for nid in news_topk_ids]\n",
    "    except KeyError as e:\n",
    "        raise HTTPException(\n",
    "            status_code=400, detail=f\"유사 뉴스 ID {str(e)}가 DB에 존재하지 않습니다.\"\n",
    "        )\n",
    "\n",
    "    topk_exts = (\n",
    "        db.query(NewsModel_v2_External)\n",
    "        .filter(NewsModel_v2_External.news_id.in_(news_topk_ids))\n",
    "        .all()\n",
    "    )\n",
    "    ext_map = {ext.news_id: ext for ext in topk_exts}\n",
    "    extBs = [[getattr(ext_map[nid], col) for col in ext_cols] for nid in news_topk_ids]\n",
    "\n",
    "    topk_topics = (\n",
    "        db.query(NewsModel_v2_Topic)\n",
    "        .filter(NewsModel_v2_Topic.news_id.in_(news_topk_ids))\n",
    "        .all()\n",
    "    )\n",
    "    topic_map = {topic.news_id: topic for topic in topk_topics}\n",
    "    topicB_cols = [\n",
    "        col.name\n",
    "        for col in NewsModel_v2_Topic.__table__.columns\n",
    "        if col.name.startswith(\"topic_\")\n",
    "    ]\n",
    "    topicBs = [\n",
    "        [getattr(topic_map[nid], col) for col in topicB_cols] for nid in news_topk_ids\n",
    "    ]\n",
    "\n",
    "    extB_total = [ext + topic for ext, topic in zip(extBs, topicBs)]\n",
    "\n",
    "    missing_ext_ids = [nid for nid in news_topk_ids if nid not in ext_map]\n",
    "    missing_topic_ids = [nid for nid in news_topk_ids if nid not in topic_map]\n",
    "\n",
    "    if missing_ext_ids:\n",
    "        raise HTTPException(\n",
    "            status_code=400, detail=f\"외부 변수 없는 뉴스 ID: {missing_ext_ids}\"\n",
    "        )\n",
    "    if missing_topic_ids:\n",
    "        raise HTTPException(\n",
    "            status_code=400, detail=f\"토픽 변수 없는 뉴스 ID: {missing_topic_ids}\"\n",
    "        )\n",
    "\n",
    "    # 유사도 점수 계산\n",
    "    results = await compute_similarity(\n",
    "        db=db,\n",
    "        summary=summary,\n",
    "        extA=extA,\n",
    "        topicA=topicA,\n",
    "        similar_summaries=similar_summaries,\n",
    "        extBs=extBs,\n",
    "        topicBs=topicBs,\n",
    "        scalers=scalers,\n",
    "        ae_sess=ae_sess,\n",
    "        regressor_sess=regressor_sess,\n",
    "        embedding_api_func=embedding_api_func,\n",
    "        ext_col_names=ext_cols,\n",
    "        topic_col_names=topic_cols,\n",
    "        news_topk_ids=news_topk_ids,\n",
    "    )\n",
    "\n",
    "    # news_id 매핑\n",
    "    news_id_map = dict(zip(similar_summaries, news_topk_ids))\n",
    "    for r in results:\n",
    "        r[\"news_id\"] = news_id_map.get(r[\"summary\"], \"unknown\")\n",
    "\n",
    "    # 유사도 score 기준 정렬\n",
    "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8976aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "payload = {\n",
    "    'news_id': \"20250523_0002\",\n",
    "    'news_topk_ids': ['20250523_0002']\n",
    "}\n",
    "test = await get_similar_news(payload, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e50cca58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'news_id': '20250523_0002',\n",
       "  'summary': '23일 정보기술(IT)·투자은행(IB) 업계에 따르면 국내 대표 전자결제사업자인 카카오페이가 SSG닷컴 쓱페이와 G마켓 스마일페이 인수를 위해 신세계이마트 측과 협상을 진행 중인 것으로 알려졌다.',\n",
       "  'wdate': '',\n",
       "  'score': 0.6836361885070801,\n",
       "  'rank': 1}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54f9c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Embeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "def get_vectordb():\n",
    "    \"\"\"\n",
    "    vectordb 로딩\n",
    "    \"\"\"\n",
    "\n",
    "    class OnnxEmbedder(Embeddings):\n",
    "        def __init__(self, model_path: str, tokenizer_path: str):\n",
    "            self.session = ort.InferenceSession(model_path)\n",
    "            self.tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "        def _embed(self, text: str):\n",
    "            encoding = self.tokenizer.encode(text)\n",
    "            input_ids = np.array([encoding.ids], dtype=np.int64)\n",
    "            attention_mask = np.array([[1] * len(encoding.ids)], dtype=np.int64)\n",
    "\n",
    "            outputs = self.session.run(\n",
    "                None, {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "            )\n",
    "\n",
    "            raw_vector = outputs[0][0]\n",
    "            norm_vector = raw_vector / (np.linalg.norm(raw_vector) + 1e-10)\n",
    "            return norm_vector.tolist()\n",
    "\n",
    "        def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "            return [self._embed(text) for text in texts]\n",
    "\n",
    "        def embed_query(self, text: str) -> list[float]:\n",
    "            return self._embed(text)\n",
    "\n",
    "    model_base_path = Path(\"../../modelapi/models\")\n",
    "\n",
    "    embedding = OnnxEmbedder(\n",
    "        model_path=str(model_base_path / \"kr_sbert_mean_onnx/kr_sbert.onnx\"),\n",
    "        tokenizer_path=str(model_base_path / \"kr_sbert_mean_onnx/tokenizer.json\"),\n",
    "    )\n",
    "\n",
    "    db_base_path = Path(\"../../modelapi/db\")\n",
    "\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=str(db_base_path / \"chroma_store\"),\n",
    "        embedding_function=embedding,\n",
    "    )\n",
    "\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e5efb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = get_vectordb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8971ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "def safe_parse_list(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except Exception:\n",
    "            return []\n",
    "    return val if isinstance(val, list) else []\n",
    "\n",
    "\n",
    "def get_news_similar_list(payload, vectordb):\n",
    "    \"\"\"\n",
    "    유사 뉴스 top_k\n",
    "    \"\"\"\n",
    "\n",
    "    article = payload['article']\n",
    "    top_k = payload['top_k']\n",
    "\n",
    "\n",
    "    # 검색\n",
    "    results = vectordb.similarity_search_with_score(article, k=100)\n",
    "\n",
    "    news_similar_list = []\n",
    "    seen_titles = set()\n",
    "\n",
    "    for doc, score in results:\n",
    "        similarity = round(1 - float(score), 2)\n",
    "\n",
    "        if similarity > 0.9:\n",
    "            continue  # 유사도가 너무 높으면 제외 (0.9 이상 필터링)\n",
    "\n",
    "        title = doc.metadata.get(\"title\")\n",
    "        if title in seen_titles:\n",
    "            continue  # 이미 추가한 title이면 스킵\n",
    "        seen_titles.add(title)\n",
    "\n",
    "        news_id = doc.metadata.get(\"news_id\")\n",
    "        wdate = doc.metadata.get(\"wdate\")\n",
    "        summary = doc.page_content\n",
    "        url = doc.metadata.get(\"url\")\n",
    "        image = doc.metadata.get(\"image\")\n",
    "        stock_list = safe_parse_list(doc.metadata.get(\"stock_list\"))\n",
    "        industry_list = safe_parse_list(doc.metadata.get(\"industry_list\"))\n",
    "\n",
    "        if not news_id or not wdate or not summary:\n",
    "            continue\n",
    "\n",
    "        news_similar_list.append(\n",
    "            {\n",
    "                'news_id': news_id,\n",
    "                'wdate': wdate,\n",
    "                'title': title,\n",
    "                'summary': summary,\n",
    "                'url': url,\n",
    "                'image': image,\n",
    "                'stock_list': stock_list,\n",
    "                'industry_list': industry_list,\n",
    "                'similarity': similarity,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return news_similar_list[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92b332d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload2 = {\n",
    "    \"article\": \"'범용인공지능(AGI) 칩 생산이 가능한 파운드리 생태계를 확보한 삼성전자는 메모리와 함께 턴키 공급이 가능한 유일한 업체로 고객사로부터 긍정적 요소로 작용할 전망이다.\",\n",
    "    \"top_k\": 5,\n",
    "}\n",
    "top_k = get_news_similar_list(payload2, vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d330dc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'news_id': '20250123_0190',\n",
       "  'wdate': '2025-01-23 05:01:00',\n",
       "  'title': 'SK하이닉스, 오늘 4분기 성적표 공개…영업이익 8조 넘을까',\n",
       "  'summary': '범용(레거시) 메모리 업황 부진에도 고부가 제품인 고대역폭 메모리(HBM) 경쟁력을 내세워 사상 최대 실적 기록을 새로 쓸 것으로 기대되는 SK하이닉스의 HBM 시장 우위는 당분간 이어질 전망이다.',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/001/0015175914',\n",
       "  'image': 'https://imgnews.pstatic.net/image/001/2025/01/23/PYH2024102411930001300_P4_20250123050115442.jpg?type=w800',\n",
       "  'stock_list': [{'stock_id': '000660', 'stock_name': 'SK하이닉스'}],\n",
       "  'industry_list': [{'stock_id': '000660',\n",
       "    'industry_id': '32601',\n",
       "    'industry_name': '반도체 제조업'}],\n",
       "  'similarity': 0.52},\n",
       " {'news_id': '20240814_0485',\n",
       "  'wdate': '2024-08-14 07:39:00',\n",
       "  'title': '[클릭 e종목]\"삼성전자, 올해는 HBM보다 일반 D램으로 실적 늘 것\"',\n",
       "  'summary': '한국투자증권은 14일 삼성전자에 대해 올해까지는 고대역폭메모리(HBM)보다 일반 D램에 의한 실적 증가가 있을 것으로 내다봤다.',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/277/0005458839',\n",
       "  'image': 'https://imgnews.pstatic.net/image/277/2024/08/14/0005458839_001_20240814074014958.jpg?type=w800',\n",
       "  'stock_list': [{'stock_id': '005930', 'stock_name': '삼성전자'}],\n",
       "  'industry_list': [{'stock_id': '005930',\n",
       "    'industry_id': '32604',\n",
       "    'industry_name': '통신 및 방송 장비 제조업'}],\n",
       "  'similarity': 0.51},\n",
       " {'news_id': '20240708_0133',\n",
       "  'wdate': '2024-07-08 10:22:00',\n",
       "  'title': \"'12만 전자' 가나…증권가, 삼전 호실적에 목표가 줄상향\",\n",
       "  'summary': '삼성전자는 삼성전자의 호실적이 D램 등 범용 메모리의 판가 상승에서 왔다고 분석하며 고대역폭메모리(HBM)에서의 성과가 향후 밸류에이션을 결정지을 것으로 내다봤다.',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/011/0004363260',\n",
       "  'image': 'https://imgnews.pstatic.net/image/011/2024/07/08/0004363260_001_20240708114707845.jpg?type=w800',\n",
       "  'stock_list': [{'stock_id': '005930', 'stock_name': '삼성전자'}],\n",
       "  'industry_list': [{'stock_id': '005930',\n",
       "    'industry_id': '32604',\n",
       "    'industry_name': '통신 및 방송 장비 제조업'}],\n",
       "  'similarity': 0.49},\n",
       " {'news_id': '20240704_0281',\n",
       "  'wdate': '2024-07-04 08:11:00',\n",
       "  'title': '삼성전자, \"2Q 실적 전망치 부합 예상\"…여전히 HBM이 관건-상상인',\n",
       "  'summary': '4일 상상 상상인증권은 삼성전자가 HBM(고대역폭메모리)의 안정적인 양산에 사활을 걸어야 한다고 4일 분석했다.',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/008/0005059354',\n",
       "  'image': 'https://ssl.pstatic.net/static.news/image/news/ogtag/navernews_800x420_20221201.jpg',\n",
       "  'stock_list': [{'stock_id': '005930', 'stock_name': '삼성전자'}],\n",
       "  'industry_list': [{'stock_id': '005930',\n",
       "    'industry_id': '32604',\n",
       "    'industry_name': '통신 및 방송 장비 제조업'}],\n",
       "  'similarity': 0.47},\n",
       " {'news_id': '20240226_0131',\n",
       "  'wdate': '2024-02-26 13:47:00',\n",
       "  'title': \"신고가 행진 SK하이닉스, 'HBM 랠리' 이끈다\",\n",
       "  'summary': 'SK하이닉스가 HBM 1등 리더십 수성을 넘어 시장 지배력 강화와 종합 솔루션 제공을 목표로 하면서 협력업체들에 대한 성장 기대감도 높아지고 있는 가운데, 한미반도체는 한미반도체 대표 수혜주로 꼽힌다.    hBM은 D램 여러 개를 수직으로 쌓아 연결해 성능을 대폭 끌어올린 고성능 D램이다.',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/014/0005147461',\n",
       "  'image': 'https://imgnews.pstatic.net/image/014/2024/02/26/0005147461_001_20240226134705437.jpg?type=w800',\n",
       "  'stock_list': [{'stock_id': '000660', 'stock_name': 'SK하이닉스'},\n",
       "   {'stock_id': '042700', 'stock_name': '한미반도체'}],\n",
       "  'industry_list': [{'stock_id': '000660',\n",
       "    'industry_id': '32601',\n",
       "    'industry_name': '반도체 제조업'},\n",
       "   {'stock_id': '042700',\n",
       "    'industry_id': '32902',\n",
       "    'industry_name': '특수 목적용 기계 제조업'}],\n",
       "  'similarity': 0.46}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6812d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "\n",
    "async def find_news_similar_v2(\n",
    "    db: Session, news_id: str, top_n: int, min_gap_days: int, min_gap_between: int\n",
    "):\n",
    "    # 기준 뉴스 조회\n",
    "    ref_news_meta = (\n",
    "        db.query(NewsModel_v2_Metadata)\n",
    "        .filter(NewsModel_v2_Metadata.news_id == news_id)\n",
    "        .first()\n",
    "    )\n",
    "    ref_news_raw = (\n",
    "        db.query(NewsModel_v2).filter(NewsModel_v2.news_id == news_id).first()\n",
    "    )\n",
    "\n",
    "    if not ref_news_raw:\n",
    "        return []\n",
    "\n",
    "    ref_wdate = ref_news_raw.wdate\n",
    "\n",
    "    # 기준 뉴스 텍스트 추출\n",
    "    # text = ref_news_meta.summary if ref_news_meta else ref_news_raw.article[:300]\n",
    "    text = ref_news_raw.title + ref_news_raw.article[:300]\n",
    "    if not text.strip():\n",
    "        return []\n",
    "\n",
    "    # 유사 뉴스 API 호출\n",
    "    payload2 = {\"article\": text, \"top_k\": 10}\n",
    "    similar_news_list = get_news_similar_list(payload2, vector_db)\n",
    "\n",
    "    # 필터링 조건 적용\n",
    "    min_date = ref_wdate - timedelta(days=min_gap_days)\n",
    "\n",
    "    def is_far_enough(new_date: datetime, selected_dates) -> bool:\n",
    "        return all(abs((new_date - d).days) >= min_gap_between for d in selected_dates)\n",
    "\n",
    "    filtered_output = []\n",
    "    selected_dates = []\n",
    "\n",
    "    similar_news_list = sorted(\n",
    "        similar_news_list, key=lambda x: x[\"similarity\"], reverse=True\n",
    "    )\n",
    "\n",
    "    for item in similar_news_list:\n",
    "        item_date = datetime.fromisoformat(item[\"wdate\"])\n",
    "        if (\n",
    "            item[\"similarity\"]\n",
    "            < 0.9\n",
    "            # and item_date <= min_date\n",
    "            # and is_far_enough(item_date, selected_dates)\n",
    "        ):\n",
    "            filtered_output.append(item)\n",
    "            selected_dates.append(item_date)\n",
    "        # if len(filtered_output) >= top_n:\n",
    "        # break\n",
    "\n",
    "    similar_news_ids = [item[\"news_id\"] for item in filtered_output]\n",
    "    filtered_ids = [nid for nid in similar_news_ids if nid != news_id]\n",
    "\n",
    "    # 유사 뉴스 Rerank API 호출\n",
    "    payload = {\"news_id\": ref_news_raw.news_id, \"news_topk_ids\": filtered_ids}\n",
    "\n",
    "    try:\n",
    "        similar_news_reranked_list = await get_similar_news(payload, db)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 유사 뉴스 Rerank API 요청 실패: {e}\")\n",
    "        print(f\"텍스트 유사도만 조회하도록 합니다.: {e}\")\n",
    "\n",
    "        similar_news_reranked_list = filtered_output\n",
    "        # return []\n",
    "\n",
    "    # filtered_output = []\n",
    "    # selected_dates = []\n",
    "\n",
    "    # for item in similar_news_reranked_list:\n",
    "    #     item_date = datetime.fromisoformat(item[\"wdate\"])\n",
    "    #     if (\n",
    "    #         item[\"similarity\"] < 0.9\n",
    "    #         and item_date <= min_date\n",
    "    #         and is_far_enough(item_date, selected_dates)\n",
    "    #     ):\n",
    "    #         filtered_output.append(item)\n",
    "    #         selected_dates.append(item_date)\n",
    "    #     if len(filtered_output) >= top_n:\n",
    "    #         break\n",
    "\n",
    "    # similar_news_reranked_list = filtered_output\n",
    "\n",
    "    # 유사 뉴스 요약 맵\n",
    "    summary_map = {\n",
    "        item[\"news_id\"]: {\n",
    "            \"summary\": item[\"summary\"],\n",
    "            \"similarity\": item.get(\"score\") or item.get(\"similarity\"),\n",
    "        }\n",
    "        for item in similar_news_reranked_list\n",
    "    }\n",
    "\n",
    "    similar_ids = list(summary_map.keys())\n",
    "\n",
    "    # DB에서 메타 정보 조회\n",
    "    results = (\n",
    "        db.query(\n",
    "            NewsModel_v2.news_id,\n",
    "            NewsModel_v2.wdate,\n",
    "            NewsModel_v2.title,\n",
    "            NewsModel_v2.image,\n",
    "            NewsModel_v2.press,\n",
    "            NewsModel_v2.url,\n",
    "        )\n",
    "        .filter(NewsModel_v2.news_id.in_(similar_ids))\n",
    "        .all()\n",
    "    )\n",
    "\n",
    "    # SimilarNewsV2 객체 생성\n",
    "    output = []\n",
    "    for row in results:\n",
    "        meta = summary_map.get(row.news_id)\n",
    "        if meta:\n",
    "            output.append(\n",
    "                {\n",
    "                    'news_id': row.news_id,\n",
    "                    'wdate': row.wdate.isoformat(),\n",
    "                    'title': row.title,\n",
    "                    'press': row.press,\n",
    "                    'url': row.url,\n",
    "                    'image': row.image,\n",
    "                    'summary': meta[\"summary\"],\n",
    "                    'similarity': round(meta[\"similarity\"], 3),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # 유사도 높은 순 정렬\n",
    "    output.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "    return output[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6d31d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await find_news_similar_v2(db, \"20250523_0076\", 5, 90, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98a7c5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'news_id': '20241008_0315',\n",
       "  'wdate': '2024-10-08T06:30:00',\n",
       "  'title': '[기로의상장사]금양①유증으로 770억 현금 챙기는 류광지 회장',\n",
       "  'press': '아시아경제',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/277/0005481064',\n",
       "  'image': 'https://imgnews.pstatic.net/image/277/2024/10/08/0005481064_001_20241010112513308.jpg?type=w800',\n",
       "  'summary': '금양은 지난달 27일 4502억원 규모 주주배정 후 실권주 일반공모 방식의 유상증자를 결정하였으며 류광지 금양 회장은 증자에 참여하지 않으며 본인이 회사에 빌려줬던 돈을 증자대금으로 돌려받을 계획이다.',\n",
       "  'similarity': 0.633},\n",
       " {'news_id': '20250519_0154',\n",
       "  'wdate': '2025-05-19T08:29:00',\n",
       "  'title': '텔코웨어, 자진 상폐 추진...\"1대 주주 공개매수\"',\n",
       "  'press': '한국경제TV',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/215/0001209687',\n",
       "  'image': 'https://imgnews.pstatic.net/image/215/2025/05/19/A202505190085_1_20250519083007553.jpg?type=w800',\n",
       "  'summary': '회사의 1대 주주인 금한태 텔코웨어 대표이사는 이번 달 19일부터 다음 달 10일까지 회사 보통주 233만2천438주(지분율 25.24%)를 공개 매수한 후 자진 상폐를 신청할 계획이라고 밝혔다.',\n",
       "  'similarity': 0.613},\n",
       " {'news_id': '20241129_0043',\n",
       "  'wdate': '2024-11-29T15:44:00',\n",
       "  'title': '하이브 상장 때 4000억 따로 챙긴 방시혁…당국, 제재 여부 검토',\n",
       "  'press': '한국경제',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/015/0005063326',\n",
       "  'image': 'https://imgnews.pstatic.net/image/015/2024/11/29/0005063326_001_20241129155613852.jpg?type=w800',\n",
       "  'summary': '방시혁 하이브 의장은 2020년 하이브 상장 전 스틱인베스트먼트, 이스톤에쿼티파트너스, 뉴메인에쿼티 등과 주주 간 계약을 맺었으나 IPO 후 이들 PEF의 매각 차익 중 약 30%를 받고, 기한 내 IPO에 실패하면 지분을 되사주기로 했다.',\n",
       "  'similarity': 0.587},\n",
       " {'news_id': '20240731_0081',\n",
       "  'wdate': '2024-07-31T14:53:00',\n",
       "  'title': '락앤락 추가매수도 지지부진 … 어피너티, 자진 상장폐지 계획 어쩌나',\n",
       "  'press': '매일경제',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/009/0005343232',\n",
       "  'image': 'https://imgnews.pstatic.net/image/009/2024/07/31/0005343232_001_20240731145310027.png?type=w800',\n",
       "  'summary': '당초 어피너티는 6월 12일까지 락앤락 보유지분을 86.38%까지 늘렸지만, 당초 공개매수를 통해 추진하려고 했던 자진상장폐지를 위해선 95% 지분이 필요한 상황으로 소액주주가 NH투자증권을 통해 응모하면 되는 식이다.',\n",
       "  'similarity': 0.455},\n",
       " {'news_id': '20250519_0036',\n",
       "  'wdate': '2025-05-19T14:49:00',\n",
       "  'title': '텔코웨어, 공개매수 후 자진 상폐 추진 소식에…상한가',\n",
       "  'press': '머니투데이',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/008/0005196053',\n",
       "  'image': 'https://imgnews.pstatic.net/image/008/2025/05/19/0005196053_001_20250519145010122.jpg?type=w800',\n",
       "  'summary': '통신 소프트웨어 기업 텔코웨어가 19일 장 시작과 함께 상한가로 직행한 이후 다음 달 10일까지 회사 보통주 233만2438주(지분율 25.24%)를 공개 매수한다고 밝히며 주주가치 제고를 위해 자사주를 소각해야 한다는 지적이 나오고 있다.',\n",
       "  'similarity': 0.419}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4b0e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../db/news_2023_2025_metadata2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf5ea860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13720 entries, 0 to 13719\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   news_id          13720 non-null  object \n",
      " 1   summary          13720 non-null  object \n",
      " 2   stock_list       13720 non-null  object \n",
      " 3   stock_list_view  13720 non-null  object \n",
      " 4   industry_list    13720 non-null  object \n",
      " 5   impact_score     13720 non-null  float64\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 643.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2639dabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13720"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_ids = df['news_id'].tolist()\n",
    "len(news_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d8e67ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 유사 뉴스 검색 중: 100%|██████████| 13720/13720 [2:04:10<00:00,  1.84it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "# 또는 일반 tqdm로도 충분할 수 있습니다:\n",
    "# from tqdm import tqdm\n",
    "\n",
    "sim_results = []\n",
    "\n",
    "for news_id in tqdm(news_ids, desc=\"🔍 유사 뉴스 검색 중\"):\n",
    "    results = await find_news_similar_v2(db, news_id, 5, 90, 30)\n",
    "\n",
    "    for result in results:\n",
    "        data = {\n",
    "            \"news_id\": news_id,\n",
    "            \"sim_news_id\": result['news_id'],\n",
    "            \"wdate\": result['wdate'],\n",
    "            \"title\": result['title'],\n",
    "            \"summary\": result['summary'],\n",
    "            \"press\": result['press'],\n",
    "            \"url\": result['url'],\n",
    "            \"image\": result['image'],\n",
    "            \"similarity\": result['similarity']\n",
    "        }\n",
    "\n",
    "        sim_results.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cd38af50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'news_id': '20250523_0004',\n",
       "  'sim_news_id': '20250220_0001',\n",
       "  'wdate': '2025-02-20T19:40:00',\n",
       "  'title': '에프앤가이드, 신임 대표에 이기태 전 삼성증권 상무 내정',\n",
       "  'summary': '20일 투자은행(IB)업계에 따르면 금융정보업체 에프앤가이드는 최근 이기태 전 삼성증권 중부지역 본부장(상무)을 신임 대표로 내정했으며 오는 3월 열리는 주주총회에서 이 내정자의 선임 안건을 처리할 예정이다.',\n",
       "  'press': '한국경제',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/015/0005097117',\n",
       "  'image': 'https://ssl.pstatic.net/static.news/image/news/ogtag/navernews_800x420_20221201.jpg',\n",
       "  'similarity': 0.68},\n",
       " {'news_id': '20250523_0004',\n",
       "  'sim_news_id': '20250220_0059',\n",
       "  'wdate': '2025-02-20T13:16:00',\n",
       "  'title': '[단독] 에프앤가이드 신임 대표에 ‘삼성맨’ 낙점....이기태 前삼성증권 상무 내정',\n",
       "  'summary': '20일 투자은행(IB)업계에 따르면 국내 1위 금융정보분석 제공업체인 에프앤가이드가 최근 이기태 전 삼성증권 중부지역 본부장(상무)을 신임 대표로 내정했으며 오는 3월 주주총회에서 이 신임 대표의 선임 안건을 처리할 것으로 알려졌다.',\n",
       "  'press': '파이낸셜뉴스',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/014/0005311132',\n",
       "  'image': 'https://imgnews.pstatic.net/image/014/2025/02/20/0005311132_001_20250220131615667.jpg?type=w800',\n",
       "  'similarity': 0.679},\n",
       " {'news_id': '20250523_0004',\n",
       "  'sim_news_id': '20250407_0038',\n",
       "  'wdate': '2025-04-07T13:51:00',\n",
       "  'title': '[인사]한양증권',\n",
       "  'summary': '<한양증권 <선임> ▷총괄 △S&T총괄 배성수(상무) △CM본부장 박상훈(상무)  △CS본부장 유문성(상무), △CRS부장 이승곤(이사) △채권부장 정지현(이사)',\n",
       "  'press': '머니투데이',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/008/0005176857',\n",
       "  'image': 'https://ssl.pstatic.net/static.news/image/news/ogtag/navernews_800x420_20221201.jpg',\n",
       "  'similarity': 0.673},\n",
       " {'news_id': '20250523_0004',\n",
       "  'sim_news_id': '20240308_0041',\n",
       "  'wdate': '2024-03-08T15:56:00',\n",
       "  'title': \"[fn마켓워치] '3파전 압축' NH투자증권 왕좌의 게임...누구 품에?\",\n",
       "  'summary': 'NH투자증권이 임원후보추천위원회를 열어 유찬형 전 농협중앙회 부회장, 윤병운 NH투자증권 IB1사업부 대표, 사재훈 전 삼성증권 부사장 등 3인을 차기 대표이사 후보로 압축한 가운데, 후보 면면에도 관심이 쏠린다.',\n",
       "  'press': '파이낸셜뉴스',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/014/0005153400',\n",
       "  'image': 'https://imgnews.pstatic.net/image/014/2024/03/08/0005153400_001_20240308155605992.jpg?type=w800',\n",
       "  'similarity': 0.661},\n",
       " {'news_id': '20250523_0004',\n",
       "  'sim_news_id': '20240305_0071',\n",
       "  'wdate': '2024-03-05T15:50:00',\n",
       "  'title': 'NH證 차기 사장 후보 3명 압축···윤병운 부사장, 유찬형 부회장, 사재훈 부사장',\n",
       "  'summary': 'NH투자증권증권을 이끌 차기 사장 후보가 윤병운 NH투자증권 부사장, 유찬형 전 농협중앙회 부회장, 사재훈 전 삼성증권 부사장 등 3명으로 압축된 가운데, 시장에서는 자본시장을 제대로 이해하는 인물이 차기 사장 후보로 적합하다는 공감대가 형성되며 일부 후보의 경우 임무를 제대로 수행할 수 있을지에 대한 의구심이 제기되는 상황이다.',\n",
       "  'press': '매일경제',\n",
       "  'url': 'https://n.news.naver.com/mnews/article/009/0005267611',\n",
       "  'image': 'https://ssl.pstatic.net/static.news/image/news/ogtag/navernews_800x420_20221201.jpg',\n",
       "  'similarity': 0.646}]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_results[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25b0be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_v2_df = pd.DataFrame(sim_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b35833f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_v2_df[\"wdate\"] = pd.to_datetime(news_v2_df[\"wdate\"]).dt.strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06616cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68600 entries, 0 to 68599\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   news_id      68600 non-null  object \n",
      " 1   sim_news_id  68600 non-null  object \n",
      " 2   wdate        68600 non-null  object \n",
      " 3   title        68600 non-null  object \n",
      " 4   summary      68600 non-null  object \n",
      " 5   press        68600 non-null  object \n",
      " 6   url          68600 non-null  object \n",
      " 7   image        68600 non-null  object \n",
      " 8   similarity   68600 non-null  float64\n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "news_v2_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fe4b5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>sim_news_id</th>\n",
       "      <th>wdate</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>press</th>\n",
       "      <th>url</th>\n",
       "      <th>image</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20250523_0002</td>\n",
       "      <td>20250523_0007</td>\n",
       "      <td>2025-05-23 18:00:00</td>\n",
       "      <td>[단독] 전자결제 강자 카카오페이 쓱·스마일 페이 인수 추진</td>\n",
       "      <td>23일 정보기술(IT)·투자은행(IB) 업계에 따르면 카카오페이가 SSG닷컴 쓱페이...</td>\n",
       "      <td>매일경제</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "      <td>https://ssl.pstatic.net/static.news/image/news...</td>\n",
       "      <td>0.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20250523_0002</td>\n",
       "      <td>20250224_0128</td>\n",
       "      <td>2025-02-24 08:27:00</td>\n",
       "      <td>카카오, 오픈AI 협력으로 커머스 가치↑…목표가 상향-키움</td>\n",
       "      <td>카카오가 커머스 사업부문 적정 가치를 기존 6조3000억원에서 7조1000억원으로 ...</td>\n",
       "      <td>머니투데이</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/008/000...</td>\n",
       "      <td>https://imgnews.pstatic.net/image/008/2025/02/...</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20250523_0002</td>\n",
       "      <td>20250120_0170</td>\n",
       "      <td>2025-01-20 07:41:00</td>\n",
       "      <td>이마트, 변화로 실적 회복 가속화 기대-상상인</td>\n",
       "      <td>SSG닷컴 FI 교체 성공에 이어 CJ대한통운(000120)과의 물류 협업을 통한 ...</td>\n",
       "      <td>이데일리</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/018/000...</td>\n",
       "      <td>https://imgnews.pstatic.net/image/018/2025/01/...</td>\n",
       "      <td>0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20250523_0002</td>\n",
       "      <td>20250523_0011</td>\n",
       "      <td>2025-05-23 17:52:00</td>\n",
       "      <td>몸집 키우는 카카오…'간편결제 빅3' 흔드나</td>\n",
       "      <td>카카오페이가 신세계 이마트 측에서 쓱페이·스마일페이 인수를 추진하고 나선 건 국내 ...</td>\n",
       "      <td>매일경제</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "      <td>https://imgnews.pstatic.net/image/009/2025/05/...</td>\n",
       "      <td>0.543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20250523_0002</td>\n",
       "      <td>20240228_0063</td>\n",
       "      <td>2024-02-28 16:16:00</td>\n",
       "      <td>카카오페이증권 새 수장에 신호철 카카오페이 부사장</td>\n",
       "      <td>28일 진행된페이증권 이사회에서 신호철 현 카카오페이 사업개발실장이 카카오페이증권 ...</td>\n",
       "      <td>파이낸셜뉴스</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/014/000...</td>\n",
       "      <td>https://imgnews.pstatic.net/image/014/2024/02/...</td>\n",
       "      <td>0.516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         news_id    sim_news_id                wdate  \\\n",
       "0  20250523_0002  20250523_0007  2025-05-23 18:00:00   \n",
       "1  20250523_0002  20250224_0128  2025-02-24 08:27:00   \n",
       "2  20250523_0002  20250120_0170  2025-01-20 07:41:00   \n",
       "3  20250523_0002  20250523_0011  2025-05-23 17:52:00   \n",
       "4  20250523_0002  20240228_0063  2024-02-28 16:16:00   \n",
       "\n",
       "                               title  \\\n",
       "0  [단독] 전자결제 강자 카카오페이 쓱·스마일 페이 인수 추진   \n",
       "1   카카오, 오픈AI 협력으로 커머스 가치↑…목표가 상향-키움   \n",
       "2          이마트, 변화로 실적 회복 가속화 기대-상상인   \n",
       "3           몸집 키우는 카카오…'간편결제 빅3' 흔드나   \n",
       "4        카카오페이증권 새 수장에 신호철 카카오페이 부사장   \n",
       "\n",
       "                                             summary   press  \\\n",
       "0  23일 정보기술(IT)·투자은행(IB) 업계에 따르면 카카오페이가 SSG닷컴 쓱페이...    매일경제   \n",
       "1  카카오가 커머스 사업부문 적정 가치를 기존 6조3000억원에서 7조1000억원으로 ...   머니투데이   \n",
       "2  SSG닷컴 FI 교체 성공에 이어 CJ대한통운(000120)과의 물류 협업을 통한 ...    이데일리   \n",
       "3  카카오페이가 신세계 이마트 측에서 쓱페이·스마일페이 인수를 추진하고 나선 건 국내 ...    매일경제   \n",
       "4  28일 진행된페이증권 이사회에서 신호철 현 카카오페이 사업개발실장이 카카오페이증권 ...  파이낸셜뉴스   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://n.news.naver.com/mnews/article/009/000...   \n",
       "1  https://n.news.naver.com/mnews/article/008/000...   \n",
       "2  https://n.news.naver.com/mnews/article/018/000...   \n",
       "3  https://n.news.naver.com/mnews/article/009/000...   \n",
       "4  https://n.news.naver.com/mnews/article/014/000...   \n",
       "\n",
       "                                               image  similarity  \n",
       "0  https://ssl.pstatic.net/static.news/image/news...       0.701  \n",
       "1  https://imgnews.pstatic.net/image/008/2025/02/...       0.592  \n",
       "2  https://imgnews.pstatic.net/image/018/2025/01/...       0.588  \n",
       "3  https://imgnews.pstatic.net/image/009/2025/05/...       0.543  \n",
       "4  https://imgnews.pstatic.net/image/014/2024/02/...       0.516  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_v2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8178b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_v2_df.to_csv('../../db/news_2023_2025_similarity2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4cb7db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-0602",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
