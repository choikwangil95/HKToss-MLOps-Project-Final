{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50cc2b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:239: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:271: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:194: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ ONNX ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ: ..\\..\\automation\\models\\kobart_summary_onnx\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = \"digit82/kobart-summarization\"\n",
    "onnx_dir = Path(\"../../automation/models/kobart_summary_onnx\")\n",
    "\n",
    "# âœ… 1. ëª¨ë¸ ë³€í™˜ (export=True) â† ì €ì¥ ì•ˆë¨\n",
    "model = ORTModelForSeq2SeqLM.from_pretrained(model_name, export=True)\n",
    "\n",
    "# âœ… 2. ONNX ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(onnx_dir)\n",
    "\n",
    "# âœ… 3. í† í¬ë‚˜ì´ì € ì €ì¥\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(onnx_dir)\n",
    "\n",
    "print(\"ğŸ‰ ONNX ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ:\", onnx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bbb22af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def get_summarize_model():\n",
    "    \"\"\"\n",
    "    ONNX ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "\n",
    "    model_dir = Path(\"../../automation/models/kobart_summary_onnx\")\n",
    "    model_summarize = ORTModelForSeq2SeqLM.from_pretrained(model_dir, local_files_only=True)\n",
    "\n",
    "    tokenizer_summarize = Tokenizer.from_file(str(model_dir / \"tokenizer.json\"))\n",
    "\n",
    "    return model_summarize, tokenizer_summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f99488bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_summarize, tokenizer_summarize = get_summarize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "29515723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_event_focused(text, model_summarize, tokenizer_summarize):\n",
    "    # í† í°í™”\n",
    "    encoding = tokenizer_summarize.encode(text)\n",
    "    input_ids = torch.tensor([encoding.ids])\n",
    "\n",
    "    text_length = len(text)\n",
    "\n",
    "    # 1. ìµœì†Œ ê¸¸ì´ëŠ” ì§§ì€ ë³¸ë¬¸ì€ ê³ ì •, ê¸´ ë³¸ë¬¸ì€ ì ì§„ì ìœ¼ë¡œ ì¦ê°€\n",
    "    def compute_min_length(text_length: int) -> int:\n",
    "        if text_length < 300:\n",
    "            return 30\n",
    "        else:\n",
    "            return 50\n",
    "\n",
    "    min_len = compute_min_length(text_length)\n",
    "    max_len = round(text_length * 0.5) + 50  # ë” ì—¬ìœ ë¥¼ ì£¼ë˜ max ê¸¸ì´ ì œí•œ\n",
    "\n",
    "    # ìƒì„±\n",
    "    summary_ids = model_summarize.generate(\n",
    "        input_ids,\n",
    "        min_length=min_len,  # ìµœì†Œ ìš”ì•½ ê¸¸ì´ (ì˜ˆ: 30)\n",
    "        max_new_tokens=max_len,  # ì¶”ê°€ ìƒì„± ìµœëŒ€ í† í° ìˆ˜\n",
    "        num_beams=4,  # ë¹” ì„œì¹˜ ê°œìˆ˜\n",
    "        length_penalty=1.0,  # ë„ˆë¬´ ê¸´ ë¬¸ì¥ ë°©ì§€\n",
    "        repetition_penalty=1.3,  # ë°˜ë³µ ì–µì œ\n",
    "        no_repeat_ngram_size=3,  # ë™ì¼í•œ 3-gram ë°˜ë³µ ê¸ˆì§€\n",
    "        early_stopping=True,  # <eos> í† í° ìƒì„± ì‹œ ì¤‘ë‹¨\n",
    "        # do_sampleì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ONNX ë¯¸ì§€ì›)\n",
    "    )\n",
    "    summary = tokenizer_summarize.decode(summary_ids[0].tolist(), skip_special_tokens=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6731acf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ìš”ì•½: ì¹´ì¹´ì˜¤í˜ì´ê°€ ì‹ ì„¸ê³„ ì´ë§ˆíŠ¸ ì¸¡ì—ì„œ ì“±í˜ì´Â·ìŠ¤ë§ˆì¼í˜ì´ ì¸ìˆ˜ë¥¼ ì¶”ì§„í•˜ê³  ë‚˜ì„  ê±´ êµ­ë‚´ ê°„í¸ê²°ì œ ì‹œì¥ì—ì„œ ì˜í–¥ë ¥ì„ í™•ëŒ€í•˜ê¸° ìœ„í•œ ì°¨ì›ì´ë©°, ì“±í˜ì´ì™€ ìŠ¤ë§ˆì¼í˜ì´ëŠ” ì£¼ ì´ìš©ë¥  ë¹ˆë„ëŠ” ë‚®ì§€ë§Œ, ì´ìš©ìê°€ ì•½ 2500ë§Œëª…ì— ë‹¬í•´ ì¹´ì¹´ì˜¤í˜ì´ì™€ ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì…ë ¥ ë¬¸ì¥\n",
    "text = \"\"\"\n",
    "ì¹´ì¹´ì˜¤í˜ì´, ì‹ ì„¸ê³„ í˜ì´ í’ˆë‚˜ ì´ì»¤ë¨¸ìŠ¤ ê²°ì œ ê°•í™” ì ìœ  í™•ëŒ€ ì••ë„ì  1ìœ„ ë„¤ì´ë²„ì— ë„ì „ì¥ ë§¤ê°ê°€ 5ì²œì–µì› ì•ˆíŒ ë‹¬í•  ë“¯ ì¹´ì¹´ì˜¤í˜ì´ê°€ ì‹ ì„¸ê³„ ì´ë§ˆíŠ¸ ì¸¡ì—ì„œ ì“±í˜ì´Â·ìŠ¤ë§ˆì¼í˜ì´ ì¸ìˆ˜ë¥¼ ì¶”ì§„í•˜ê³  ë‚˜ì„  ê±´ êµ­ë‚´ ê°„í¸ê²°ì œ ì‹œì¥ì—ì„œ ì˜í–¥ë ¥ì„ í™•ëŒ€í•˜ê¸° ìœ„í•œ ì°¨ì›ì´ë‹¤. ë¯¸ë¦¬ ê³„ì¢ŒÂ·ì¹´ë“œë¥¼ ë“±ë¡ë§Œ í•´ë†“ìœ¼ë©´ ê°„í¸í•˜ê²Œ ë¬¼ê±´Â·ì„œë¹„ìŠ¤ë¥¼ ê²°ì œí•  ìˆ˜ ìˆëŠ” ê°„í¸ê²°ì œ ì‹œì¥ì€ í•´ë§ˆë‹¤ ê°€íŒŒë¥¸ ì„±ì¥ì„¸ë¥¼ ë³´ì´ë©° ì „ìê¸ˆìœµ ì—…ê³„ì˜ ì¤‘ìš”í•œ ë¨¹ê±°ë¦¬ë¡œ ê¸‰ë¶€ìƒ ì¤‘ì´ë‹¤. 23ì¼ í•œêµ­ì€í–‰ ë“±ì— ë”°ë¥´ë©´ êµ­ë‚´ ê°„í¸ì§€ê¸‰(ê°„í¸ê²°ì œ) ì„œë¹„ìŠ¤ ì‹œì¥ì˜ ì¼í‰ê·  ì´ìš© ê¸ˆì•¡ì€ ì§€ë‚œí•´ ê¸°ì¤€ 9594ì–µì›ì— ë‹¬í•œë‹¤. ì´ ì¤‘ ë„¤ì´ë²„í˜ì´, ì¹´ì¹´ì˜¤í˜ì´, í† ìŠ¤í˜ì´ ë“± ì „ìê¸ˆìœµì—…ìê°€ ì¼í‰ê·  4814ì–µì›, íœ´ëŒ€í° ì œì¡° íšŒì‚¬(ì‚¼ì„±í˜ì´Â·ì• í”Œí˜ì´ ë“±)ê°€ ì¼í‰ê·  2443ì–µì›, ê¸ˆìœµíšŒì‚¬(ì¹´ë“œì‚¬Â·ì€í–‰ ë“±)ê°€ ì¼í‰ê·  2337ì–µì›ì„ ë‹´ë‹¹í•˜ê³  ìˆë‹¤. ì „ìê¸ˆìœµì—…ìê°€ ê°„í¸ì§€ê¸‰ ì‹œì¥ì˜ 50.7%ë¥¼ ì°¨ì§€í•˜ê³  ìˆëŠ” ì…ˆì´ë‹¤. í˜„ì¬ ì „ìê¸ˆìœµì—…ìëŠ” ì´ 40ê°œì— ë‹¬í•˜ëŠ”ë°, ì´ ì¤‘ ë¹…3ì¸ ë„¤ì´ë²„í˜ì´, ì¹´ì¹´ì˜¤í˜ì´, í† ìŠ¤í˜ì´ê°€ 90%ì˜ ì ìœ ìœ¨ì„ ê¸°ë¡ ì¤‘ì´ë‹¤. ì´ ë°–ì— ì“±í˜ì´, ë‹¹ê·¼í˜ì´, ë¬´ì‹ ì‚¬í˜ì´, ì»¬ë¦¬í˜ì´ ë“±ë„ ê²½ìŸì„ í¼ì¹˜ê³  ìˆë‹¤. ì¹´ì¹´ì˜¤í˜ì´ëŠ” ì„ ë¬¼í•˜ê¸° ë“± ê²°ì œ ê¸°ëŠ¥ì´ ìˆì§€ë§Œ, ì´ì»¤ë¨¸ìŠ¤ì™€ ì—°ë™ë¼ ìˆëŠ” ë„¤ì´ë²„, ì¹´ë“œì‚¬ì™€ ì—°ë™ë¼ ìˆëŠ” í† ìŠ¤ì— ë¹„í•´ ì‹œì¥ì ìœ ìœ¨ì´ ë‚®ì€ ìƒí™©ì´ë‹¤. ì´ ë•Œë¬¸ì— ì¹´ì¹´ì˜¤í˜ì´ëŠ” ì´ì»¤ë¨¸ìŠ¤ ê¸°ëŠ¥ì´ ìˆëŠ” ì“±í˜ì´Â·ìŠ¤ë§ˆì¼í˜ì´ ì¸ìˆ˜ë¥¼ ì‹œë„í•˜ë©° ì‹œì¥ì ìœ ìœ¨ í™•ëŒ€ë¥¼ ë…¸ë¦´ ê²ƒìœ¼ë¡œ ì „ë§ëœë‹¤. ì“±í˜ì´ì™€ ìŠ¤ë§ˆì¼í˜ì´ëŠ” ì£¼ ì´ìš©ë¥  ë¹ˆë„ëŠ” ë‚®ì§€ë§Œ, ì´ìš©ìê°€ ì•½ 2500ë§Œëª…ì— ë‹¬í•´ ì¹´ì¹´ì˜¤í˜ì´ì™€ ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆë‹¤. ì•ì„œ ì‹ ì„¸ê³„ê·¸ë£¹ì€ ì§€ë‚œí•´ í•œ ì°¨ë¡€ ì“±í˜ì´Â·ìŠ¤ë§ˆì¼í˜ì´ì— ëŒ€í•œ ë§¤ê° ì‘ì—…ì„ ì¶”ì§„í•˜ë©° ìš°ì„ í˜‘ìƒëŒ€ìƒìë¡œ í† ìŠ¤(ë¹„ë°”ë¦¬í¼ë¸”ë¦¬ì¹´)ë¥¼ ì„ ì •í•œ ë°” ìˆë‹¤. ë‹¹ì‹œ ê±°ë¡ ëœ ê°€ê²©ì€ 7000ì–µì› ì„ ì´ì—ˆë‹¤. í•˜ì§€ë§Œ ì–‘ì¸¡ì€ ì´í›„ ì‹œë„ˆì§€ ì°½ì¶œ ë°©ì•ˆê³¼ ê´€ë ¨í•´ ì´ê²¬ì´ ìƒê²¼ê³ , ê²°êµ­ ë§¤ê° ì‘ì—…ì´ ë¬´ì‚°ëë‹¤. ì´ëŸ° ê°€ìš´ë° ì´ë‚  ì´ë§ˆíŠ¸ì˜ ìíšŒì‚¬ SSGë‹·ì»´ì€ ê°„í¸ê²°ì œ ì‚¬ì—…ë¶€ì¸ \\'ì“±í˜ì´\\'ë¥¼ ë¬¼ì ë¶„í• í•´ ì‹ ì„¤ íšŒì‚¬ì¸ \\'í”Œë˜í‹°ë„˜í˜ì´ë¨¼ì¸ \\'ë¥¼ ì„¤ë¦½í•œë‹¤ê³  ë°í˜”ë‹¤. ë¶„í•  ê¸°ì¼ì€ 2025ë…„ 7ì›” 1ì¼ì´ë©°, ì‹ ì„¤ ë²•ì¸ì€ SSGë‹·ì»´ ìíšŒì‚¬ë¡œ í¸ì…ë  ì˜ˆì •ì´ë‹¤. SSGë‹·ì»´ ì¸¡ì€ \"ê°„í¸ê²°ì œ ì„œë¹„ìŠ¤ì˜ ì „ë¬¸ì„±ì„ ê°•í™”í•˜ê³ , ë²”ìš©ì„±ì„ ë†’ì—¬ ê²½ì˜ íš¨ìœ¨ì„±ì„ ì œê³ í•˜ê¸° ìœ„í•œ ê²°ì •\"ì´ë¼ê³  ë°í˜”ë‹¤. ê´€ë ¨ ì—…ê³„ì—ì„œëŠ” ê°„í¸ê²°ì œ ì‚¬ì—… ë§¤ê°ì„ ìœ„í•œ ì´ë§ˆíŠ¸ ì¸¡ì˜ ì‚¬ì „ì •ì§€ ì‘ì—…ìœ¼ë¡œ í•´ì„í•˜ê³  ìˆë‹¤. í˜„ì¬ íˆ¬ìì€í–‰(IB) ì—…ê³„ì—ì„œ ê±°ë¡ ë˜ëŠ” ì“±í˜ì´Â·ìŠ¤ë§ˆì¼í˜ì´ì˜ ë§¤ê°ê°€ëŠ” ì•½ 5000ì–µì› ì•ˆíŒì´ë‹¤. ì´ë²ˆ ë§¤ê°ìœ¼ë¡œ ìê¸ˆì´ ìœ ì…ë˜ë©´ ì´ë§ˆíŠ¸ì˜ ì¬ë¬´ ê°œì„ ì—ë„ ë„ì›€ì´ ë  ì „ë§ì´ë‹¤. ì´ë§ˆíŠ¸ì˜ ë¶€ì±„ ë¹„ìœ¨ì€ ì˜¬í•´ 3ì›” ë§ ê¸°ì¤€ 117.4%ë¡œ, 2022~2023ë…„ 90%ëŒ€ ì´ˆë°˜ì´ì—ˆë˜ ê²ƒê³¼ ë¹„êµí•˜ë©´ ë¶€ì±„ê°€ ëŠ˜ì–´ë‚œ ìƒí™©ì´ë‹¤\n",
    "\"\"\"\n",
    "summary = summarize_event_focused(text, model_summarize, tokenizer_summarize)\n",
    "\n",
    "print(\"ğŸ“„ ìš”ì•½:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8df65d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# í–‰ ì „ì²´ ì¶œë ¥\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# ì—´ ì „ì²´ ì¶œë ¥\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "# ê° ì—´ ë„ˆë¹„ ë¬´ì œí•œ ì¶œë ¥\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "# ì „ì²´ ë„“ì´ ì œí•œ í•´ì œ\n",
    "pd.set_option(\"display.width\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deb883fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from kss import split_sentences\n",
    "\n",
    "def remove_market_related_sentences(text: str) -> str:\n",
    "    # ì¤„ë°”ê¿ˆ ì œê±°\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # ëŒ€ê´„í˜¸ í¬í•¨ í…ìŠ¤íŠ¸ ì œê±°: [íŒŒì´ë‚¸ì…œë‰´ìŠ¤], [ì‚¬ì§„] ë“±\n",
    "    text = re.sub(r\"\\[[^\\]]*\\]\", \"\", text)\n",
    "\n",
    "    # '/ì‚¬ì§„', '/ì‚¬ì§„ì œê³µ' ì œê±°\n",
    "    text = re.sub(r\"/ì‚¬ì§„(ì œê³µ)?\", \"\", text)\n",
    "\n",
    "    # ì´ë©”ì¼ ì£¼ì†Œ ì œê±° (ì˜ˆ: josh@yna.co.kr)\n",
    "    text = re.sub(r\"\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b\", \"\", text)\n",
    "\n",
    "    # ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬ (ê°„ë‹¨í•˜ê²Œ ë§ˆì¹¨í‘œ ê¸°ì¤€, í•„ìš”ì‹œ KSS ë“± ì ìš© ê°€ëŠ¥)\n",
    "    sentences = split_sentences(text)\n",
    "\n",
    "    # ì œê±°í•  íŒ¨í„´ë“¤ (ë‰´ìŠ¤ ë¬¸ì¥ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” íŒ¨í„´)\n",
    "    patterns = [\n",
    "        r\"(ìì„¸í•œ ë‚´ìš©|ìì„¸í•œ ì‚¬í•­)\",  # ë‰´ìŠ¤ ê¸°ë³¸ í‘œí˜„\n",
    "        r\"\\d{4}[.-]\\d{1,2}[.-]\\d{1,2}\",  # ë‚ ì§œ (ì˜ˆ: 2025.03.26, 2024-12-01)\n",
    "        r\"([0-9,]+(?:ë§Œ)?[0-9,]*\\s?(?:ì›|ë§Œì›))\",  # ê°€ê²© (ì˜ˆ: 3,500ì›, 12000ì›)\n",
    "        r\"(ê°•ì„¸|í€ë“œ|ì‹œê°€ì´ì•¡|ë“±ë½ë¥ |í•œêµ­ê±°ë˜ì†Œ)\",  # ì¦ì‹œ ìš©ì–´\n",
    "        r\"\\([+-]?[0-9.,]+%\\)\",  # ê´„í˜¸ ì•ˆ í¼ì„¼íŠ¸ ë“±ë½ë¥ \n",
    "        r\"(íˆ¬ìì˜ê²¬|ì—°êµ¬ì›|í‰ê°€|ì˜ˆìƒì¹˜|ì¦ê¶Œê°€|ë¦¬í¬íŠ¸|íŒ€ì¥)\",  # ì• ë„ë¦¬ìŠ¤íŠ¸ ìš©ì–´\n",
    "        r\"(ìˆœì´ìµ|ì „ë…„|ë§¤ì¶œ|ì˜ì—…ì´ìµ|ì˜ì—…ì ì|ì¦ì‹œ|ì½”ìŠ¤í”¼|ì½”ìŠ¤ë‹¥|ë‹¤ìš°|ë‚˜ìŠ¤ë‹¥|ë§¤ì¶œì•¡|ê±°ë˜ì¼|í˜¸ì¡°ì„¸|ë ˆë²„ë¦¬ì§€|íˆ¬ìì|ì¡°ì •|ìì‚°|ìˆ˜ìµë¥ |ì´ìµë¥ |ìˆ˜ìµì„±|ë‚´ë¦¬ë§‰|ë¶€ì§„í•œ|ë‚™í­|ê¸°ëŒ€ì¹˜|ì‹¤ì ë°œí‘œ|ê¸°ì—… ê°€ì¹˜)\",  # ì‹œì¥ ìš©ì–´\n",
    "    ]\n",
    "\n",
    "    # í•˜ë‚˜ì˜ í†µí•© íŒ¨í„´ìœ¼ë¡œ ì»´íŒŒì¼\n",
    "    combined_pattern = re.compile(\"|\".join(patterns))\n",
    "\n",
    "    # í•„í„°ë§ëœ ë¬¸ì¥ë§Œ ìœ ì§€\n",
    "    filtered = [s for s in sentences if not combined_pattern.search(s)]\n",
    "\n",
    "    text_preprocessed = \" \".join(filtered)\n",
    "\n",
    "    # print(f\"ì›ë¬¸:{sentences}\\n|\\nì „ì²˜ë¦¬ ëœ ë¬¸ì¥: {text_preprocessed}\\n\\n\")\n",
    "\n",
    "    return text_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5df16d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_5 = df.head(5).copy()\n",
    "df_5['article'] = df_5['article'].apply(remove_market_related_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75c77c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì¹´ì¹´ì˜¤í˜ì´, ì‹ ì„¸ê³„ í˜ì´ í’ˆë‚˜ ì´ì»¤ë¨¸ìŠ¤ ê²°ì œ ê°•í™” ì ìœ  í™•ëŒ€ ì••ë„ì  1ìœ„ ë„¤ì´ë²„ì— ë„ì „ì¥ ë§¤ê°ê°€ 5ì²œì–µì› ì•ˆíŒ ë‹¬í•  ë“¯ ì¹´ì¹´ì˜¤í˜ì´ê°€ ì‹ ì„¸ê³„ ì´ë§ˆíŠ¸ ì¸¡ì—ì„œ ì“±í˜ì´Â·ìŠ¤ë§ˆì¼í˜ì´ ì¸ìˆ˜ë¥¼ ì¶”ì§„í•˜ê³  ë‚˜ì„  ê±´ êµ­ë‚´ ê°„í¸ê²°ì œ ì‹œì¥ì—ì„œ ì˜í–¥ë ¥ì„ í™•ëŒ€í•˜ê¸° ìœ„í•œ ì°¨ì›ì´ë‹¤. ë¯¸ë¦¬ ê³„ì¢ŒÂ·ì¹´ë“œë¥¼ ë“±ë¡ë§Œ í•´ë†“ìœ¼ë©´ ê°„í¸í•˜ê²Œ ë¬¼ê±´Â·ì„œë¹„ìŠ¤ë¥¼ ê²°ì œí•  ìˆ˜ ìˆëŠ” ê°„í¸ê²°ì œ ì‹œì¥ì€ í•´ë§ˆë‹¤ ê°€íŒŒë¥¸ ì„±ì¥ì„¸ë¥¼ ë³´ì´ë©° ì „ìê¸ˆìœµ ì—…ê³„ì˜ ì¤‘ìš”í•œ ë¨¹ê±°ë¦¬ë¡œ ê¸‰ë¶€ìƒ ì¤‘ì´ë‹¤. 23ì¼ í•œêµ­ì€í–‰ ë“±ì— ë”°ë¥´ë©´ êµ­ë‚´ ê°„í¸ì§€ê¸‰(ê°„í¸ê²°ì œ) ì„œë¹„ìŠ¤ ì‹œì¥ì˜ ì¼í‰ê·  ì´ìš© ê¸ˆì•¡ì€ ì§€ë‚œí•´ ê¸°ì¤€ 9594ì–µì›ì— ë‹¬í•œë‹¤. ì´ ì¤‘ ë„¤ì´ë²„í˜ì´, ì¹´ì¹´ì˜¤í˜ì´, í† ìŠ¤í˜ì´ ë“± ì „ìê¸ˆìœµì—…ìê°€ ì¼í‰ê·  4814ì–µì›, íœ´ëŒ€í° ì œì¡° íšŒì‚¬(ì‚¼ì„±í˜ì´Â·ì• í”Œí˜ì´ ë“±)ê°€ ì¼í‰ê·  2443ì–µì›, ê¸ˆìœµíšŒì‚¬(ì¹´ë“œì‚¬Â·ì€í–‰ ë“±)ê°€ ì¼í‰ê·  2337ì–µì›ì„ ë‹´ë‹¹í•˜ê³  ìˆë‹¤. ì „ìê¸ˆìœµì—…ìê°€ ê°„í¸ì§€ê¸‰ ì‹œì¥ì˜ 50.7%ë¥¼ ì°¨ì§€í•˜ê³  ìˆëŠ” ì…ˆì´ë‹¤. í˜„ì¬ ì „ìê¸ˆìœµì—…ìëŠ” ì´ 40ê°œì— ë‹¬í•˜ëŠ”ë°, ì´ ì¤‘ ë¹…3ì¸ ë„¤ì´ë²„í˜ì´, ì¹´ì¹´ì˜¤í˜ì´, í† ìŠ¤í˜ì´ê°€ 90%ì˜ ì ìœ ìœ¨ì„ ê¸°ë¡ ì¤‘ì´ë‹¤. ì´ ë°–ì— ì“±í˜ì´, ë‹¹ê·¼í˜ì´, ë¬´ì‹ ì‚¬í˜ì´, ì»¬ë¦¬í˜ì´ ë“±ë„ ê²½ìŸì„ í¼ì¹˜ê³  ìˆë‹¤. ì¹´ì¹´ì˜¤í˜ì´ëŠ” ì„ ë¬¼í•˜ê¸° ë“± ê²°ì œ ê¸°ëŠ¥ì´ ìˆì§€ë§Œ, ì´ì»¤ë¨¸ìŠ¤ì™€ ì—°ë™ë¼ ìˆëŠ” ë„¤ì´ë²„, ì¹´ë“œì‚¬ì™€ ì—°ë™ë¼ ìˆëŠ” í† ìŠ¤ì— ë¹„í•´ ì‹œì¥ì ìœ ìœ¨ì´ ë‚®ì€ ìƒí™©ì´ë‹¤. ì´ ë•Œë¬¸ì— ì¹´ì¹´ì˜¤í˜ì´ëŠ” ì´ì»¤ë¨¸ìŠ¤ ê¸°ëŠ¥ì´ ìˆëŠ” ì“±í˜ì´Â·ìŠ¤ë§ˆì¼í˜ì´ ì¸ìˆ˜ë¥¼ ì‹œë„í•˜ë©° ì‹œì¥ì ìœ ìœ¨ í™•ëŒ€ë¥¼ ë…¸ë¦´ ê²ƒìœ¼ë¡œ ì „ë§ëœë‹¤. ì“±í˜ì´ì™€ ìŠ¤ë§ˆì¼í˜ì´ëŠ” ì£¼ ì´ìš©ë¥  ë¹ˆë„ëŠ” ë‚®ì§€ë§Œ, ì´ìš©ìê°€ ì•½ 2500ë§Œëª…ì— ë‹¬í•´ ì¹´ì¹´ì˜¤í˜ì´ì™€ ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆë‹¤. ì•ì„œ ì‹ ì„¸ê³„ê·¸ë£¹ì€ ì§€ë‚œí•´ í•œ ì°¨ë¡€ ì“±í˜ì´Â·ìŠ¤ë§ˆì¼í˜ì´ì— ëŒ€í•œ ë§¤ê° ì‘ì—…ì„ ì¶”ì§„í•˜ë©° ìš°ì„ í˜‘ìƒëŒ€ìƒìë¡œ í† ìŠ¤(ë¹„ë°”ë¦¬í¼ë¸”ë¦¬ì¹´)ë¥¼ ì„ ì •í•œ ë°” ìˆë‹¤. ë‹¹ì‹œ ê±°ë¡ ëœ ê°€ê²©ì€ 7000ì–µì› ì„ ì´ì—ˆë‹¤. í•˜ì§€ë§Œ ì–‘ì¸¡ì€ ì´í›„ ì‹œë„ˆì§€ ì°½ì¶œ ë°©ì•ˆê³¼ ê´€ë ¨í•´ ì´ê²¬ì´ ìƒê²¼ê³ , ê²°êµ­ ë§¤ê° ì‘ì—…ì´ ë¬´ì‚°ëë‹¤. ì´ëŸ° ê°€ìš´ë° ì´ë‚  ì´ë§ˆíŠ¸ì˜ ìíšŒì‚¬ SSGë‹·ì»´ì€ ê°„í¸ê²°ì œ ì‚¬ì—…ë¶€ì¸ \\'ì“±í˜ì´\\'ë¥¼ ë¬¼ì ë¶„í• í•´ ì‹ ì„¤ íšŒì‚¬ì¸ \\'í”Œë˜í‹°ë„˜í˜ì´ë¨¼ì¸ \\'ë¥¼ ì„¤ë¦½í•œë‹¤ê³  ë°í˜”ë‹¤. ë¶„í•  ê¸°ì¼ì€ 2025ë…„ 7ì›” 1ì¼ì´ë©°, ì‹ ì„¤ ë²•ì¸ì€ SSGë‹·ì»´ ìíšŒì‚¬ë¡œ í¸ì…ë  ì˜ˆì •ì´ë‹¤. SSGë‹·ì»´ ì¸¡ì€ \"ê°„í¸ê²°ì œ ì„œë¹„ìŠ¤ì˜ ì „ë¬¸ì„±ì„ ê°•í™”í•˜ê³ , ë²”ìš©ì„±ì„ ë†’ì—¬ ê²½ì˜ íš¨ìœ¨ì„±ì„ ì œê³ í•˜ê¸° ìœ„í•œ ê²°ì •\"ì´ë¼ê³  ë°í˜”ë‹¤. ê´€ë ¨ ì—…ê³„ì—ì„œëŠ” ê°„í¸ê²°ì œ ì‚¬ì—… ë§¤ê°ì„ ìœ„í•œ ì´ë§ˆíŠ¸ ì¸¡ì˜ ì‚¬ì „ì •ì§€ ì‘ì—…ìœ¼ë¡œ í•´ì„í•˜ê³  ìˆë‹¤. í˜„ì¬ íˆ¬ìì€í–‰(IB) ì—…ê³„ì—ì„œ ê±°ë¡ ë˜ëŠ” ì“±í˜ì´Â·ìŠ¤ë§ˆì¼í˜ì´ì˜ ë§¤ê°ê°€ëŠ” ì•½ 5000ì–µì› ì•ˆíŒì´ë‹¤. ì´ë²ˆ ë§¤ê°ìœ¼ë¡œ ìê¸ˆì´ ìœ ì…ë˜ë©´ ì´ë§ˆíŠ¸ì˜ ì¬ë¬´ ê°œì„ ì—ë„ ë„ì›€ì´ ë  ì „ë§ì´ë‹¤. ì´ë§ˆíŠ¸ì˜ ë¶€ì±„ ë¹„ìœ¨ì€ ì˜¬í•´ 3ì›” ë§ ê¸°ì¤€ 117.4%ë¡œ, 2022~2023ë…„ 90%ëŒ€ ì´ˆë°˜ì´ì—ˆë˜ ê²ƒê³¼ ë¹„êµí•˜ë©´ ë¶€ì±„ê°€ ëŠ˜ì–´ë‚œ ìƒí™©ì´ë‹¤.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5['article'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a54f312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NER ONNX ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ..\\..\\automoation\\models\\ner_onnx\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForTokenClassification\n",
    "from pathlib import Path\n",
    "\n",
    "model_id = \"KPF/KPF-BERT-NER\"\n",
    "save_dir = Path(\"../../automoation/models/ner_onnx\")\n",
    "\n",
    "# ONNXë¡œ export\n",
    "model = ORTModelForTokenClassification.from_pretrained(model_id, export=True)\n",
    "model.save_pretrained(save_dir)\n",
    "\n",
    "# tokenizerë„ ì €ì¥\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"âœ… NER ONNX ëª¨ë¸ ì €ì¥ ì™„ë£Œ:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "def get_ner_tokenizer():\n",
    "    \"\"\"\n",
    "    ONNX NER ëª¨ë¸ì„ ìœ„í•œ í† í¬ë‚˜ì´ì €ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "\n",
    "    # ğŸŸ¢ í† í¬ë‚˜ì´ì € ë¡œë”©\n",
    "    tokenizer_ner = Tokenizer.from_file(\"../../automation/models/ner_onnx/tokenizer.json\")\n",
    "\n",
    "    # ğŸŸ¢ ONNX ëª¨ë¸ ì„¸ì…˜ ë¡œë”©\n",
    "    session_ner = ort.InferenceSession(\"../../automation/models/ner_onnx/model.onnx\")\n",
    "\n",
    "    return tokenizer_ner, session_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "017227ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from label_map import id2label  # ë¼ë²¨ ID â†” ë¼ë²¨ëª… ë§¤í•‘\n",
    "\n",
    "tokenizer_ner, session_ner = get_ner_tokenizer()\n",
    "\n",
    "def get_ner_tokens(tokenizer, session, text, id2label):\n",
    "    # ğŸŸ¡ í† í°í™” ë° ì…ë ¥ê°’ ì¤€ë¹„\n",
    "    encoding = tokenizer.encode(text)\n",
    "    input_ids = np.array([encoding.ids], dtype=np.int64)\n",
    "    attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
    "    token_type_ids = np.zeros_like(input_ids, dtype=np.int64)\n",
    "\n",
    "    # ğŸ”µ ONNX ì¶”ë¡  ì‹¤í–‰\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "    }\n",
    "\n",
    "    logits = session.run(None, inputs)[0]  # shape: (1, seq_len, num_labels)\n",
    "\n",
    "    # ğŸ”µ ë¼ë²¨ ì¸ë±ìŠ¤ â†’ ì‹¤ì œ ë¼ë²¨ëª…\n",
    "    preds = np.argmax(logits, axis=-1)[0]\n",
    "    labels = [id2label[p] for p in preds[: len(encoding.tokens)]]\n",
    "\n",
    "    # ğŸ”µ ì‹œê°í™”\n",
    "    tokens = encoding.tokens\n",
    "\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d80bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ogg_economy(tokens, labels, target_label=\"OGG_ECONOMY\"):\n",
    "    merged_words = []\n",
    "    current_word = \"\"\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        token_clean = token.replace(\"##\", \"\") if token.startswith(\"##\") else token\n",
    "\n",
    "        if label == f\"B-{target_label}\":\n",
    "            if current_word:\n",
    "                merged_words.append(current_word)\n",
    "            current_word = token_clean\n",
    "\n",
    "        elif label == f\"I-{target_label}\":\n",
    "            current_word += token_clean\n",
    "\n",
    "        else:\n",
    "            if current_word:\n",
    "                merged_words.append(current_word)\n",
    "                current_word = \"\"\n",
    "\n",
    "    if current_word:\n",
    "        merged_words.append(current_word)\n",
    "\n",
    "    stock_list = merged_words.copy()\n",
    "\n",
    "    return stock_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8efe21cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ê³¨ë“œë§Œì‚­ìŠ¤', 'í•œêµ­ì€í–‰', 'ì‚¼ì„±ì „ì', 'MBKíŒŒíŠ¸ë„ˆìŠ¤']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from label_map import id2label  # ë¼ë²¨ ID â†” ë¼ë²¨ëª… ë§¤í•‘\n",
    "\n",
    "text = \"\"\"\n",
    "ì„¸ê³„ 2ìœ„ íˆ¬ìì€í–‰(IB)ì¸ ê³¨ë“œë§Œì‚­ìŠ¤ì˜ ì‚¬ì¥ ê²¸ ìµœê³ ìš´ì˜ì±…ì„ì(COO)ì¸ ì¡´ ì›”ë“œë¡ ì´ ë°©í•œí•´ ì´ì°½ìš© í•œêµ­ì€í–‰ ì´ì¬, ì´ì¬ìš© ì‚¼ì„±ì „ì íšŒì¥, ê¹€ë³‘ì£¼ MBKíŒŒíŠ¸ë„ˆìŠ¤ íšŒì¥ ë“±ê³¼ í•¨ê»˜ í•œêµ­ì˜ ê¸ˆìœµë‹¹êµ­, ê¸°ì—… ë° ê¸ˆìœµê³„ì™€ì˜ í˜‘ë ¥ ë°©ì•ˆì„ ë…¼ì˜í–ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "tokens, labels = get_ner_tokens(tokenizer_ner, session_ner, text, id2label)\n",
    "stock_list = extract_ogg_economy(tokens, labels, target_label=\"OGG_ECONOMY\")\n",
    "stock_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de794ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-0602",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
