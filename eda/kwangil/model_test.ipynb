{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50cc2b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:239: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:271: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:194: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ ONNX ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ: ..\\..\\automation\\models\\kobart_summary_onnx\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = \"digit82/kobart-summarization\"\n",
    "onnx_dir = Path(\"../../automation/models/kobart_summary_onnx\")\n",
    "\n",
    "# âœ… 1. ëª¨ë¸ ë³€í™˜ (export=True) â† ì €ì¥ ì•ˆë¨\n",
    "model = ORTModelForSeq2SeqLM.from_pretrained(model_name, export=True)\n",
    "\n",
    "# âœ… 2. ONNX ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(onnx_dir)\n",
    "\n",
    "# âœ… 3. í† í¬ë‚˜ì´ì € ì €ì¥\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(onnx_dir)\n",
    "\n",
    "print(\"ğŸ‰ ONNX ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ:\", onnx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb22af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\test-0602\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def get_summarize_model():\n",
    "    \"\"\"\n",
    "    ONNX ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "\n",
    "    model_dir = Path(\"../../automation/models/kobart_summary_onnx\")\n",
    "    model_summarize = ORTModelForSeq2SeqLM.from_pretrained(model_dir, local_files_only=True)\n",
    "\n",
    "    tokenizer_summarize = Tokenizer.from_file(str(model_dir / \"tokenizer.json\"))\n",
    "\n",
    "    return model_summarize, tokenizer_summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ac95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "# â¬‡ ëª¨ë¸ ì„¸ì…˜ ì´ˆê¸°í™”\n",
    "def load_model():\n",
    "    base_path = Path(\"../../automation/models/kobart_summary_onnx\")\n",
    "    encoder_sess = ort.InferenceSession(str(base_path / \"encoder_model.onnx\"))\n",
    "    decoder_sess = ort.InferenceSession(str(base_path / \"decoder_model.onnx\"))\n",
    "    tokenizer = Tokenizer.from_file(str(base_path / \"tokenizer.json\"))\n",
    "    return encoder_sess, decoder_sess, tokenizer\n",
    "\n",
    "\n",
    "# â¬‡ ë””ì½”ë”© í•¨ìˆ˜ (greedy ë˜ëŠ” beam=1)\n",
    "def summarize_event_focused(\n",
    "    text,\n",
    "    encoder_sess,\n",
    "    decoder_sess,\n",
    "    tokenizer,\n",
    "    max_length=128,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.2,\n",
    "):\n",
    "    input_ids = tokenizer.encode(text).ids\n",
    "    input_ids_np = np.array([input_ids], dtype=np.int64)\n",
    "    attention_mask = np.ones_like(input_ids_np, dtype=np.int64)\n",
    "\n",
    "    encoder_outputs = encoder_sess.run(\n",
    "        None, {\"input_ids\": input_ids_np, \"attention_mask\": attention_mask}\n",
    "    )[0]\n",
    "\n",
    "    decoder_input_ids = [tokenizer.token_to_id(\"<s>\")]\n",
    "    generated_ids = decoder_input_ids.copy()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        decoder_input_np = np.array([generated_ids], dtype=np.int64)\n",
    "        decoder_inputs = {\n",
    "            \"input_ids\": decoder_input_np,\n",
    "            \"encoder_hidden_states\": encoder_outputs,\n",
    "            \"encoder_attention_mask\": attention_mask,\n",
    "        }\n",
    "        logits = decoder_sess.run(None, decoder_inputs)[0]\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "\n",
    "        # repetition penalty ì ìš©\n",
    "        for token_id in set(generated_ids):\n",
    "            next_token_logits[0, token_id] /= repetition_penalty\n",
    "\n",
    "        # no_repeat_ngram_size ì ìš©\n",
    "        if no_repeat_ngram_size > 0 and len(generated_ids) >= no_repeat_ngram_size:\n",
    "            ngram = tuple(generated_ids[-(no_repeat_ngram_size - 1) :])\n",
    "            banned = {\n",
    "                tuple(generated_ids[i : i + no_repeat_ngram_size])\n",
    "                for i in range(len(generated_ids) - no_repeat_ngram_size + 1)\n",
    "            }\n",
    "            for token_id in range(next_token_logits.shape[-1]):\n",
    "                if ngram + (token_id,) in banned:\n",
    "                    next_token_logits[0, token_id] = -1e9  # í° ë§ˆì´ë„ˆìŠ¤\n",
    "\n",
    "        # greedy ì„ íƒ\n",
    "        next_token_id = int(np.argmax(next_token_logits, axis=-1)[0])\n",
    "\n",
    "        if next_token_id == tokenizer.token_to_id(\"</s>\"):\n",
    "            break\n",
    "\n",
    "        generated_ids.append(next_token_id)\n",
    "\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f99488bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_sess, decoder_sess, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6731acf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ìš”ì•½: ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” ì˜ì•½í’ˆ ìœ„íƒ ê°œë°œÂ·ìƒì‚°(CDMO) ë¶€ë¬¸ì€ ì¡´ì†ë²•ì¸ì— ë‚¨ê¸°ê³ , ì‹ ì„¤ë²•ì¸ì¸ ì‚¼ì„±ë°”ì´ì˜¤ì—í”¼ìŠ¤ ì§€ë¶„ 100%ë¥¼ ìŠ¹ê³„í•˜ë„ë¡ í•˜ëŠ” ë°©ì•ˆì„ ì¶”ì§„í•˜ê² ë‹¤ê³  ë°íˆë©° ë³¸ê²©ì ì¸ ì‹ ì•½ ê°œë°œì— ë‚˜ì„œê² ë‹¤ëŠ” ì˜ë¯¸í•˜ê³  í•´ì„í•˜ì˜€ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì…ë ¥ ë¬¸ì¥\n",
    "text = \"\"\"\n",
    "'ì‚¬ì§„=í•œê²½DB\\ní‚¤ì›€ì¦ê¶Œì€ 23ì¼ ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ì— ëŒ€í•´ ì¸ì ë¶„í• ì„ ì¶”ì§„í•˜ëŠ” ê±´ ë³¸ê²©ì ì¸ ì‹ ì•½ ê°œë°œì— ë‚˜ì„œê² ë‹¤ëŠ” ì˜ë¯¸í•˜ê³  í•´ì„í–ˆë‹¤.\\nì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” ì¸ì ë¶„í• ì„ í†µí•´ ì˜ì•½í’ˆ ìœ„íƒ ê°œë°œÂ·ìƒì‚°(CDMO) ë¶€ë¬¸ì€ ì¡´ì†ë²•ì¸ì— ë‚¨ê¸°ê³ , ì‹ ì„¤ë²•ì¸ì¸ ì‚¼ì„±ì—í”¼ìŠ¤í™€ë”©ìŠ¤ì— ì‚¼ì„±ë°”ì´ì˜¤ì—í”¼ìŠ¤ ì§€ë¶„ 100%ë¥¼ ìŠ¹ê³„í•˜ë„ë¡ í•˜ëŠ” ë°©ì•ˆì„ ì¶”ì§„í•˜ê² ë‹¤ê³  ê³µì‹œí–ˆë‹¤.\\nì´ì— ëŒ€í•´ í—ˆí˜œë¯¼ í‚¤ì›€ì¦ê¶Œ ì—°êµ¬ì›ì€ â€œì‚¼ì„±ê·¸ë£¹ì´ (ì‚¼ì„±ì—í”¼ìŠ¤í™€ë”©ìŠ¤ë¥¼ í†µí•´) ë³¸ê²©ì ìœ¼ë¡œ ì‹ ì•½ ê°œë°œì„ ì‹œì‘í•œë‹¤ëŠ” ì˜ë¯¸ë¡œ í•´ì„ëœë‹¤â€ê³  ë§í–ˆë‹¤. ì´ì–´ â€œì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” ê¸€ë¡œë²Œ íŒŒíŠ¸ë„ˆì‹­ í™•ëŒ€ë¥¼ ìœ„í•œ ì˜ì—…í™œë™ì´ ìˆ˜ì›”í•´ì§ˆ ê²ƒâ€ì´ë¼ë©° â€œë¶„í• ëœ ì‹ ì„¤ë²•ì¸ì€ íˆ¬ìì§€ì£¼íšŒì‚¬ë¡œ ì‹ ì„±ì¥ ë™ë ¥ ë°œêµ´ê³¼ ì—°êµ¬Â·ê°œë°œ(R&D) ë° ê¸°ì—… ì¸ìˆ˜Â·í•©ë³‘(M&A)ë¥¼ í†µí•´ ì ê·¹ì ì¸ ì„±ì¥ì„ ì¶”ì§„í•  ê²ƒâ€ì´ë¼ê³  ë‚´ë‹¤ë´¤ë‹¤.\\nì´ë²ˆ ë¶„í•  ì¶”ì§„ì´ ë‹¨ê¸°ì ìœ¼ë¡œ í€ë”ë©˜í„¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ë¯¸ë¯¸í•  ê²ƒìœ¼ë¡œ ë¶„ì„ëë‹¤.\\në‹¤ë§Œ í—ˆ ì—°êµ¬ì›ì€ â€œìµœê·¼ ì¸ì ë¶„í•  ì—…ì²´ë“¤ì˜ ë¶„í• ìƒì¥ ì „ ì£¼ê°€ ìˆ˜ìµë¥ ì´ ëŒ€ì²´ë¡œ ìƒìŠ¹ íë¦„ì„ ë‚˜íƒ€ëƒˆë‹¤â€ë©° â€œì´í›„ ê° ì‚¬ì˜ í€ë”ë©˜í„¸, ìˆ˜ê¸‰, ëª¨ë©˜í…€ì— ë”°ë¼ ë³€ë™í–ˆë‹¤â€ê³  ì „í–ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_event_focused(text, encoder_sess, decoder_sess, tokenizer)\n",
    "\n",
    "print(\"ğŸ“„ ìš”ì•½:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8df65d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# í–‰ ì „ì²´ ì¶œë ¥\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# ì—´ ì „ì²´ ì¶œë ¥\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "# ê° ì—´ ë„ˆë¹„ ë¬´ì œí•œ ì¶œë ¥\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "# ì „ì²´ ë„“ì´ ì œí•œ í•´ì œ\n",
    "pd.set_option(\"display.width\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deb883fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from kss import split_sentences\n",
    "\n",
    "def remove_market_related_sentences(text: str) -> str:\n",
    "    # ì¤„ë°”ê¿ˆ ì œê±°\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # ëŒ€ê´„í˜¸ í¬í•¨ í…ìŠ¤íŠ¸ ì œê±°: [íŒŒì´ë‚¸ì…œë‰´ìŠ¤], [ì‚¬ì§„] ë“±\n",
    "    text = re.sub(r\"\\[[^\\]]*\\]\", \"\", text)\n",
    "\n",
    "    # '/ì‚¬ì§„', '/ì‚¬ì§„ì œê³µ' ì œê±°\n",
    "    text = re.sub(r\"/ì‚¬ì§„(ì œê³µ)?\", \"\", text)\n",
    "\n",
    "    # ì´ë©”ì¼ ì£¼ì†Œ ì œê±° (ì˜ˆ: josh@yna.co.kr)\n",
    "    text = re.sub(r\"\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b\", \"\", text)\n",
    "\n",
    "    # ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬ (ê°„ë‹¨í•˜ê²Œ ë§ˆì¹¨í‘œ ê¸°ì¤€, í•„ìš”ì‹œ KSS ë“± ì ìš© ê°€ëŠ¥)\n",
    "    sentences = split_sentences(text)\n",
    "\n",
    "    # ì œê±°í•  íŒ¨í„´ë“¤ (ë‰´ìŠ¤ ë¬¸ì¥ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” íŒ¨í„´)\n",
    "    patterns = [\n",
    "        r\"(ìì„¸í•œ ë‚´ìš©|ìì„¸í•œ ì‚¬í•­)\",  # ë‰´ìŠ¤ ê¸°ë³¸ í‘œí˜„\n",
    "        r\"\\d{4}[.-]\\d{1,2}[.-]\\d{1,2}\",  # ë‚ ì§œ (ì˜ˆ: 2025.03.26, 2024-12-01)\n",
    "        r\"([0-9,]+(?:ë§Œ)?[0-9,]*\\s?(?:ì›|ë§Œì›))\",  # ê°€ê²© (ì˜ˆ: 3,500ì›, 12000ì›)\n",
    "        r\"(ê°•ì„¸|í€ë“œ|ì‹œê°€ì´ì•¡|ë“±ë½ë¥ |í•œêµ­ê±°ë˜ì†Œ)\",  # ì¦ì‹œ ìš©ì–´\n",
    "        r\"\\([+-]?[0-9.,]+%\\)\",  # ê´„í˜¸ ì•ˆ í¼ì„¼íŠ¸ ë“±ë½ë¥ \n",
    "        r\"(íˆ¬ìì˜ê²¬|ì—°êµ¬ì›|í‰ê°€|ì˜ˆìƒì¹˜|ì¦ê¶Œê°€|ë¦¬í¬íŠ¸|íŒ€ì¥)\",  # ì• ë„ë¦¬ìŠ¤íŠ¸ ìš©ì–´\n",
    "        r\"(ìˆœì´ìµ|ì „ë…„|ë§¤ì¶œ|ì˜ì—…ì´ìµ|ì˜ì—…ì ì|ì¦ì‹œ|ì½”ìŠ¤í”¼|ì½”ìŠ¤ë‹¥|ë‹¤ìš°|ë‚˜ìŠ¤ë‹¥|ë§¤ì¶œì•¡|ê±°ë˜ì¼|í˜¸ì¡°ì„¸|ë ˆë²„ë¦¬ì§€|íˆ¬ìì|ì¡°ì •|ìì‚°|ìˆ˜ìµë¥ |ì´ìµë¥ |ìˆ˜ìµì„±|ë‚´ë¦¬ë§‰|ë¶€ì§„í•œ|ë‚™í­|ê¸°ëŒ€ì¹˜|ì‹¤ì ë°œí‘œ|ê¸°ì—… ê°€ì¹˜)\",  # ì‹œì¥ ìš©ì–´\n",
    "    ]\n",
    "\n",
    "    # í•˜ë‚˜ì˜ í†µí•© íŒ¨í„´ìœ¼ë¡œ ì»´íŒŒì¼\n",
    "    combined_pattern = re.compile(\"|\".join(patterns))\n",
    "\n",
    "    # í•„í„°ë§ëœ ë¬¸ì¥ë§Œ ìœ ì§€\n",
    "    filtered = [s for s in sentences if not combined_pattern.search(s)]\n",
    "\n",
    "    text_preprocessed = \" \".join(filtered)\n",
    "\n",
    "    # print(f\"ì›ë¬¸:{sentences}\\n|\\nì „ì²˜ë¦¬ ëœ ë¬¸ì¥: {text_preprocessed}\\n\\n\")\n",
    "\n",
    "    return text_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5df16d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_5 = df.head(5).copy()\n",
    "df_5['article'] = df_5['article'].apply(remove_market_related_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75c77c57",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_5\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_5' is not defined"
     ]
    }
   ],
   "source": [
    "df_5['article'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a54f312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NER ONNX ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ..\\..\\automoation\\models\\ner_onnx\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForTokenClassification\n",
    "from pathlib import Path\n",
    "\n",
    "model_id = \"KPF/KPF-BERT-NER\"\n",
    "save_dir = Path(\"../../automoation/models/ner_onnx\")\n",
    "\n",
    "# ONNXë¡œ export\n",
    "model = ORTModelForTokenClassification.from_pretrained(model_id, export=True)\n",
    "model.save_pretrained(save_dir)\n",
    "\n",
    "# tokenizerë„ ì €ì¥\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"âœ… NER ONNX ëª¨ë¸ ì €ì¥ ì™„ë£Œ:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "def get_ner_tokenizer():\n",
    "    \"\"\"\n",
    "    ONNX NER ëª¨ë¸ì„ ìœ„í•œ í† í¬ë‚˜ì´ì €ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "\n",
    "    # ğŸŸ¢ í† í¬ë‚˜ì´ì € ë¡œë”©\n",
    "    tokenizer_ner = Tokenizer.from_file(\"../../automation/models/ner_onnx/tokenizer.json\")\n",
    "\n",
    "    # ğŸŸ¢ ONNX ëª¨ë¸ ì„¸ì…˜ ë¡œë”©\n",
    "    session_ner = ort.InferenceSession(\"../../automation/models/ner_onnx/model.onnx\")\n",
    "\n",
    "    return tokenizer_ner, session_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "017227ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from label_map import id2label  # ë¼ë²¨ ID â†” ë¼ë²¨ëª… ë§¤í•‘\n",
    "\n",
    "tokenizer_ner, session_ner = get_ner_tokenizer()\n",
    "\n",
    "def get_ner_tokens(tokenizer, session, text, id2label):\n",
    "    # ğŸŸ¡ í† í°í™” ë° ì…ë ¥ê°’ ì¤€ë¹„\n",
    "    encoding = tokenizer.encode(text)\n",
    "    input_ids = np.array([encoding.ids], dtype=np.int64)\n",
    "    attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
    "    token_type_ids = np.zeros_like(input_ids, dtype=np.int64)\n",
    "\n",
    "    # ğŸ”µ ONNX ì¶”ë¡  ì‹¤í–‰\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "    }\n",
    "\n",
    "    logits = session.run(None, inputs)[0]  # shape: (1, seq_len, num_labels)\n",
    "\n",
    "    # ğŸ”µ ë¼ë²¨ ì¸ë±ìŠ¤ â†’ ì‹¤ì œ ë¼ë²¨ëª…\n",
    "    preds = np.argmax(logits, axis=-1)[0]\n",
    "    labels = [id2label[p] for p in preds[: len(encoding.tokens)]]\n",
    "\n",
    "    # ğŸ”µ ì‹œê°í™”\n",
    "    tokens = encoding.tokens\n",
    "\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d80bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ogg_economy(tokens, labels, target_label=\"OGG_ECONOMY\"):\n",
    "    merged_words = []\n",
    "    current_word = \"\"\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        token_clean = token.replace(\"##\", \"\") if token.startswith(\"##\") else token\n",
    "\n",
    "        if label == f\"B-{target_label}\":\n",
    "            if current_word:\n",
    "                merged_words.append(current_word)\n",
    "            current_word = token_clean\n",
    "\n",
    "        elif label == f\"I-{target_label}\":\n",
    "            current_word += token_clean\n",
    "\n",
    "        else:\n",
    "            if current_word:\n",
    "                merged_words.append(current_word)\n",
    "                current_word = \"\"\n",
    "\n",
    "    if current_word:\n",
    "        merged_words.append(current_word)\n",
    "\n",
    "    stock_list = merged_words.copy()\n",
    "\n",
    "    return stock_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8efe21cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ê³¨ë“œë§Œì‚­ìŠ¤', 'í•œêµ­ì€í–‰', 'ì‚¼ì„±ì „ì', 'MBKíŒŒíŠ¸ë„ˆìŠ¤']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from label_map import id2label  # ë¼ë²¨ ID â†” ë¼ë²¨ëª… ë§¤í•‘\n",
    "\n",
    "text = \"\"\"\n",
    "ì„¸ê³„ 2ìœ„ íˆ¬ìì€í–‰(IB)ì¸ ê³¨ë“œë§Œì‚­ìŠ¤ì˜ ì‚¬ì¥ ê²¸ ìµœê³ ìš´ì˜ì±…ì„ì(COO)ì¸ ì¡´ ì›”ë“œë¡ ì´ ë°©í•œí•´ ì´ì°½ìš© í•œêµ­ì€í–‰ ì´ì¬, ì´ì¬ìš© ì‚¼ì„±ì „ì íšŒì¥, ê¹€ë³‘ì£¼ MBKíŒŒíŠ¸ë„ˆìŠ¤ íšŒì¥ ë“±ê³¼ í•¨ê»˜ í•œêµ­ì˜ ê¸ˆìœµë‹¹êµ­, ê¸°ì—… ë° ê¸ˆìœµê³„ì™€ì˜ í˜‘ë ¥ ë°©ì•ˆì„ ë…¼ì˜í–ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "tokens, labels = get_ner_tokens(tokenizer_ner, session_ner, text, id2label)\n",
    "stock_list = extract_ogg_economy(tokens, labels, target_label=\"OGG_ECONOMY\")\n",
    "stock_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7de794ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì‚¬ì§„=í•œê²½DB\\ní‚¤ì›€ì¦ê¶Œì€ 23ì¼ ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ì— ëŒ€í•´ ì¸ì ë¶„í• ì„ ì¶”ì§„í•˜ëŠ” ê±´ ë³¸ê²©ì ì¸ ì‹ ì•½ ê°œë°œì— ë‚˜ì„œê² ë‹¤ëŠ” ì˜ë¯¸í•˜ê³  í•´ì„í–ˆë‹¤.\\nì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” ì¸ì ë¶„í• ì„ í†µí•´ ì˜ì•½í’ˆ ìœ„íƒ ê°œë°œÂ·ìƒì‚°(CDMO) ë¶€ë¬¸ì€ ì¡´ì†ë²•ì¸ì— ë‚¨ê¸°ê³ , ì‹ ì„¤ë²•ì¸ì¸ ì‚¼ì„±ì—í”¼ìŠ¤í™€ë”©ìŠ¤ì— ì‚¼ì„±ë°”ì´ì˜¤ì—í”¼ìŠ¤ ì§€ë¶„ 100%ë¥¼ ìŠ¹ê³„í•˜ë„ë¡ í•˜ëŠ” ë°©ì•ˆì„ ì¶”ì§„í•˜ê² ë‹¤ê³  ê³µì‹œí–ˆë‹¤.\\nì´ì— ëŒ€í•´ í—ˆí˜œë¯¼ í‚¤ì›€ì¦ê¶Œ ì—°êµ¬ì›ì€ â€œì‚¼ì„±ê·¸ë£¹ì´ (ì‚¼ì„±ì—í”¼ìŠ¤í™€ë”©ìŠ¤ë¥¼ í†µí•´) ë³¸ê²©ì ìœ¼ë¡œ ì‹ ì•½ ê°œë°œì„ ì‹œì‘í•œë‹¤ëŠ” ì˜ë¯¸ë¡œ í•´ì„ëœë‹¤â€ê³  ë§í–ˆë‹¤. ì´ì–´ â€œì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ëŠ” ê¸€ë¡œë²Œ íŒŒíŠ¸ë„ˆì‹­ í™•ëŒ€ë¥¼ ìœ„í•œ ì˜ì—…í™œë™ì´ ìˆ˜ì›”í•´ì§ˆ ê²ƒâ€ì´ë¼ë©° â€œë¶„í• ëœ ì‹ ì„¤ë²•ì¸ì€ íˆ¬ìì§€ì£¼íšŒì‚¬ë¡œ ì‹ ì„±ì¥ ë™ë ¥ ë°œêµ´ê³¼ ì—°êµ¬Â·ê°œë°œ(R&D) ë° ê¸°ì—… ì¸ìˆ˜Â·í•©ë³‘(M&A)ë¥¼ í†µí•´ ì ê·¹ì ì¸ ì„±ì¥ì„ ì¶”ì§„í•  ê²ƒâ€ì´ë¼ê³  ë‚´ë‹¤ë´¤ë‹¤.\\nì´ë²ˆ ë¶„í•  ì¶”ì§„ì´ ë‹¨ê¸°ì ìœ¼ë¡œ í€ë”ë©˜í„¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ë¯¸ë¯¸í•  ê²ƒìœ¼ë¡œ ë¶„ì„ëë‹¤.\\në‹¤ë§Œ í—ˆ ì—°êµ¬ì›ì€ â€œìµœê·¼ ì¸ì ë¶„í•  ì—…ì²´ë“¤ì˜ ë¶„í• ìƒì¥ ì „ ì£¼ê°€ ìˆ˜ìµë¥ ì´ ëŒ€ì²´ë¡œ ìƒìŠ¹ íë¦„ì„ ë‚˜íƒ€ëƒˆë‹¤â€ë©° â€œì´í›„ ê° ì‚¬ì˜ í€ë”ë©˜í„¸, ìˆ˜ê¸‰, ëª¨ë©˜í…€ì— ë”°ë¼ ë³€ë™í–ˆë‹¤â€ê³  ì „í–ˆë‹¤.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../db/news_2023_2025_summarized_view.csv')\n",
    "df['article'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a89b7375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>stock_list</th>\n",
       "      <th>industry_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20250523_0002</td>\n",
       "      <td>23ì¼ ì •ë³´ê¸°ìˆ (IT)Â·íˆ¬ìì€í–‰(IB) ì—…ê³„ì— ë”°ë¥´ë©´ êµ­ë‚´ ëŒ€í‘œ ì „ìê²°ì œì‚¬ì—…ìì¸ ì¹´...</td>\n",
       "      <td>['ì¹´ì¹´ì˜¤í˜ì´']</td>\n",
       "      <td>['ê¸ˆìœµ ì§€ì› ì„œë¹„ìŠ¤ì—…']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20250523_0004</td>\n",
       "      <td>ì„¸ê³„ 2ìœ„ íˆ¬ìì€í–‰(IB)ì¸ ê³¨ë“œë§Œì‚­ìŠ¤ì˜ ì‚¬ì¥ ê²¸ ìµœê³ ìš´ì˜ì±…ì„ì(COO)ì¸ ì¡´ ì›”ë“œ...</td>\n",
       "      <td>['ì‚¼ì„±ì „ì']</td>\n",
       "      <td>['í†µì‹  ë° ë°©ì†¡ ì¥ë¹„ ì œì¡°ì—…']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20250523_0007</td>\n",
       "      <td>23ì¼ ì •ë³´ê¸°ìˆ (IT)Â·íˆ¬ìì€í–‰(IB) ì—…ê³„ì— ë”°ë¥´ë©´ ì¹´ì¹´ì˜¤í˜ì´ê°€ SSGë‹·ì»´ ì“±í˜ì´...</td>\n",
       "      <td>['ì¹´ì¹´ì˜¤í˜ì´']</td>\n",
       "      <td>['ê¸ˆìœµ ì§€ì› ì„œë¹„ìŠ¤ì—…']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20250523_0010</td>\n",
       "      <td>23ì¼ íš¨ì„±ì¤‘ê³µì—…ì€ ì¡° íšŒì¥ì´ ìƒì†ì„¸ ì¬ì›ì„ ë§ˆë ¨í•˜ê¸° ìœ„í•´ ì‹œê°„ ì™¸ ë§¤ë§¤ë¡œ íš¨ì„±ì¤‘ ...</td>\n",
       "      <td>['íš¨ì„±ì¤‘ê³µì—…']</td>\n",
       "      <td>['ì „ë™ê¸°, ë°œì „ê¸° ë° ì „ê¸° ë³€í™˜ Â· ê³µê¸‰ Â· ì œì–´ ì¥ì¹˜ ì œì¡°ì—…']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20250523_0011</td>\n",
       "      <td>ì¹´ì¹´ì˜¤í˜ì´ê°€ ì‹ ì„¸ê³„ ì´ë§ˆíŠ¸ ì¸¡ì—ì„œ ì“±í˜ì´Â·ìŠ¤ë§ˆì¼í˜ì´ ì¸ìˆ˜ë¥¼ ì¶”ì§„í•˜ê³  ë‚˜ì„  ê±´ êµ­ë‚´ ...</td>\n",
       "      <td>['ì¹´ì¹´ì˜¤í˜ì´']</td>\n",
       "      <td>['ê¸ˆìœµ ì§€ì› ì„œë¹„ìŠ¤ì—…']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         news_id                                            summary  \\\n",
       "0  20250523_0002  23ì¼ ì •ë³´ê¸°ìˆ (IT)Â·íˆ¬ìì€í–‰(IB) ì—…ê³„ì— ë”°ë¥´ë©´ êµ­ë‚´ ëŒ€í‘œ ì „ìê²°ì œì‚¬ì—…ìì¸ ì¹´...   \n",
       "1  20250523_0004  ì„¸ê³„ 2ìœ„ íˆ¬ìì€í–‰(IB)ì¸ ê³¨ë“œë§Œì‚­ìŠ¤ì˜ ì‚¬ì¥ ê²¸ ìµœê³ ìš´ì˜ì±…ì„ì(COO)ì¸ ì¡´ ì›”ë“œ...   \n",
       "2  20250523_0007  23ì¼ ì •ë³´ê¸°ìˆ (IT)Â·íˆ¬ìì€í–‰(IB) ì—…ê³„ì— ë”°ë¥´ë©´ ì¹´ì¹´ì˜¤í˜ì´ê°€ SSGë‹·ì»´ ì“±í˜ì´...   \n",
       "3  20250523_0010  23ì¼ íš¨ì„±ì¤‘ê³µì—…ì€ ì¡° íšŒì¥ì´ ìƒì†ì„¸ ì¬ì›ì„ ë§ˆë ¨í•˜ê¸° ìœ„í•´ ì‹œê°„ ì™¸ ë§¤ë§¤ë¡œ íš¨ì„±ì¤‘ ...   \n",
       "4  20250523_0011  ì¹´ì¹´ì˜¤í˜ì´ê°€ ì‹ ì„¸ê³„ ì´ë§ˆíŠ¸ ì¸¡ì—ì„œ ì“±í˜ì´Â·ìŠ¤ë§ˆì¼í˜ì´ ì¸ìˆ˜ë¥¼ ì¶”ì§„í•˜ê³  ë‚˜ì„  ê±´ êµ­ë‚´ ...   \n",
       "\n",
       "  stock_list                          industry_list  \n",
       "0  ['ì¹´ì¹´ì˜¤í˜ì´']                         ['ê¸ˆìœµ ì§€ì› ì„œë¹„ìŠ¤ì—…']  \n",
       "1   ['ì‚¼ì„±ì „ì']                     ['í†µì‹  ë° ë°©ì†¡ ì¥ë¹„ ì œì¡°ì—…']  \n",
       "2  ['ì¹´ì¹´ì˜¤í˜ì´']                         ['ê¸ˆìœµ ì§€ì› ì„œë¹„ìŠ¤ì—…']  \n",
       "3  ['íš¨ì„±ì¤‘ê³µì—…']  ['ì „ë™ê¸°, ë°œì „ê¸° ë° ì „ê¸° ë³€í™˜ Â· ê³µê¸‰ Â· ì œì–´ ì¥ì¹˜ ì œì¡°ì—…']  \n",
       "4  ['ì¹´ì¹´ì˜¤í˜ì´']                         ['ê¸ˆìœµ ì§€ì› ì„œë¹„ìŠ¤ì—…']  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['news_id', 'summary', 'stock_list', 'industry_list']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aca8af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../db/news_2023_2025_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a399d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-0602",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
