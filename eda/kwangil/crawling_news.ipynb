{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b5c5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "362df51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2147483646"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ctypes\n",
    "\n",
    "# SetThreadExecutionState: ì‹œìŠ¤í…œì´ ìŠ¬ë¦½í•˜ê±°ë‚˜ í™”ë©´ì´ êº¼ì§€ëŠ” ê²ƒ ë°©ì§€\n",
    "ctypes.windll.kernel32.SetThreadExecutionState(0x80000002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "62cfb19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENTS = [\n",
    "    # ìƒëµ ì—†ì´ 20ê°œ ì „ì²´ í¬í•¨\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/125.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/124.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_2) Gecko/20100101 Firefox/110.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:110.0) Gecko/20100101 Firefox/110.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/110.0.0.0 Safari/537.36 Edg/110.0.1587.57\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 16_2 like Mac OS X) AppleWebKit/605.1.15 Version/16.2 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Linux; Android 13; SM-S918N) AppleWebKit/537.36 Chrome/113.0.0.0 Mobile Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Linux; Android 10; SM-G970F) AppleWebKit/537.36 Chrome/80.0.3987.119 SamsungBrowser/13.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/124.0.0.0 Safari/537.36 Brave/124.0.0.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/123.0.0.0 Safari/537.36 OPR/89.0.4447.83\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) Chrome/117.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; WOW64) Chrome/118.0.5993.90 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPad; CPU OS 15_5 like Mac OS X) AppleWebKit/605.1.15 Version/15.5 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/125.0.0.1 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) Gecko/20100101 Firefox/111.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 11_6_5) AppleWebKit/605.1.15 Version/15.5 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 Chrome/104.0.0.0 Safari/537.36\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_random_headers():\n",
    "    return {\n",
    "        \"User-Agent\": random.choice(USER_AGENTS),\n",
    "        \"Referer\": \"https://www.google.com\",\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_to_public_url(href):\n",
    "    parsed = urlparse(href)\n",
    "    params = parse_qs(parsed.query)\n",
    "    article_id = params.get(\"article_id\", [\"\"])[0]\n",
    "    office_id = params.get(\"office_id\", [\"\"])[0]\n",
    "    if article_id and office_id:\n",
    "        return f\"https://n.news.naver.com/mnews/article/{office_id}/{article_id}\"\n",
    "    return href\n",
    "\n",
    "\n",
    "def fetch_article_details(url):\n",
    "    try:\n",
    "        res = requests.get(url, headers=get_random_headers(), timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "        # ë³¸ë¬¸\n",
    "        article_elem = soup.select_one(\"article#dic_area\")\n",
    "        article = (\n",
    "            article_elem.get_text(strip=True, separator=\"\\n\") if article_elem else \"\"\n",
    "        )\n",
    "\n",
    "        # ì´ë¯¸ì§€\n",
    "        image_elem = soup.select_one('meta[property=\"og:image\"]')\n",
    "        image = image_elem[\"content\"] if image_elem else \"\"\n",
    "\n",
    "        return article, image\n",
    "    except Exception:\n",
    "        return \"\", \"\"\n",
    "\n",
    "\n",
    "def fetch_news_by_date(date: str, max_pages: int = 10):\n",
    "    all_news = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        try:\n",
    "            url = f\"https://finance.naver.com/news/news_list.naver?mode=LSS3D&section_id=101&section_id2=258&section_id3=402&date={date}&page={page}\"\n",
    "            res = requests.get(url, headers=get_random_headers(), timeout=10)\n",
    "            res.raise_for_status()\n",
    "            soup = BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "            subject_tags = soup.select(\"dl > dd.articleSubject, dl > dt.articleSubject\")\n",
    "            summary_tags = soup.select(\"dl > dd.articleSummary\")\n",
    "\n",
    "            if not subject_tags or not summary_tags:\n",
    "                break\n",
    "\n",
    "            for subject_tag, summary_tag in zip(subject_tags, summary_tags):\n",
    "                try:\n",
    "                    a_tag = subject_tag.a\n",
    "                    if not a_tag:\n",
    "                        continue\n",
    "\n",
    "                    title = a_tag.get(\"title\") or a_tag.text.strip()\n",
    "                    article_url = convert_to_public_url(a_tag[\"href\"])\n",
    "                    press = summary_tag.select_one(\".press\").text.strip()\n",
    "                    wdate = summary_tag.select_one(\".wdate\").text.strip()\n",
    "\n",
    "                    print(f\"ğŸ“° í¬ë¡¤ë§ ì¤‘: [{wdate}] {title} ({press}) - {article_url}\")\n",
    "\n",
    "                    # ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸, ìš”ì•½, ì´ë¯¸ì§€ ì¶”ê°€\n",
    "                    article_text, image = fetch_article_details(article_url)\n",
    "\n",
    "                    all_news.append(\n",
    "                        {\n",
    "                            \"wdate\": wdate,\n",
    "                            \"title\": title,\n",
    "                            \"article\": article_text,\n",
    "                            \"press\": press,\n",
    "                            \"url\": article_url,\n",
    "                            \"image\": image,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    time.sleep(random.uniform(0.5, 1.5))  # ëœë¤ ëŒ€ê¸° ì‹œê°„\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return all_news\n",
    "\n",
    "\n",
    "def save_news_to_csv(news_data, date_str, folder=\"news_data\"):\n",
    "    # ë‚ ì§œ íŒŒì‹±í•´ì„œ ì—°ë„ì™€ ì›” ì¶”ì¶œ\n",
    "    date_obj = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    year = date_obj.strftime(\"%Y\")\n",
    "    month = date_obj.strftime(\"%m\")\n",
    "\n",
    "    # í´ë” êµ¬ì¡°: news_data/YYYY/MM/\n",
    "    full_path = os.path.join(folder, year, month)\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "    # CSV ì €ì¥\n",
    "    file_path = os.path.join(full_path, f\"{date_str}.csv\")\n",
    "    pd.DataFrame(news_data).to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "def crawl_news_range(start_date_str, end_date_str, max_pages=5, folder=\"news_data\"):\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y%m%d\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y%m%d\")\n",
    "    total_days = (end_date - start_date).days + 1\n",
    "    current_date = start_date\n",
    "\n",
    "    for i in range(total_days):\n",
    "        date_str = current_date.strftime(\"%Y%m%d\")\n",
    "        progress = (i + 1) / total_days * 100\n",
    "        print(f\"[{i + 1}/{total_days}] ğŸ“… í¬ë¡¤ë§ ì¤‘: {date_str} ({progress:.1f}%)\")\n",
    "\n",
    "        daily_news = fetch_news_by_date(date_str, max_pages=max_pages)\n",
    "        if daily_news:\n",
    "            save_news_to_csv(daily_news, date_str, folder)\n",
    "\n",
    "        current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ad9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_news_range(\"20250423\", \"20250523\", max_pages=20, folder=\"news_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-0521-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
