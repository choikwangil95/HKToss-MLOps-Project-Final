from news_pipeline import fetch_latest_news, remove_market_related_sentences, get_summarize_model, summarize_event_focused, get_ner_pipeline, get_stock_names, load_official_stock_list, filter_official_stocks_from_list, load_stock_to_industry_map, get_industry_list_from_stocks, save_to_db_metadata
import schedule
import time
import logging
import os
import joblib

log = logging.getLogger("news_logger")


def job(tokenizer_summarize, model_summarize, device, ner_pipe, official_stock_set, stock_to_industry, vectorizer, lda_model, stopwords):
    log.info("ğŸ•’ [ìŠ¤ì¼€ì¤„ëŸ¬] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1 ë‰´ìŠ¤ ì‹¤ì‹œê°„ ìˆ˜ì§‘
    # - 1ë¶„ë§ˆë‹¤ ìˆ˜ì§‘
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # 1 ë‰´ìŠ¤ ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì‹¤í–‰ í•¨ìˆ˜
    news_crawled = fetch_latest_news()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2 ë‰´ìŠ¤ ì „ì²˜ë¦¬
    # - ë³¸ë¬¸ ì „ì²˜ë¦¬ ë° ìš”ì•½, ì¢…ëª©ê³¼ ì—…ì¢…ëª… ë§¤ì¹­
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # 1 ë‰´ìŠ¤ ë³¸ë¬¸ ì „ì²˜ë¦¬ ì‹¤í–‰ í•¨ìˆ˜
    filtered_news = []

    if len(news_crawled) != 0:
        for news in news_crawled:
            news_article = news["article"]
            news_article_preprocessed = remove_market_related_sentences(news_article)

            if len(news_article_preprocessed) < 70:
                continue  # ë³¸ë¬¸ ê¸¸ì´ ì§§ìœ¼ë©´ ì œì™¸

            news["article_preprocessed"] = news_article_preprocessed
            filtered_news.append(news)

    print(f"\ní•„í„°ë¦°ë­ ë‰´ìŠ¤ {filtered_news}\n")

    # 2 ë‰´ìŠ¤ ë³¸ë¬¸ ìš”ì•… í•¨ìˆ˜
    summarzied_news = []

    if len(filtered_news) != 0:
        for news in filtered_news:
            news_article = news["article_preprocessed"]
            news_article_summarized = summarize_event_focused(news_article, tokenizer_summarize, model_summarize, device)

            if len(news_article_summarized) < 70:
                continue  # ë³¸ë¬¸ ê¸¸ì´ ì§§ìœ¼ë©´ ì œì™¸

            news["summary"] = news_article_summarized
            summarzied_news.append(news)

    print(f"\nìš”ì•½ ë‰´ìŠ¤ í™•ì¸ {summarzied_news}\n")

    ner_news = []

    # 3 ë‰´ìŠ¤ ì¢…ëª©, ì—…ì¢…ëª… ë§¤ì¹­ í•¨ìˆ˜
    if len(summarzied_news) != 0:
        for news in summarzied_news:
            news_summary = news["summary"]
            stock_list = get_stock_names(ner_pipe, news_summary)

            # ğŸ”½ ì—¬ê¸°ì„œ í•„í„°ë§
            stock_list = filter_official_stocks_from_list(stock_list, official_stock_set)

            if len(stock_list) > 4 or len(stock_list) < 1:
                continue  # ì¢…ëª© ì—†ê±°ë‚˜ ë„ˆë¬´ ë§ìœ¼ë©´ ì œì™¸

            industry_list = get_industry_list_from_stocks(stock_list, stock_to_industry)

            if len(industry_list) < 1:
                continue

            news["stock_list"] = stock_list
            news["industry_list"] = industry_list

            ner_news.append(news)

    print(f"\nì¢…ëª©, ì—…ì¢…ëª… ë§¤ì¹­ ë‰´ìŠ¤ í™•ì¸ {ner_news}\n")

    save_to_db_metadata(ner_news)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3 ë‰´ìŠ¤ ê²½ì œ ë° í–‰ë™ ì§€í‘œ í”¼ì³ ì¶”ê°€
# - ì£¼ê°€ D+1~D+30 ë³€ë™ë¥ , ê¸ˆë¦¬, í™˜ìœ¨, ê¸°ê´€ ë§¤ë§¤ë™í–¥, ìœ ê°€ ë“±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ìœ¼ì•…

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4 ë‰´ìŠ¤ ì‹œë©˜í‹± í”¼ì³ ì¶”ê°€
# - topicë³„ ë¶„í¬ê°’, í´ëŸ¬ìŠ¤í„° ë™ì¼ ì—¬ë¶€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ìœ¼ì•…


if __name__ == "__main__":
    log.info("ğŸŸ¡ summarize ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    tokenizer_summarize, model_summarize, device = get_summarize_model()
    log.info("ğŸŸ¢ summarize ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    log.info("ğŸŸ¡ NER ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    ner_pipe = get_ner_pipeline()
    log.info("ğŸŸ¢ NER ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    BASE_DIR = os.path.dirname(os.path.abspath(__file__))

    official_stock_path = os.path.join(BASE_DIR, "../db/KRX_KOSPI.csv")
    industry_map_path = os.path.join(BASE_DIR, "../db/kospi_description.csv")

    official_stock_set = load_official_stock_list(official_stock_path)
    stock_to_industry = load_stock_to_industry_map(industry_map_path)

    lda_model_path = os.path.join(BASE_DIR, "../db/best_lda_model.pkl")
    count_vectorizer_path = os.path.join(BASE_DIR, "../db/count_vectorizer.pkl")
    stopwords_path = os.path.join(BASE_DIR, "../db/stopwords-ko.txt")

    vectorizer = joblib.load(count_vectorizer_path)
    lda_model = joblib.load(lda_model_path)
    with open(stopwords_path, "r", encoding="utf-8") as f:
        stopwords = [word.strip() for word in f.readlines()]

    log.info("âœ… run_scheduler.py ì‹œì‘ë¨")

    # ì²« ì‹¤í–‰ ì¦‰ì‹œ
    job(tokenizer_summarize, model_summarize, device, ner_pipe, official_stock_set, stock_to_industry, vectorizer, lda_model, stopwords)

    # ì´í›„ ë§¤ 1ë¶„ë§ˆë‹¤ ì‹¤í–‰
    schedule.every(1).minutes.do(lambda: job(tokenizer_summarize, model_summarize, ner_pipe, device, official_stock_set, stock_to_industry, vectorizer, lda_model, stopwords))

    while True:
        schedule.run_pending()
        time.sleep(1)
