# news_pipeline.py
import requests, random, time, os, logging, concurrent.futures
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse, parse_qs
from requests.adapters import HTTPAdapter, Retry
from logging.handlers import RotatingFileHandler
import psycopg2
from psycopg2.extras import execute_batch

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Œ ë¡œê·¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

log = logging.getLogger("news_logger")
log.setLevel(logging.INFO)

# ë¡œê·¸ í´ë” ìƒì„±
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOG_DIR = os.path.join(BASE_DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)

# ğŸ”§ í¬ë§·í„° ì„¤ì •
formatter = logging.Formatter(
    fmt="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# ğŸ”§ íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •
log_path = os.path.join(LOG_DIR, "news.log")
file_handler = RotatingFileHandler(
    log_path,
    maxBytes=5 * 1024 * 1024,  # 5MB ë„˜ìœ¼ë©´ ìˆœí™˜
    backupCount=3             # ìµœëŒ€ 3ê°œ ë°±ì—… ë³´ê´€
)
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
log.addHandler(file_handler)

# ğŸ”§ ì½˜ì†” ì¶œë ¥ë„ ì›í•  ê²½ìš° (ì„ íƒ)
stream_handler = logging.StreamHandler()
stream_handler.setLevel(logging.INFO)
stream_handler.setFormatter(formatter)
log.addHandler(stream_handler)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Œ ìœ í‹¸ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

NEWS_URL = "https://finance.naver.com/news/news_list.naver?mode=LSS3D&section_id=101&section_id2=258&section_id3=402"
LAST_CRAWLED_FILE = "automation/data/last_crawled.txt"

def parse_wdate(text):
    return datetime.strptime(text, "%Y-%m-%d %H:%M")

def convert_to_public_url(href):
    parsed = urlparse(href)
    params = parse_qs(parsed.query)
    article_id = params.get("article_id", [""])[0]
    office_id = params.get("office_id", [""])[0]
    if article_id and office_id:
        return f"https://n.news.naver.com/mnews/article/{office_id}/{article_id}"
    return href

def get_random_headers():
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15",
        "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0",
        "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15A372 Safari/604.1",
    ]
    return {
        "User-Agent": random.choice(user_agents),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Referer": "https://www.google.com/",
        "DNT": "1",
        "Upgrade-Insecure-Requests": "1",
    }

def safe_soup_parse(text, timeout=3):
    def parse():
        return BeautifulSoup(text, "lxml")

    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(parse)
        try:
            return future.result(timeout=timeout)
        except concurrent.futures.TimeoutError:
            log.error("soup íŒŒì‹± íƒ€ì„ì•„ì›ƒ ë°œìƒ")
            return None
        except Exception as e:
            log.error(f"soup íŒŒì‹± ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return None

def get_retry_session():
    session = requests.Session()
    retries = Retry(total=3, backoff_factor=2, status_forcelist=[429, 500, 502, 503, 504])
    session.mount("https://", HTTPAdapter(max_retries=retries))
    return session

def fetch_article_details(url):
    try:
        headers = get_random_headers()
        # log.info(f"ğŸ“° ìš”ì²­ URL: {url}")

        session = get_retry_session()
        res = session.get(url, headers=headers, timeout=10)
        # log.info(f"ğŸ“… ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {res.status_code}")
        res.raise_for_status()
        # log.info(f"ğŸ“„ ì‘ë‹µ ë³¸ë¬¸ ê¸¸ì´: {len(res.text)}")

        soup = safe_soup_parse(res.text)
        if soup is None:
            return None, ""

        # log.info("soup ìƒì„± ì™„ë£Œ")

        image_tag = soup.select_one('meta[property="og:image"]')
        image = image_tag["content"] if image_tag and image_tag.has_attr("content") else None

        article_tag = soup.select_one("article#dic_area")
        article = article_tag.get_text(strip=True, separator="\n") if article_tag else ""

        # log.info(f"ì¶”ì¶œ ì„±ê³µ: ì´ë¯¸ì§€ ìˆìŒ? {bool(image)}, ë³¸ë¬¸ ê¸¸ì´: {len(article)}")

        # ìš”ì²­ ì‚¬ì´ì— ë¬´ì‘ìœ„ ëŒ€ê¸°
        time.sleep(random.uniform(1.0, 2.5))

        return image, article

    except Exception as e:
        log.error(f"ì „ì²´ fetch ì‹¤íŒ¨ - {url}: {type(e).__name__}: {e}")
        return None, ""

def get_or_create_last_time(filepath: str) -> str:
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M")

    try:
        if os.path.exists(filepath):
            with open(filepath, "r") as f:
                last_time = f.read().strip()
            if last_time:
                log.info(f"ì´ì „ ê¸°ë¡ëœ ì‹œê°„: {last_time}")
                return last_time
        log.info("ê¸°ë¡ì´ ì—†ì–´ í˜„ì¬ ì‹œê°ìœ¼ë¡œ ì´ˆê¸°í™”")
        with open(filepath, "w") as f:
            f.write(now_str)
        return now_str
    except Exception as e:
        log.error(f"ì‹œê°„ ì½ê¸°/ì“°ê¸° ì‹¤íŒ¨ ({type(e).__name__}): {e}")
        return now_str

def save_latest_time(filepath: str, time_str: str):
    try:
        with open(filepath, "w") as f:
            f.write(time_str)
        log.info(f"ìµœì‹  ì‹œê°„ ì €ì¥: {time_str}")
    except Exception as e:
        log.error(f"ì‹œê°„ ì €ì¥ ì‹¤íŒ¨ ({type(e).__name__}): {e}")


# ì¤‘ë³µ ë°©ì§€ìš© ì„¸íŠ¸
generated_ids = set()

def generate_news_id(date_str):
    while True:
        rand_part = f"{random.randint(0, 99999999):08d}"  # 0000 ~ 9999
        news_id = f"{date_str}_{rand_part}"
        if news_id not in generated_ids:
            generated_ids.add(news_id)
            return news_id
        

def save_to_db(articles):
    if not articles:
        log.info("ì €ì¥í•  ë‰´ìŠ¤ ì—†ìŒ")
        return

    try:
        DB_URL = os.getenv("DATABASE_URL", "postgresql://postgres:password@localhost:5432/news_db")
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor()

        insert_query = """
        INSERT INTO news_v2 (news_id, wdate, title, article, press, url, image)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (news_id) DO NOTHING;
        """

        values = [
            (
                article["news_id"],
                parse_wdate(article["wdate"]),
                article["title"],
                article["article"],
                article["press"],
                article["url"],
                article["image"]
            )
            for article in articles
        ]

        execute_batch(cur, insert_query, values)
        conn.commit()

        log.info(f"ğŸ§¾ DB ì €ì¥ ì™„ë£Œ: {len(values)}ê±´ ì €ì¥")

    except Exception as e:
        log.error(f"âŒ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ({type(e).__name__}): {e}")

    finally:
        if cur:
            cur.close()
        if conn:
            conn.close()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë©”ì¸ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_latest_news():
    try:
        res = requests.get(NEWS_URL, headers=get_random_headers(), timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "lxml")
    except Exception as e:
        log.error(f"ë‰´ìŠ¤ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨ ({type(e).__name__}): {e}")
        return []

    last_time = get_or_create_last_time(LAST_CRAWLED_FILE)
    last_time_dt = parse_wdate(last_time)

    articles = soup.select("dl > dd.articleSummary")
    new_articles = []

    for article in articles:
        try:
            title_tag = article.find_previous_sibling("dd", class_="articleSubject").a
            title = title_tag.text.strip()
            url = convert_to_public_url(title_tag["href"])
            press = article.select_one(".press").text.strip()
            wdate = article.select_one(".wdate").text.strip()
            article_time_dt = parse_wdate(wdate)

            if article_time_dt > last_time_dt:
                new_articles.append({
                    "wdate": wdate,
                    "title": title,
                    "press": press,
                    "url": url,
                })
        except Exception as e:
            log.error(f"ê°œë³„ ë‰´ìŠ¤ íŒŒì‹± ì‹¤íŒ¨ ({type(e).__name__}): {e}")

    log.info(f"ìƒˆ ë‰´ìŠ¤ ìˆ˜: {len(new_articles)}")

    new_articles_crawled = []

    if new_articles:
        latest_time = max(parse_wdate(a["wdate"]) for a in new_articles)
        save_latest_time(LAST_CRAWLED_FILE, latest_time.strftime("%Y-%m-%d %H:%M"))

        for article in new_articles:
            try:
                # log.info(f"\nê¸°ì‚¬ ì²˜ë¦¬ ì¤‘: {article['title']}")
                image, article_text = fetch_article_details(article["url"])

                wdate = article['wdate']
                title = article['title']
                press = article['press']
                url = article['url']

                article_time_dt = parse_wdate(wdate)
                date_str = article_time_dt.strftime("%Y%m%d")

                news_id = generate_news_id(date_str)

                new_articles_crawled.append({
                    "news_id": news_id,
                    "wdate": wdate,
                    "title": title,
                    "article": article_text,
                    "press": press,
                    "url": url,
                    "image": image,
                })

                preview = article_text[:300] if isinstance(article_text, str) else ""
                log.info(f"[NEW] {article['wdate']} - {article['title']} ({article['press']})")
                log.info(f"{preview}...\n")
            except Exception as e:
                log.error(f"ë³¸ë¬¸ íŒŒì‹± ì‹¤íŒ¨ ({type(e).__name__}): {e}")
    else:
        log.info("ìƒˆ ë‰´ìŠ¤ ì—†ìŒ")

    if new_articles_crawled:
        save_to_db(new_articles_crawled)

    return new_articles_crawled

if __name__ == "__main__":
    log.info("ë¡œê·¸ í…ŒìŠ¤íŠ¸: news_pipeline.py ì§ì ‘ ì‹¤í–‰ë¨")
    fetch_latest_news()