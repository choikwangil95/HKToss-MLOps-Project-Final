# news_pipeline.py
import requests, random, time, os, logging, concurrent.futures
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse, parse_qs
from requests.adapters import HTTPAdapter, Retry
from logging.handlers import RotatingFileHandler
import psycopg2
from psycopg2.extras import execute_batch
import re
from kss import split_sentences
import os
import pandas as pd
import re
import numpy as np
import json
import redis
from dotenv import load_dotenv
import ast
from pykrx import stock
from datetime import timedelta

load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Œ ë¡œê·¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


log = logging.getLogger("news_logger")
log.setLevel(logging.INFO)

# ë¡œê·¸ í´ë” ìƒì„±
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOG_DIR = os.path.join(BASE_DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)

# ğŸ”§ í¬ë§·í„° ì„¤ì •
formatter = logging.Formatter(
    fmt="%(asctime)s [%(levelname)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
)

# ğŸ”§ íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •
log_path = os.path.join(LOG_DIR, "news.log")
file_handler = RotatingFileHandler(
    log_path,
    maxBytes=5 * 1024 * 1024,  # 5MB ë„˜ìœ¼ë©´ ìˆœí™˜
    backupCount=3,  # ìµœëŒ€ 3ê°œ ë°±ì—… ë³´ê´€
)
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
log.addHandler(file_handler)

# ğŸ”§ ì½˜ì†” ì¶œë ¥ë„ ì›í•  ê²½ìš° (ì„ íƒ)
stream_handler = logging.StreamHandler()
stream_handler.setLevel(logging.INFO)
stream_handler.setFormatter(formatter)
log.addHandler(stream_handler)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Œ ìœ í‹¸ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

LAST_CRAWLED_FILE = os.path.join(DATA_DIR, "last_crawled.txt")


def parse_wdate(text):
    return datetime.strptime(text, "%Y-%m-%d %H:%M")


def convert_to_public_url(href):
    parsed = urlparse(href)
    params = parse_qs(parsed.query)
    article_id = params.get("article_id", [""])[0]
    office_id = params.get("office_id", [""])[0]
    if article_id and office_id:
        return f"https://n.news.naver.com/mnews/article/{office_id}/{article_id}"
    return href


def get_random_headers():
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15",
        "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0",
        "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15A372 Safari/604.1",
    ]
    return {
        "User-Agent": random.choice(user_agents),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Referer": "https://www.google.com/",
        "DNT": "1",
        "Upgrade-Insecure-Requests": "1",
    }


def safe_soup_parse(text, timeout=3):
    def parse():
        return BeautifulSoup(text, "lxml")

    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(parse)
        try:
            return future.result(timeout=timeout)
        except concurrent.futures.TimeoutError:
            log.error("soup íŒŒì‹± íƒ€ì„ì•„ì›ƒ ë°œìƒ")
            return None
        except Exception as e:
            log.error(f"soup íŒŒì‹± ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return None


def get_retry_session():
    session = requests.Session()
    retries = Retry(
        total=3, backoff_factor=2, status_forcelist=[429, 500, 502, 503, 504]
    )
    session.mount("https://", HTTPAdapter(max_retries=retries))
    return session


def fetch_article_details(url):
    try:
        headers = get_random_headers()
        # log.info(f"ğŸ“° ìš”ì²­ URL: {url}")

        session = get_retry_session()
        res = session.get(url, headers=headers, timeout=10)
        # log.info(f"ğŸ“… ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {res.status_code}")
        res.raise_for_status()
        # log.info(f"ğŸ“„ ì‘ë‹µ ë³¸ë¬¸ ê¸¸ì´: {len(res.text)}")

        soup = safe_soup_parse(res.text)
        if soup is None:
            return None, ""

        # log.info("soup ìƒì„± ì™„ë£Œ")

        image_tag = soup.select_one('meta[property="og:image"]')
        image = (
            image_tag["content"]
            if image_tag and image_tag.has_attr("content")
            else None
        )

        article_tag = soup.select_one("article#dic_area")
        article = (
            article_tag.get_text(strip=True, separator="\n") if article_tag else ""
        )

        # log.info(f"ì¶”ì¶œ ì„±ê³µ: ì´ë¯¸ì§€ ìˆìŒ? {bool(image)}, ë³¸ë¬¸ ê¸¸ì´: {len(article)}")

        # ìš”ì²­ ì‚¬ì´ì— ë¬´ì‘ìœ„ ëŒ€ê¸°
        time.sleep(random.uniform(1.0, 2.5))

        return image, article

    except Exception as e:
        log.error(f"ì „ì²´ fetch ì‹¤íŒ¨ - {url}: {type(e).__name__}: {e}")
        return None, ""


def get_or_create_last_time(filepath: str) -> str:
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M")

    try:
        if os.path.exists(filepath):
            with open(filepath, "r") as f:
                last_time = f.read().strip()
            if last_time:
                log.info(f"ì´ì „ ê¸°ë¡ëœ ì‹œê°„: {last_time}")
                return last_time
        log.info("ê¸°ë¡ì´ ì—†ì–´ í˜„ì¬ ì‹œê°ìœ¼ë¡œ ì´ˆê¸°í™”")
        with open(filepath, "w") as f:
            f.write(now_str)
        return now_str
    except Exception as e:
        log.error(f"ì‹œê°„ ì½ê¸°/ì“°ê¸° ì‹¤íŒ¨ ({type(e).__name__}): {e}")
        return now_str


def save_latest_time(filepath: str, time_str: str):
    try:
        with open(filepath, "w") as f:
            f.write(time_str)
        log.info(f"ìµœì‹  ì‹œê°„ ì €ì¥: {time_str}")
    except Exception as e:
        log.error(f"ì‹œê°„ ì €ì¥ ì‹¤íŒ¨ ({type(e).__name__}): {e}")


# ì¤‘ë³µ ë°©ì§€ìš© ì„¸íŠ¸
generated_ids = set()


def generate_news_id(date_str):
    while True:
        rand_part = f"{random.randint(0, 99999999):08d}"  # 0000 ~ 9999
        news_id = f"{date_str}_{rand_part}"
        if news_id not in generated_ids:
            generated_ids.add(news_id)
            return news_id


def save_to_db(articles):
    if not articles:
        log.info("ì €ì¥í•  ë‰´ìŠ¤ ì—†ìŒ")
        return

    conn = None  # âœ… ë¨¼ì € Noneìœ¼ë¡œ ì´ˆê¸°í™”
    cur = None

    try:
        DB_URL = os.getenv(
            "DATABASE_URL", "postgresql://postgres:password@localhost:5432/news_db"
        )
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor()

        insert_query = """
		INSERT INTO news_v2 (news_id, wdate, title, article, press, url, image)
		VALUES (%s, %s, %s, %s, %s, %s, %s)
		ON CONFLICT (news_id) DO NOTHING;
		"""

        values = [
            (
                article["news_id"],
                parse_wdate(article["wdate"]),
                article["title"],
                article["article"],
                article["press"],
                article["url"],
                article["image"],
            )
            for article in articles
        ]

        execute_batch(cur, insert_query, values)
        conn.commit()

        log.info(f"ğŸ§¾ DB ì €ì¥ ì™„ë£Œ: {len(values)}ê±´ ì €ì¥")

    except Exception as e:
        log.error(f"âŒ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ({type(e).__name__}): {e}")

    finally:
        if cur:
            cur.close()
        if conn:
            conn.close()


def save_to_db_metadata(articles):
    if not articles:
        log.info("ì €ì¥í•  ë‰´ìŠ¤ ì—†ìŒ")
        return

    try:
        DB_URL = os.getenv(
            "DATABASE_URL", "postgresql://postgres:password@localhost:5432/news_db"
        )
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor()

        insert_query = """
		INSERT INTO news_v2_metadata (news_id, summary, stock_list, stock_list_view, industry_list, impact_score)
		VALUES (%s, %s, %s, %s, %s, %s)
		ON CONFLICT (news_id) DO NOTHING;
		"""

        values = [
            (
                article["news_id"],
                article["summary"],
                (
                    json.dumps(article["stock_list"], ensure_ascii=False)
                    if article["stock_list"] is not None
                    else None
                ),
                (
                    json.dumps(article["stock_list_view"], ensure_ascii=False)
                    if article["stock_list_view"] is not None
                    else None
                ),
                (
                    json.dumps(article["industry_list"], ensure_ascii=False)
                    if article["industry_list"] is not None
                    else None
                ),
                None,  # impact_scoreëŠ” ì•„ì§ ê³„ì‚°ë˜ì§€ ì•ŠìŒ
            )
            for article in articles
        ]

        execute_batch(cur, insert_query, values)
        conn.commit()

        log.info(f"ğŸ§¾ Metadata DB ì €ì¥ ì™„ë£Œ: {len(values)}ê±´ ì €ì¥")

    except Exception as e:
        log.error(f"âŒ Metadata DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ({type(e).__name__}): {e}")

    finally:
        if cur:
            cur.close()
        if conn:
            conn.close()


def save_to_db_topics(articles):
    if not articles:
        log.info("ì €ì¥í•  ë‰´ìŠ¤ ì—†ìŒ")
        return

    insert_query = """
    INSERT INTO news_v2_topic (news_id, topic_1, topic_2, topic_3, topic_4, topic_5, topic_6, topic_7, topic_8, topic_9)
    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
    ON CONFLICT (news_id) DO NOTHING;
    """

    values = [
        (
            article["news_id"],
            article["topic_1"],
            article["topic_2"],
            article["topic_3"],
            article["topic_4"],
            article["topic_5"],
            article["topic_6"],
            article["topic_7"],
            article["topic_8"],
            article["topic_9"],
        )
        for article in articles
    ]

    try:
        DB_URL = os.getenv(
            "DATABASE_URL", "postgresql://postgres:password@localhost:5432/news_db"
        )
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor()

        execute_batch(cur, insert_query, values)
        conn.commit()

        log.info(f"ğŸ§¾ Topic DB ì €ì¥ ì™„ë£Œ: {len(values)}ê±´ ì €ì¥")

    except Exception as e:
        log.error(f"âŒ Topic DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ({type(e).__name__}): {e}")

    finally:
        if cur:
            cur.close()
        if conn:
            conn.close()


def save_to_db_external(market_datas):
    if not market_datas:
        log.info("ì €ì¥í•  ë‰´ìŠ¤ ì—†ìŒ")
        return

    columns = [
        "news_id",
        "d_minus_5_date_close",
        "d_minus_5_date_volume",
        "d_minus_5_date_foreign",
        "d_minus_5_date_institution",
        "d_minus_5_date_individual",
        "d_minus_4_date_close",
        "d_minus_4_date_volume",
        "d_minus_4_date_foreign",
        "d_minus_4_date_institution",
        "d_minus_4_date_individual",
        "d_minus_3_date_close",
        "d_minus_3_date_volume",
        "d_minus_3_date_foreign",
        "d_minus_3_date_institution",
        "d_minus_3_date_individual",
        "d_minus_2_date_close",
        "d_minus_2_date_volume",
        "d_minus_2_date_foreign",
        "d_minus_2_date_institution",
        "d_minus_2_date_individual",
        "d_minus_1_date_close",
        "d_minus_1_date_volume",
        "d_minus_1_date_foreign",
        "d_minus_1_date_institution",
        "d_minus_1_date_individual",
        "d_plus_1_date_close",
        "d_plus_2_date_close",
        "d_plus_3_date_close",
        "d_plus_4_date_close",
        "d_plus_5_date_close",
        "fx",
        "bond10y",
        "base_rate",
    ]

    insert_query = f"""
        INSERT INTO news_v2_external ({', '.join(columns)})
        VALUES ({', '.join(['%s'] * len(columns))})
        ON CONFLICT (news_id) DO NOTHING;
    """

    values = [
        tuple(market_data.get(col, None) for col in columns)
        for market_data in market_datas
    ]

    try:
        DB_URL = os.getenv(
            "DATABASE_URL", "postgresql://postgres:password@localhost:5432/news_db"
        )
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor()

        execute_batch(cur, insert_query, values)
        conn.commit()

        log.info(f"ğŸ§¾ External DB ì €ì¥ ì™„ë£Œ: {len(values)}ê±´ ì €ì¥")

    except Exception as e:
        log.error(f"âŒ External DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ({type(e).__name__}): {e}")

    finally:
        try:
            cur.close()
            conn.close()
        except:
            pass


def update_db_impact_score(score_datas):
    if not score_datas:
        log.info("ì—…ë°ì´íŠ¸í•  ë°ì´í„° ì—†ìŒ")
        return

    update_query = """
        UPDATE news_v2_metadata
        SET impact_score = %s,
            d_plus_1_date_close = %s,
            d_plus_2_date_close = %s,
            d_plus_3_date_close = %s,
            d_plus_4_date_close = %s,
            d_plus_5_date_close = %s
        WHERE news_id = %s;
    """

    values = [
        (
            data["score"],
            data["d_plus"][0],
            data["d_plus"][1],
            data["d_plus"][2],
            data["d_plus"][3],
            data["d_plus"][4],
            data["news_id"],
        )
        for data in score_datas
    ]

    try:
        DB_URL = os.getenv(
            "DATABASE_URL", "postgresql://postgres:password@localhost:5432/news_db"
        )
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor()

        execute_batch(cur, update_query, values)
        conn.commit()

        log.info(f"ğŸ§¾ Impact Score ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(values)}ê±´")

    except Exception as e:
        log.error(f"âŒ Impact Score ì—…ë°ì´íŠ¸ ì˜¤ë¥˜ ({type(e).__name__}): {e}")

    finally:
        try:
            cur.close()
            conn.close()
        except:
            pass


def update_db_external(score_datas):
    if not score_datas:
        log.info("ì—…ë°ì´íŠ¸í•  ë°ì´í„° ì—†ìŒ")
        return

    update_query = """
        UPDATE news_v2_external
        SET d_plus_1_date_close = %s,
            d_plus_2_date_close = %s,
            d_plus_3_date_close = %s,
            d_plus_4_date_close = %s,
            d_plus_5_date_close = %s
        WHERE news_id = %s;
    """

    values = [
        (
            data["d_plus"][0],
            data["d_plus"][1],
            data["d_plus"][2],
            data["d_plus"][3],
            data["d_plus"][4],
            data["news_id"],
        )
        for data in score_datas
    ]

    try:
        DB_URL = os.getenv(
            "DATABASE_URL", "postgresql://postgres:password@localhost:5432/news_db"
        )
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor()

        execute_batch(cur, update_query, values)
        conn.commit()

        log.info(f"ğŸ§¾ External ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(values)}ê±´")

    except Exception as e:
        log.error(f"âŒ External ì—…ë°ì´íŠ¸ ì˜¤ë¥˜ ({type(e).__name__}): {e}")

    finally:
        try:
            cur.close()
            conn.close()
        except:
            pass


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë©”ì¸ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def fetch_latest_news():
    NEWS_URL = "https://finance.naver.com/news/news_list.naver?mode=LSS3D&section_id=101&section_id2=258&section_id3=402"

    try:
        res = requests.get(NEWS_URL, headers=get_random_headers(), timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "lxml")
    except Exception as e:
        log.error(f"ë‰´ìŠ¤ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨ ({type(e).__name__}): {e}")
        return []

    last_time = get_or_create_last_time(LAST_CRAWLED_FILE)
    last_time_dt = parse_wdate(last_time)

    articles = soup.select("dl > dd.articleSummary")
    new_articles = []

    for article in articles:
        try:
            title_tag = article.find_previous_sibling("dd", class_="articleSubject").a
            title = title_tag.text.strip()
            url = convert_to_public_url(title_tag["href"])
            press = article.select_one(".press").text.strip()
            wdate = article.select_one(".wdate").text.strip()
            article_time_dt = parse_wdate(wdate)

            if article_time_dt > last_time_dt:
                new_articles.append(
                    {
                        "wdate": wdate,
                        "title": title,
                        "press": press,
                        "url": url,
                    }
                )
        except Exception as e:
            log.error(f"ê°œë³„ ë‰´ìŠ¤ íŒŒì‹± ì‹¤íŒ¨ ({type(e).__name__}): {e}")

    log.info(f"ìƒˆ ë‰´ìŠ¤ ìˆ˜: {len(new_articles)}")

    new_articles_crawled = []

    if new_articles:
        latest_time = max(parse_wdate(a["wdate"]) for a in new_articles)
        save_latest_time(LAST_CRAWLED_FILE, latest_time.strftime("%Y-%m-%d %H:%M"))

        for article in new_articles:
            try:
                # log.info(f"\nê¸°ì‚¬ ì²˜ë¦¬ ì¤‘: {article['title']}")
                image, article_text = fetch_article_details(article["url"])

                wdate = article["wdate"]
                title = article["title"]
                press = article["press"]
                url = article["url"]

                article_time_dt = parse_wdate(wdate)
                date_str = article_time_dt.strftime("%Y%m%d")

                news_id = generate_news_id(date_str)

                new_articles_crawled.append(
                    {
                        "news_id": news_id,
                        "wdate": wdate,
                        "title": title,
                        "article": article_text,
                        "press": press,
                        "url": url,
                        "image": image,
                    }
                )

                preview = article_text[:300] if isinstance(article_text, str) else ""
                log.info(
                    f"[NEW] {article['wdate']} - {article['title']} ({article['press']})"
                )
            except Exception as e:
                log.error(f"ë³¸ë¬¸ íŒŒì‹± ì‹¤íŒ¨ ({type(e).__name__}): {e}")
    else:
        log.info("ìƒˆ ë‰´ìŠ¤ ì—†ìŒ")

    return new_articles_crawled


def enrich_stock_list(stock_names_raw, stock_name_to_code):
    try:
        stock_names = stock_names_raw
        result = []
        for name in stock_names:
            code = stock_name_to_code.get(name)
            if code:
                result.append({"stock_id": str(code), "stock_name": name})
        return result
    except Exception:
        return []


def extract_industries(stock_list, code_to_industry):
    if len(stock_list) == 0:
        return []

    industries = []
    seen = set()
    for stock in stock_list:
        stock_id = stock.get("stock_id")
        if stock_id is None:
            continue
        industry = code_to_industry.get(int(stock_id))
        if industry:
            key = (stock_id, industry["industry_id"])
            if key not in seen:
                seen.add(key)
                industries.append(
                    {
                        "stock_id": str(stock_id),
                        "industry_id": industry["industry_id"],
                        "industry_name": industry["industry_name"],
                    }
                )
    return industries


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë©”ì¸ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def remove_market_related_sentences(text: str) -> str:
    if not isinstance(text, str) or text.strip() == "":
        return ""

    # ì¤„ë°”ê¿ˆ ì œê±°
    text = text.replace("\n", " ")

    # ëŒ€ê´„í˜¸ í¬í•¨ í…ìŠ¤íŠ¸ ì œê±°: [íŒŒì´ë‚¸ì…œë‰´ìŠ¤], [ì‚¬ì§„] ë“±
    text = re.sub(r"\[[^\]]*\]", "", text)

    # '/ì‚¬ì§„', '/ì‚¬ì§„ì œê³µ' ì œê±°
    text = re.sub(r"/ì‚¬ì§„(ì œê³µ)?", "", text)

    # ì´ë©”ì¼ ì£¼ì†Œ ì œê±° (ì˜ˆ: josh@yna.co.kr)
    text = re.sub(r"\b[\w.-]+@[\w.-]+\.\w+\b", "", text)

    # ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬ (ê°„ë‹¨í•˜ê²Œ ë§ˆì¹¨í‘œ ê¸°ì¤€, í•„ìš”ì‹œ KSS ë“± ì ìš© ê°€ëŠ¥)
    try:
        sentences = split_sentences(text)
    except Exception as e:
        print(f"[â—KSS ì˜¤ë¥˜] ë¬¸ì¥ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
        sentences = text.split(". ")

    # ì œê±°í•  íŒ¨í„´ë“¤ (ë‰´ìŠ¤ ë¬¸ì¥ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” íŒ¨í„´)
    patterns = [
        r"(ìì„¸í•œ ë‚´ìš©|ìì„¸í•œ ì‚¬í•­)",  # ë‰´ìŠ¤ ê¸°ë³¸ í‘œí˜„
        r"\d{4}[.-]\d{1,2}[.-]\d{1,2}",  # ë‚ ì§œ (ì˜ˆ: 2025.03.26, 2024-12-01)
        r"([0-9,]+(?:ë§Œ)?[0-9,]*\s?(?:ì›|ë§Œì›))",  # ê°€ê²© (ì˜ˆ: 3,500ì›, 12000ì›)
        r"(ê°•ì„¸|í€ë“œ|ì‹œê°€ì´ì•¡|ë“±ë½ë¥ |í•œêµ­ê±°ë˜ì†Œ)",  # ì¦ì‹œ ìš©ì–´
        r"\([+-]?[0-9.,]+%\)",  # ê´„í˜¸ ì•ˆ í¼ì„¼íŠ¸ ë“±ë½ë¥ 
        r"(íˆ¬ìì˜ê²¬|ì—°êµ¬ì›|í‰ê°€|ì˜ˆìƒì¹˜|ì¦ê¶Œê°€|ë¦¬í¬íŠ¸|íŒ€ì¥)",  # ì• ë„ë¦¬ìŠ¤íŠ¸ ìš©ì–´
        r"(ìˆœì´ìµ|ì „ë…„|ë§¤ì¶œ|ì˜ì—…ì´ìµ|ì˜ì—…ì ì|ì¦ì‹œ|ì½”ìŠ¤í”¼|ì½”ìŠ¤ë‹¥|ë‹¤ìš°|ë‚˜ìŠ¤ë‹¥|ë§¤ì¶œì•¡|ê±°ë˜ì¼|í˜¸ì¡°ì„¸|ë ˆë²„ë¦¬ì§€|íˆ¬ìì|ì¡°ì •|ìì‚°|ìˆ˜ìµë¥ |ì´ìµë¥ |ìˆ˜ìµì„±|ë‚´ë¦¬ë§‰|ë¶€ì§„í•œ|ë‚™í­|ê¸°ëŒ€ì¹˜|ì‹¤ì ë°œí‘œ|ê¸°ì—… ê°€ì¹˜)",  # ì‹œì¥ ìš©ì–´
    ]

    # í•˜ë‚˜ì˜ í†µí•© íŒ¨í„´ìœ¼ë¡œ ì»´íŒŒì¼
    combined_pattern = re.compile("|".join(patterns))

    # í•„í„°ë§ëœ ë¬¸ì¥ë§Œ ìœ ì§€
    filtered = [s for s in sentences if not combined_pattern.search(s)]

    text_preprocessed = " ".join(filtered)

    # print(f"ì›ë¬¸:{sentences}\n|\nì „ì²˜ë¦¬ ëœ ë¬¸ì¥: {text_preprocessed}\n\n")

    return text_preprocessed


def get_article_summary(
    text,
):
    url = "http://15.165.211.100:8000/plm/summarize"  # ë˜ëŠ” EC2 ë‚´ë¶€/ì™¸ë¶€ ì£¼ì†Œ
    payload = {"article": text}

    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()

        summary = response.json()["summary"]  # ë˜ëŠ” ì‹¤ì œ ë¦¬í„´ í•„ë“œì— ë”°ë¼ ì¡°ì •

        return summary

    except Exception as e:
        print(f"âŒ ìš”ì•½ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return ""


def get_stock_list(text):
    # ğŸŸ¡ í† í°í™” ë° ì…ë ¥ê°’ ì¤€ë¹„

    url = "http://15.165.211.100:8000/plm/stocks"
    payload = {"article": text}

    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()["stock_list"]  # í˜¹ì€ API ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì •
    except Exception as e:
        print(f"âŒ ì¢…ëª©ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []


def get_lda_topic(text):
    # ğŸŸ¡ í† í°í™” ë° ì…ë ¥ê°’ ì¤€ë¹„

    url = "http://15.165.211.100:8000/news/topics"
    payload = {"article": text}

    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()["lda_topics"]  # í˜¹ì€ API ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì •
    except Exception as e:
        print(f"âŒ ì¢…ëª©ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []


# ì¢…ëª©ëª… ì§‘í•© ë¶ˆëŸ¬ì˜¤ê¸°
def load_official_stock_list(krx_csv_path):
    df = pd.read_csv(krx_csv_path, encoding="cp949")

    stock_list = list(set(df["ì¢…ëª©ëª…"].dropna().unique()))
    stock_name_to_code = dict(zip(df["ì¢…ëª©ëª…"], df["ì¢…ëª©ì½”ë“œ"]))
    return stock_list, stock_name_to_code


# ì¢…ëª© ë¦¬ìŠ¤íŠ¸ì—ì„œ ê³µì‹ ì¢…ëª©ë§Œ í•„í„°ë§
def filter_official_stocks_from_list(stock_list, official_stock_set):
    return [stock for stock in stock_list if stock in official_stock_set]


# ì¢…ëª© â†’ ì—…ì¢… ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
def load_stock_to_industry_map(kospi_desc_csv_path):
    df = pd.read_csv(kospi_desc_csv_path, encoding="cp949")

    industry_list = dict(zip(df["ì¢…ëª©ëª…"], df["ì—…ì¢…ëª…"]))

    code_to_industry = {
        row["ì¢…ëª©ì½”ë“œ"]: {
            "industry_id": str(row["ì—…ì¢…ì½”ë“œ"]),
            "industry_name": row["ì—…ì¢…ëª…"],
        }
        for _, row in df.iterrows()
    }

    return industry_list, code_to_industry


# ê¸°ì¤€ê¸ˆë¦¬ ë°ì´í„°í”„ë ˆì„ ë¡œë“œ
def load_rate_df(rate_path):
    rate_df = pd.read_csv(rate_path)
    rate_df["date"] = pd.to_datetime(rate_df["date"])
    rate_df = rate_df.sort_values("date")

    return rate_df


# ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ì—…ì¢… ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
def get_industry_list_from_stocks(stock_list, stock_to_industry):
    # ì¡°ê±´: ì¢…ëª© ìˆ˜ê°€ 1~4ê°œê°€ ì•„ë‹ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if not (1 <= len(stock_list) <= 4):
        return []

    return [
        stock_to_industry.get(stock["stock_id"], "")
        for stock in stock_list
        if stock_to_industry.get(stock["stock_id"], "") != ""
    ]


def get_news_deduplicate_by_title(news_list):
    seen_titles = set()
    deduped_news = []

    for news in news_list:
        title = news.get("title")
        if title not in seen_titles:
            seen_titles.add(title)
            deduped_news.append(news)

    return deduped_news


def predict_topic_for_df(df, vectorizer, lda_model, stopwords, n_topics=9):
    # 2. í˜•íƒœì†Œ ë¶„ì„ê¸° ë° ë¶ˆìš©ì–´ ë¡œë“œ
    okt = Okt()

    # 3. í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜
    def clean_text(text):
        text = re.sub(r"\[.*?\]|\(.*?\)", "", text)
        text = re.sub(r"[^ê°€-í£\s]", "", text)
        text = re.sub(r"\s+", " ", text).strip()
        return text

    # 4. ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜
    def extract_nouns(text):
        nouns = okt.nouns(text)
        nouns = [word for word in nouns if word not in stopwords and len(word) > 1]
        return " ".join(nouns)

    # 5. ë°ì´í„° ì „ì²˜ë¦¬ (ì •ì œ + ëª…ì‚¬ ì¶”ì¶œ)
    processed_texts = [extract_nouns(clean_text(text)) for text in df["summary"]]

    # 6. ë²¡í„°ë¼ì´ì¦ˆ (DTM ìƒì„±)
    new_dtm = vectorizer.transform(processed_texts)

    # 7. LDA í† í”½ ë¶„í¬ ì˜ˆì¸¡
    topic_distribution = lda_model.transform(new_dtm)

    # 8. ê²°ê³¼ DataFrame ìƒì„± (news_id, ì£¼ìš” í† í”½, í† í”½1~í† í”½n)
    topic_columns = [f"í† í”½ {i+1}" for i in range(n_topics)]
    topic_data = np.concatenate([topic_distribution], axis=1)
    topic_df = pd.DataFrame(topic_data, columns=topic_columns)
    topic_df["news_id"] = df["news_id"].values

    # 9. news_id ê¸°ì¤€ìœ¼ë¡œ merge
    result_df = pd.merge(df, topic_df, on="news_id", how="left")

    return result_df


def send_to_redis(news_data):
    try:
        r = redis.Redis(
            host="43.200.17.139",
            port=6379,
            password="q1w2e3r4!@#",
            decode_responses=True,  # bytes ëŒ€ì‹  str ë¡œ ë°›ê¸°
        )
        if not r.ping():
            log.error("Redis ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        # Redisì— ì €ì¥
        for news in news_data:
            channel = "news-channel"

            wdate = datetime.strptime(news["wdate"], "%Y-%m-%d %H:%M").isoformat()
            data = {
                "news_id": news["news_id"],
                "wdate": wdate,
                "title": news["title"],
                "article": news["article"],
                "press": news["press"],
                "url": news["url"],
                "image": news["image"],
                "impact_score": news["impact_score"],
            }
            message = json.dumps(data, ensure_ascii=False)
            r.publish(channel, message)

        log.info(f"Redisì— {len(news_data)}ê±´ ë‰´ìŠ¤ í‘¸ì‹œ ì™„ë£Œ")
    except Exception as e:
        log.error(f"Redis í‘¸ì‹œ ì‹¤íŒ¨ ({type(e).__name__}): {e}")


class NewsMarketPipeline:

    def __init__(self, news_list, df_base_rate):
        self.api_key = os.getenv("KOREA_BANK_API_KEY")

        self.df = pd.DataFrame(news_list)
        self.ticker_name_map = None
        self.trading_days = None
        self.ohlcv_dict = {}
        self.trading_dict = {}
        self.fx_df = None
        self.bond_df = None
        self.rate_df = df_base_rate

    def get_df(self):
        return self.df

    def extract_stock_name(self):
        if "stock_list" not in self.df.columns:
            raise Exception(
                "stock_list ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ì»¬ëŸ¼: "
                + str(self.df.columns.tolist())
            )

        def get_last_stock_name(x):
            try:
                items = ast.literal_eval(x) if isinstance(x, str) else x
                return items[-1]["stock_name"] if items else None
            except:
                return None

        self.df["stock_name"] = self.df["stock_list"].apply(get_last_stock_name)

    def add_news_date(self):
        if "wdate" in self.df.columns:
            self.df["wdate"] = pd.to_datetime(self.df["wdate"])
            self.df["news_date"] = self.df["wdate"].dt.normalize()
        elif "news_date" in self.df.columns:
            self.df["news_date"] = pd.to_datetime(self.df["news_date"])
        else:
            raise Exception(
                "wdate/news_date ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ì»¬ëŸ¼: "
                + str(self.df.columns.tolist())
            )

    def get_ticker_name_map(self, recent_date="2025-05-30"):
        kospi_tickers = stock.get_market_ticker_list(date=recent_date, market="KOSPI")
        return {
            stock.get_market_ticker_name(ticker): ticker for ticker in kospi_tickers
        }

    def add_ticker(self):
        if self.ticker_name_map is None:
            self.ticker_name_map = self.get_ticker_name_map()

        self.df["ticker"] = self.df["stock_name"].apply(
            lambda name: self.ticker_name_map.get(name) if pd.notna(name) else None
        )

    def get_trading_days(self, start_year=2022, end_year=2026):
        days = []
        for y in range(start_year, end_year + 1):
            for m in range(1, 13):
                try:
                    days_this_month = stock.get_previous_business_days(year=y, month=m)
                    days.extend(days_this_month)
                except:
                    pass
        return pd.to_datetime(sorted(set(days)))

    def adjust_to_nearest_trading_day(self, date):
        idx = self.trading_days.searchsorted(date, side="right") - 1
        if idx >= 0:
            return self.trading_days[idx]
        return pd.NaT

    def add_trading_dates(self):
        if self.trading_days is None:
            self.trading_days = self.get_trading_days()

        self.df["d_day_date"] = self.df["news_date"].apply(
            self.adjust_to_nearest_trading_day
        )

        offsets = {
            "d_minus_5_date": -5,
            "d_minus_4_date": -4,
            "d_minus_3_date": -3,
            "d_minus_2_date": -2,
            "d_minus_1_date": -1,
            "d_day_date": 0,
        }

        def fill_offsets(row):
            d_day = row["d_day_date"]
            if not pd.isna(d_day):
                weekday = d_day.weekday()
                if weekday == 5:
                    d_day = self.adjust_to_nearest_trading_day(
                        d_day - timedelta(days=1)
                    )
                elif weekday == 6:
                    d_day = self.adjust_to_nearest_trading_day(
                        d_day - timedelta(days=2)
                    )

            res = {}
            if pd.isna(d_day):
                for k in offsets:
                    res[k] = pd.NaT
                return pd.Series(res)

            idx = self.trading_days.searchsorted(d_day)
            for k, v in offsets.items():
                i = idx + v
                res[k] = (
                    self.trading_days[i] if 0 <= i < len(self.trading_days) else pd.NaT
                )
            return pd.Series(res)

        df_offsets = self.df.apply(fill_offsets, axis=1)
        self.df = pd.concat(
            [self.df.reset_index(drop=True), df_offsets.reset_index(drop=True)], axis=1
        )

    def fetch_ohlcv_and_trading(self):
        offsets = [f"d_minus_{i}_date" for i in range(1, 6)]
        all_dates = (
            pd.concat([self.df[col] for col in offsets], ignore_index=True)
            .dropna()
            .unique()
        )
        all_dates_str = sorted(
            [pd.to_datetime(d).strftime("%Y%m%d") for d in all_dates]
        )
        tickers = self.df["ticker"].dropna().unique().tolist()

        for ticker in tickers:
            try:
                self.ohlcv_dict[ticker] = stock.get_market_ohlcv_by_date(
                    min(all_dates_str), max(all_dates_str), ticker
                )
            except:
                pass
            try:
                self.trading_dict[ticker] = stock.get_market_trading_value_by_date(
                    min(all_dates_str), max(all_dates_str), ticker
                )
            except:
                pass

    def add_ohlcv_and_trading(self):
        offsets = [f"d_minus_{i}_date" for i in range(1, 6)]

        all_ohlcv_rows = []
        for ticker, df in self.ohlcv_dict.items():
            df = df.reset_index().rename(columns={"ë‚ ì§œ": "date"})
            df["ticker"] = ticker
            all_ohlcv_rows.append(df[["date", "ticker", "ì¢…ê°€", "ê±°ë˜ëŸ‰"]])
        df_ohlcv_all = pd.concat(all_ohlcv_rows) if all_ohlcv_rows else pd.DataFrame()

        all_trading_rows = []
        for ticker, df in self.trading_dict.items():
            df = df.reset_index().rename(columns={"ë‚ ì§œ": "date"})
            df["ticker"] = ticker
            df = df[["date", "ticker", "ì™¸êµ­ì¸í•©ê³„", "ê¸°ê´€í•©ê³„", "ê°œì¸"]]
            all_trading_rows.append(df)
        df_trading_all = (
            pd.concat(all_trading_rows) if all_trading_rows else pd.DataFrame()
        )

        for col in offsets:
            self.df = (
                self.df.merge(
                    df_ohlcv_all,
                    how="left",
                    left_on=[col, "ticker"],
                    right_on=["date", "ticker"],
                )
                .rename(columns={"ì¢…ê°€": f"{col}_close", "ê±°ë˜ëŸ‰": f"{col}_volume"})
                .drop(columns="date")
            )
            self.df = (
                self.df.merge(
                    df_trading_all,
                    how="left",
                    left_on=[col, "ticker"],
                    right_on=["date", "ticker"],
                )
                .rename(
                    columns={
                        "ì™¸êµ­ì¸í•©ê³„": f"{col}_foreign",
                        "ê¸°ê´€í•©ê³„": f"{col}_institution",
                        "ê°œì¸": f"{col}_individual",
                    }
                )
                .drop(columns="date")
            )

    def fetch_fx(self, start_date, end_date):
        if self.fx_df is not None:
            return self.fx_df
        url = f"https://ecos.bok.or.kr/api/StatisticSearch/{self.api_key}/json/kr/1/1000/731Y001/D/{start_date}/{end_date}/0000001/"
        resp = requests.get(url).json()
        if "StatisticSearch" not in resp or "row" not in resp["StatisticSearch"]:
            return pd.DataFrame()
        df = pd.DataFrame(resp["StatisticSearch"]["row"])
        df["date"] = pd.to_datetime(df["TIME"], format="%Y%m%d")
        df["usdkrw"] = pd.to_numeric(df["DATA_VALUE"], errors="coerce")
        self.fx_df = df[["date", "usdkrw"]].sort_values("date")
        return self.fx_df

    def fetch_bond10y(self, start_date, end_date):
        if self.bond_df is not None:
            return self.bond_df
        url = f"https://ecos.bok.or.kr/api/StatisticSearch/{self.api_key}/json/kr/1/1000/817Y002/D/{start_date}/{end_date}/010200000/"
        resp = requests.get(url).json()
        if "StatisticSearch" not in resp or "row" not in resp["StatisticSearch"]:
            return pd.DataFrame()
        df = pd.DataFrame(resp["StatisticSearch"]["row"])
        df["date"] = pd.to_datetime(df["TIME"], format="%Y%m%d")
        df["bond10y"] = pd.to_numeric(df["DATA_VALUE"], errors="coerce")
        self.bond_df = df[["date", "bond10y"]].sort_values("date")
        return self.bond_df

    def add_external_vars(self):
        self.df = self.df.sort_values("news_date")
        if self.trading_days is None:
            self.trading_days = self.get_trading_days()
        raw_start = self.df["news_date"].min() - timedelta(days=1)
        raw_end = self.df["news_date"].max() - timedelta(days=1)
        start_date = self.adjust_to_nearest_trading_day(raw_start)
        end_date = self.adjust_to_nearest_trading_day(raw_end)
        if pd.isna(start_date) or pd.isna(end_date):
            return

        start_str, end_str = start_date.strftime("%Y%m%d"), end_date.strftime("%Y%m%d")
        fx_df = self.fetch_fx(start_str, end_str)
        bond_df = self.fetch_bond10y(start_str, end_str)

        if not fx_df.empty:
            self.df = pd.merge_asof(
                self.df,
                fx_df.rename(columns={"date": "news_date", "usdkrw": "fx"}),
                on="news_date",
                direction="backward",
            )
        if not bond_df.empty:
            self.df = pd.merge_asof(
                self.df,
                bond_df.rename(columns={"date": "news_date"}),
                on="news_date",
                direction="backward",
            )
        if self.rate_df is not None and not self.rate_df.empty:
            self.df = pd.merge_asof(
                self.df,
                self.rate_df.rename(columns={"date": "news_date", "rate": "base_rate"}),
                on="news_date",
                direction="backward",
            )

    def run(self):
        steps = [
            ("extract_stock_name", self.extract_stock_name),
            ("add_news_date", self.add_news_date),
            ("add_ticker", self.add_ticker),
            ("add_trading_dates", self.add_trading_dates),
            ("fetch_ohlcv_and_trading", self.fetch_ohlcv_and_trading),
            ("add_ohlcv_and_trading", self.add_ohlcv_and_trading),
            ("add_external_vars", self.add_external_vars),
        ]

        for step_name, func in steps:
            try:
                func()
            except Exception as e:
                print(f"[ERROR] Step '{step_name}' failed: {e}")

        try:
            self.df = self.df.drop(
                columns=["wdate", "stock_list", "stock_name", "news_date", "ticker"]
                + [f"d_minus_{i}_date" for i in range(1, 6)]
                + ["d_day_date"],
                errors="ignore",
            )
        except Exception as e:
            print(f"[WARN] Drop columns failed: {e}")

        try:
            # ê¸°ì¤€ ì»¬ëŸ¼ë“¤
            prefixes = ["close", "volume", "institution", "foreign", "individual"]
            days = ["d_minus_5", "d_minus_4", "d_minus_3", "d_minus_2"]

            # ë“±ë½ë¥  ê³„ì‚°
            for prefix in prefixes:
                d_minus_1_col = f"d_minus_1_date_{prefix}"

                # d-1 ì»¬ëŸ¼ì— NaN ìˆìœ¼ë©´ ì „ì²´ ìŠ¤í‚µ
                if self.df[d_minus_1_col].isna().any():
                    continue

                for day in days:
                    col = f"{day}_date_{prefix}"

                    # ë¹„êµ ëŒ€ìƒ ì»¬ëŸ¼ì— NaN ìˆìœ¼ë©´ í•´ë‹¹ day ë£¨í”„ ìŠ¤í‚µ
                    if self.df[col].isna().any():
                        continue

                    self.df[col] = np.round(
                        (self.df[d_minus_1_col] - self.df[col])
                        / self.df[d_minus_1_col],
                        2,
                    )

            self.df["d_minus_1_date_close"] = 0
            self.df["d_minus_1_date_volume"] = 0
            self.df["d_minus_1_date_institution"] = 0
            self.df["d_minus_1_date_foreign"] = 0
            self.df["d_minus_1_date_individual"] = 0

            return self.df.to_dict(orient="records")
        except Exception as e:
            print(f"[ERROR] Converting to dict failed: {e}")
            return []


def request_impact_score(news_id):
    try:
        url = f"http://15.165.211.100:8000/news/{news_id}/impact_score"

        r = requests.get(url, timeout=5)
        r.raise_for_status()

        d_plus = r.json().get("d_plus", [])
        impact_score = r.json().get("impact_score", 0)

        return {"news_id": news_id, "score": impact_score, "d_plus": d_plus}

    except Exception as e:
        print(f"âŒ {news_id} ì‹¤íŒ¨: {e}")

        return {"news_id": news_id, "score": 0, "d_plus": []}


def get_impact_score(market_datas):
    return [request_impact_score(md["news_id"]) for md in market_datas]


if __name__ == "__main__":
    log.info("ë¡œê·¸ í…ŒìŠ¤íŠ¸: news_pipeline.py ì§ì ‘ ì‹¤í–‰ë¨")
    fetch_latest_news()
