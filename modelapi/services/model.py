import numpy as np
import re
from konlpy.tag import Okt
import numpy as np

from schemas.model import SimilarNewsItem
import asyncio
from fastapi.responses import StreamingResponse
import json
import redis
from concurrent.futures import ThreadPoolExecutor

import ast


def get_news_summary(
    text,
    request,
    max_length=128,
    no_repeat_ngram_size=3,
    repetition_penalty=1.2,
) -> str:
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ì„ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ì‹¤ì œ ìš”ì•½ ë¡œì§ì€ ì™¸ë¶€ ëª¨ë¸ì´ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    encoder_sess = request.app.state.encoder_sess_summarize
    decoder_sess = request.app.state.decoder_sess_summarize
    tokenizer = request.app.state.tokenizer_summarize

    text = text.strip()[:300]

    input_ids = tokenizer.encode(text).ids
    input_ids_np = np.array([input_ids], dtype=np.int64)
    attention_mask = np.ones_like(input_ids_np, dtype=np.int64)

    encoder_outputs = encoder_sess.run(
        None, {"input_ids": input_ids_np, "attention_mask": attention_mask}
    )[0]

    decoder_input_ids = [tokenizer.token_to_id("<s>")]
    generated_ids = decoder_input_ids.copy()

    for _ in range(max_length):
        decoder_input_np = np.array([generated_ids], dtype=np.int64)
        decoder_inputs = {
            "input_ids": decoder_input_np,
            "encoder_hidden_states": encoder_outputs,
            "encoder_attention_mask": attention_mask,
        }
        logits = decoder_sess.run(None, decoder_inputs)[0]
        next_token_logits = logits[:, -1, :]

        # repetition penalty ì ìš©
        for token_id in set(generated_ids):
            next_token_logits[0, token_id] /= repetition_penalty

        # no_repeat_ngram_size ì ìš©
        if no_repeat_ngram_size > 0 and len(generated_ids) >= no_repeat_ngram_size:
            ngram = tuple(generated_ids[-(no_repeat_ngram_size - 1) :])
            banned = {
                tuple(generated_ids[i : i + no_repeat_ngram_size])
                for i in range(len(generated_ids) - no_repeat_ngram_size + 1)
            }
            for token_id in range(next_token_logits.shape[-1]):
                if ngram + (token_id,) in banned:
                    next_token_logits[0, token_id] = -1e9  # í° ë§ˆì´ë„ˆìŠ¤

        # greedy ì„ íƒ
        next_token_id = int(np.argmax(next_token_logits, axis=-1)[0])

        if next_token_id == tokenizer.token_to_id("</s>"):
            break

        generated_ids.append(next_token_id)

    summary = tokenizer.decode(generated_ids, skip_special_tokens=True)

    return summary


def get_ner_tokens(text, request, id2label):
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ì—ì„œ ê°œì²´ëª… ì¸ì‹ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    tokenizer = request.app.state.tokenizer_ner
    session = request.app.state.session_ner

    # ğŸŸ¡ í† í°í™” ë° ì…ë ¥ê°’ ì¤€ë¹„
    encoding = tokenizer.encode(text)
    input_ids = np.array([encoding.ids], dtype=np.int64)
    attention_mask = np.ones_like(input_ids, dtype=np.int64)
    token_type_ids = np.zeros_like(input_ids, dtype=np.int64)

    # ğŸ”µ ONNX ì¶”ë¡  ì‹¤í–‰
    inputs = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "token_type_ids": token_type_ids,
    }

    logits = session.run(None, inputs)[0]  # shape: (1, seq_len, num_labels)

    # ğŸ”µ ë¼ë²¨ ì¸ë±ìŠ¤ â†’ ì‹¤ì œ ë¼ë²¨ëª…
    preds = np.argmax(logits, axis=-1)[0]
    labels = [id2label[p] for p in preds[: len(encoding.tokens)]]

    # ğŸ”µ ì‹œê°í™”
    tokens = encoding.tokens

    return tokens, labels


def extract_ogg_economy(tokens, labels, target_label="OGG_ECONOMY"):
    merged_words = []
    current_word = ""

    for token, label in zip(tokens, labels):
        token_clean = token.replace("##", "") if token.startswith("##") else token

        if label == f"B-{target_label}":
            if current_word:
                merged_words.append(current_word)
            current_word = token_clean

        elif label == f"I-{target_label}":
            current_word += token_clean

        else:
            if current_word:
                merged_words.append(current_word)
                current_word = ""

    if current_word:
        merged_words.append(current_word)

    stock_list = merged_words.copy()

    return stock_list


def get_news_embedding(text, request):
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ì„ ì„ë² ë”©í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    tokenizer = request.app.state.tokenizer_embedding
    session = request.app.state.session_embedding

    # ì…ë ¥ í…ìŠ¤íŠ¸
    tokens = tokenizer.encode(text)
    input_ids = np.array([tokens.ids], dtype=np.int64)
    attention_mask = np.array([[1] * len(tokens.ids)], dtype=np.int64)

    # ì¶”ë¡ 
    embedding = session.run(
        ["sentence_embedding"],
        {"input_ids": input_ids, "attention_mask": attention_mask},
    )[0]

    # (1, 768) â†’ List[List[float]]
    return [[float(x) for x in embedding[0]]]


def safe_parse_list(val):
    if isinstance(val, str):
        try:
            return ast.literal_eval(val)
        except Exception:
            return []
    return val if isinstance(val, list) else []


def get_news_similar_list(payload, request):
    """
    ìœ ì‚¬ ë‰´ìŠ¤ top_k
    """
    article = payload.article
    top_k = payload.top_k

    vectordb = request.app.state.vectordb

    # ê²€ìƒ‰
    results = vectordb.similarity_search_with_score(article, k=top_k)

    news_similar_list = []
    for doc, score in results:
        news_id = doc.metadata.get("news_id")
        wdate = doc.metadata.get("wdate")
        title = doc.metadata.get("title")
        summary = doc.page_content
        url = doc.metadata.get("url")
        image = doc.metadata.get("image")
        stock_list = safe_parse_list(doc.metadata.get("stock_list"))
        industry_list = safe_parse_list(doc.metadata.get("industry_list"))

        if not news_id or not wdate or not summary:
            continue

        news_similar_list.append(
            SimilarNewsItem(
                news_id=news_id,
                wdate=wdate,
                title=title,
                summary=summary,
                url=url,
                image=image,
                stock_list=stock_list,
                industry_list=industry_list,
                impact_score=round(1 - float(score), 2),
            )
        )

    return news_similar_list


def get_lda_topic(text, request):
    lda_model = request.app.state.lda_model
    count_vectorizer = request.app.state.count_vectorizer
    stopwords = request.app.state.stopwords

    # 2. í˜•íƒœì†Œ ë¶„ì„ê¸° ë° ë¶ˆìš©ì–´ ë¡œë“œ
    okt = Okt()

    # 3. í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜
    def clean_text(text):
        text = re.sub(r"\[.*?\]|\(.*?\)", "", text)
        text = re.sub(r"[^ê°€-í£\s]", "", text)
        text = re.sub(r"\s+", " ", text).strip()
        return text

    # 4. ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜
    def extract_nouns(text):
        nouns = okt.nouns(text)
        nouns = [word for word in nouns if word not in stopwords and len(word) > 1]
        return " ".join(nouns)

    # 5. ë°ì´í„° ì „ì²˜ë¦¬ (ì •ì œ + ëª…ì‚¬ ì¶”ì¶œ)
    processed_texts = [extract_nouns(clean_text(text))]

    # 6. ë²¡í„°ë¼ì´ì¦ˆ (DTM ìƒì„±)
    new_dtm = count_vectorizer.transform(processed_texts)

    # 7. LDA í† í”½ ë¶„í¬ ì˜ˆì¸¡
    topic_distribution = lda_model.transform(new_dtm)

    lda_topics = {}
    for index, value in enumerate(topic_distribution[0]):
        lda_topics[f"topic_{index+1}"] = value

    return lda_topics


# âœ… ì „ì—­ Redis ì—°ê²° ì¬ì‚¬ìš©
redis_conn = redis.Redis(
    host="43.200.17.139",
    port=6379,
    password="q1w2e3r4!@#",
    decode_responses=True,
)

# âœ… ì „ì—­ ThreadPoolExecutor (ìŠ¤ë ˆë“œ ì¬ì‚¬ìš©)
redis_executor = ThreadPoolExecutor(max_workers=10)


# âœ… Redis ë¹„ë™ê¸° ì „ì†¡ í•¨ìˆ˜
def send_to_redis_async(data: dict):
    def task():
        try:
            redis_conn.publish("chat-response", json.dumps(data, ensure_ascii=False))
        except Exception as e:
            print(f"[Redis Error] {type(e).__name__}: {e}")

    redis_executor.submit(task)


# âœ… SSE ì‘ë‹µ
async def get_stream_response(request, payload):
    chatbot = request.app.state.chatbot
    loop = asyncio.get_event_loop()

    # í”„ë¡¬í”„íŠ¸ ìƒì„± (ë™ê¸° â†’ ë¹„ë™ê¸°)
    prompt = await loop.run_in_executor(
        None, chatbot.make_stream_prompt, payload.question, 5
    )

    client = chatbot.get_client()
    stream = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4,
        max_tokens=1024,
        stream=True,
    )

    queue = asyncio.Queue()

    def lookahead_iter(iterator):
        """ë§ˆì§€ë§‰ ì—¬ë¶€ë¥¼ ì•Œë ¤ì£¼ëŠ” ì œë„ˆë ˆì´í„° (yield (is_last, item))"""
        it = iter(iterator)
        try:
            prev = next(it)
        except StopIteration:
            return

        for val in it:
            yield False, prev
            prev = val
        yield True, prev  # ë§ˆì§€ë§‰

    last_sent = False  # ë§ˆì§€ë§‰ trueê°€ ë‚˜ê°”ëŠ”ì§€ ì¶”ì 

    def produce_chunks():
        nonlocal last_sent
        idx = 0  # ì¸ë±ìŠ¤ ì¶”ê°€
        try:
            for is_last, chunk in lookahead_iter(stream):
                delta = chunk.choices[0].delta
                content = getattr(delta, "content", None)

                if content:
                    data = {
                        "client_id": payload.client_id,
                        "content": content,
                        "is_last": is_last,
                        "index": idx,  # ì¸ë±ìŠ¤ ë¶€ì—¬
                    }
                    idx += 1  # ì¸ë±ìŠ¤ ì¦ê°€
                    if is_last:
                        last_sent = True

                    send_to_redis_async(data)
                    msg = f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                    asyncio.run_coroutine_threadsafe(queue.put(msg), loop)
        finally:
            # í˜¹ì‹œ ë§ˆì§€ë§‰ is_lastê°€ ì•ˆ ë‚˜ê°”ìœ¼ë©´ ê°•ì œë¡œ ì „ì†¡
            if not last_sent:
                data = {
                    "client_id": payload.client_id,
                    "content": "",  # ë˜ëŠ” None
                    "is_last": True,
                    "index": idx,  # ë§ˆì§€ë§‰ ì¸ë±ìŠ¤
                }
                send_to_redis_async(data)
                msg = f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                asyncio.run_coroutine_threadsafe(queue.put(msg), loop)

            asyncio.run_coroutine_threadsafe(queue.put(None), loop)

    # ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰
    loop.run_in_executor(None, produce_chunks)

    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¼
    async def event_stream():
        while True:
            item = await queue.get()
            if item is None:
                break
            yield item

    return StreamingResponse(event_stream(), media_type="text/event-stream")


def get_news_recommended(payload, request):
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ì„ ì„ë² ë”©í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    news_clicked_list = payload.news_clicked_list
    news_candidate_list = payload.news_candidate_list

    model_recommend = request.app.state.model_recommend

    # ì¶”ë¡ 
    outputs = model_recommend.run(
        None,
        {"clicked": news_clicked_list, "candidates": news_candidate_list},
    )

    scores = outputs[0]  # [1, 5]
    top_1 = scores[0].argsort()[::-1][:1][0]  # Top-1 ì¶”ì²œ ì¸ë±ìŠ¤

    # ì˜ˆì¸¡ ê²°ê³¼ (Top-3 í›„ë³´ ì¸ë±ìŠ¤):
    # [14  0  2 16 19 13 15  1 10  7  9  4 17  5  8  3  6 18 11 12]
    return top_1
