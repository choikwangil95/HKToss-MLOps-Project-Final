import numpy as np
import re
from konlpy.tag import Okt
import numpy as np
from sqlalchemy.orm import Session
from collections import Counter

from schemas.model import SimilarNewsItem
import asyncio
from fastapi.responses import StreamingResponse
import json
import redis
from concurrent.futures import ThreadPoolExecutor
import requests
from db.postgresql import get_db
from models.custom import (
    NewsModel_v2,
    NewsModel_v2_Metadata,
    NewsModel_v2_External,
    NewsModel_v2_Topic,
)
import shap

import ast
import pandas as pd


def get_news_summary(
    text,
    request,
    max_length=128,
    no_repeat_ngram_size=3,
    repetition_penalty=1.2,
) -> str:
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ì„ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ì‹¤ì œ ìš”ì•½ ë¡œì§ì€ ì™¸ë¶€ ëª¨ë¸ì´ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    encoder_sess = request.app.state.encoder_sess_summarize
    decoder_sess = request.app.state.decoder_sess_summarize
    tokenizer = request.app.state.tokenizer_summarize

    text = text.strip()[:300]

    input_ids = tokenizer.encode(text).ids
    input_ids_np = np.array([input_ids], dtype=np.int64)
    attention_mask = np.ones_like(input_ids_np, dtype=np.int64)

    encoder_outputs = encoder_sess.run(
        None, {"input_ids": input_ids_np, "attention_mask": attention_mask}
    )[0]

    decoder_input_ids = [tokenizer.token_to_id("<s>")]
    generated_ids = decoder_input_ids.copy()

    for _ in range(max_length):
        decoder_input_np = np.array([generated_ids], dtype=np.int64)
        decoder_inputs = {
            "input_ids": decoder_input_np,
            "encoder_hidden_states": encoder_outputs,
            "encoder_attention_mask": attention_mask,
        }
        logits = decoder_sess.run(None, decoder_inputs)[0]
        next_token_logits = logits[:, -1, :]

        # repetition penalty ì ìš©
        for token_id in set(generated_ids):
            next_token_logits[0, token_id] /= repetition_penalty

        # no_repeat_ngram_size ì ìš©
        if no_repeat_ngram_size > 0 and len(generated_ids) >= no_repeat_ngram_size:
            ngram = tuple(generated_ids[-(no_repeat_ngram_size - 1) :])
            banned = {
                tuple(generated_ids[i : i + no_repeat_ngram_size])
                for i in range(len(generated_ids) - no_repeat_ngram_size + 1)
            }
            for token_id in range(next_token_logits.shape[-1]):
                if ngram + (token_id,) in banned:
                    next_token_logits[0, token_id] = -1e9  # í° ë§ˆì´ë„ˆìŠ¤

        # greedy ì„ íƒ
        next_token_id = int(np.argmax(next_token_logits, axis=-1)[0])

        if next_token_id == tokenizer.token_to_id("</s>"):
            break

        generated_ids.append(next_token_id)

    summary = tokenizer.decode(generated_ids, skip_special_tokens=True)

    return summary


def get_ner_tokens(text, request, id2label):
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ì—ì„œ ê°œì²´ëª… ì¸ì‹ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    tokenizer = request.app.state.tokenizer_ner
    session = request.app.state.session_ner

    # ğŸŸ¡ í† í°í™” ë° ì…ë ¥ê°’ ì¤€ë¹„
    encoding = tokenizer.encode(text)
    input_ids = np.array([encoding.ids], dtype=np.int64)
    attention_mask = np.ones_like(input_ids, dtype=np.int64)
    token_type_ids = np.zeros_like(input_ids, dtype=np.int64)

    # ğŸ”µ ONNX ì¶”ë¡  ì‹¤í–‰
    inputs = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "token_type_ids": token_type_ids,
    }

    logits = session.run(None, inputs)[0]  # shape: (1, seq_len, num_labels)

    # ğŸ”µ ë¼ë²¨ ì¸ë±ìŠ¤ â†’ ì‹¤ì œ ë¼ë²¨ëª…
    preds = np.argmax(logits, axis=-1)[0]
    labels = [id2label[p] for p in preds[: len(encoding.tokens)]]

    # ğŸ”µ ì‹œê°í™”
    tokens = encoding.tokens

    return tokens, labels


def extract_ogg_economy(tokens, labels, target_label="OGG_ECONOMY"):
    merged_words = []
    current_word = ""

    for token, label in zip(tokens, labels):
        token_clean = token.replace("##", "") if token.startswith("##") else token

        if label == f"B-{target_label}":
            if current_word:
                merged_words.append(current_word)
            current_word = token_clean

        elif label == f"I-{target_label}":
            current_word += token_clean

        else:
            if current_word:
                merged_words.append(current_word)
                current_word = ""

    if current_word:
        merged_words.append(current_word)

    stock_list = merged_words.copy()

    return stock_list


async def get_news_embeddings(article_list, request):
    """
    ë‰´ìŠ¤ ë³¸ë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ONNX ëª¨ë¸ì´ ë°°ì¹˜ ì…ë ¥ì„ ì§€ì›í•  ê²½ìš°, í•œ ë²ˆì— ì¶”ë¡ í•©ë‹ˆë‹¤.
    """
    tokenizer = request.app.state.tokenizer_embedding
    session = request.app.state.session_embedding

    # 1. í† í°í™”
    encoded = [tokenizer.encode(x) for x in article_list]
    input_ids = [e.ids for e in encoded]
    attention_mask = [[1] * len(ids) for ids in input_ids]

    # 2. íŒ¨ë”© (ìµœëŒ€ ê¸¸ì´ ê¸°ì¤€)
    max_len = max(len(ids) for ids in input_ids)
    input_ids_padded = [ids + [0] * (max_len - len(ids)) for ids in input_ids]
    attention_mask_padded = [
        mask + [0] * (max_len - len(mask)) for mask in attention_mask
    ]

    # 3. numpy ë°°ì—´ë¡œ ë³€í™˜
    input_ids_np = np.array(input_ids_padded, dtype=np.int64)
    attention_mask_np = np.array(attention_mask_padded, dtype=np.int64)

    # 4. ONNX ì¶”ë¡ 
    outputs = session.run(
        ["sentence_embedding"],
        {"input_ids": input_ids_np, "attention_mask": attention_mask_np},
    )[
        0
    ]  # shape: (batch_size, hidden_dim)

    # 5. ë°˜í™˜ (List[List[float]])
    return outputs.tolist()


def safe_parse_list(val):
    if isinstance(val, str):
        try:
            return ast.literal_eval(val)
        except Exception:
            return []
    return val if isinstance(val, list) else []


def get_news_similar_list(payload, request):
    """
    ìœ ì‚¬ ë‰´ìŠ¤ top_k
    """
    article = payload.article
    top_k = payload.top_k

    vectordb = request.app.state.vectordb

    # ê²€ìƒ‰
    results = vectordb.similarity_search_with_score(article, k=100)

    news_similar_list = []
    seen_titles = set()

    for doc, score in results:
        similarity = round(1 - float(score), 2)

        if similarity > 0.9:
            continue  # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ ì œì™¸ (0.9 ì´ìƒ í•„í„°ë§)

        title = doc.metadata.get("title")
        if title in seen_titles:
            continue  # ì´ë¯¸ ì¶”ê°€í•œ titleì´ë©´ ìŠ¤í‚µ
        seen_titles.add(title)

        news_id = doc.metadata.get("news_id")
        wdate = doc.metadata.get("wdate")
        summary = doc.page_content
        url = doc.metadata.get("url")
        image = doc.metadata.get("image")
        stock_list = safe_parse_list(doc.metadata.get("stock_list"))
        industry_list = safe_parse_list(doc.metadata.get("industry_list"))

        if not news_id or not wdate or not summary:
            continue

        news_similar_list.append(
            SimilarNewsItem(
                news_id=news_id,
                wdate=wdate,
                title=title,
                summary=summary,
                url=url,
                image=image,
                stock_list=stock_list,
                industry_list=industry_list,
                similarity=similarity,
            )
        )

    return news_similar_list[:top_k]


def get_lda_topic(text, request):
    lda_model = request.app.state.lda_model
    count_vectorizer = request.app.state.count_vectorizer
    stopwords = request.app.state.stopwords

    # 2. í˜•íƒœì†Œ ë¶„ì„ê¸° ë° ë¶ˆìš©ì–´ ë¡œë“œ
    okt = Okt()

    # 3. í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜
    def clean_text(text):
        text = re.sub(r"\[.*?\]|\(.*?\)", "", text)
        text = re.sub(r"[^ê°€-í£\s]", "", text)
        text = re.sub(r"\s+", " ", text).strip()
        return text

    # 4. ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜
    def extract_nouns(text):
        nouns = okt.nouns(text)
        nouns = [word for word in nouns if word not in stopwords and len(word) > 1]
        return " ".join(nouns)

    # 5. ë°ì´í„° ì „ì²˜ë¦¬ (ì •ì œ + ëª…ì‚¬ ì¶”ì¶œ)
    processed_texts = [extract_nouns(clean_text(text))]

    # 6. ë²¡í„°ë¼ì´ì¦ˆ (DTM ìƒì„±)
    new_dtm = count_vectorizer.transform(processed_texts)

    # 7. LDA í† í”½ ë¶„í¬ ì˜ˆì¸¡
    topic_distribution = lda_model.transform(new_dtm)

    lda_topics = {}
    for index, value in enumerate(topic_distribution[0]):
        lda_topics[f"topic_{index+1}"] = value

    return lda_topics


# âœ… ì „ì—­ Redis ì—°ê²° ì¬ì‚¬ìš©
redis_conn = redis.Redis(
    host="43.200.17.139",
    port=6379,
    password="q1w2e3r4!@#",
    decode_responses=True,
)

# âœ… ì „ì—­ ThreadPoolExecutor (ìŠ¤ë ˆë“œ ì¬ì‚¬ìš©)
redis_executor = ThreadPoolExecutor(max_workers=10)


# âœ… Redis ë¹„ë™ê¸° ì „ì†¡ í•¨ìˆ˜
def send_to_redis_async(data: dict):
    def task():
        try:
            redis_conn.publish("chat-response", json.dumps(data, ensure_ascii=False))
        except Exception as e:
            print(f"[Redis Error] {type(e).__name__}: {e}")

    redis_executor.submit(task)


# âœ… SSE ì‘ë‹µ
async def get_stream_response(request, payload):
    chatbot = request.app.state.chatbot
    loop = asyncio.get_event_loop()

    # í”„ë¡¬í”„íŠ¸ ìƒì„± (ë™ê¸° â†’ ë¹„ë™ê¸°)
    messages = await loop.run_in_executor(
        None, chatbot.make_stream_prompt, payload.question, 2
    )

    client = chatbot.get_client()
    stream = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=messages,
        temperature=0.4,
        max_tokens=1024,
        stream=True,
    )

    queue = asyncio.Queue()

    def lookahead_iter(iterator):
        """ë§ˆì§€ë§‰ ì—¬ë¶€ë¥¼ ì•Œë ¤ì£¼ëŠ” ì œë„ˆë ˆì´í„° (yield (is_last, item))"""
        it = iter(iterator)
        try:
            prev = next(it)
        except StopIteration:
            return

        for val in it:
            yield False, prev
            prev = val
        yield True, prev  # ë§ˆì§€ë§‰

    last_sent = False  # ë§ˆì§€ë§‰ trueê°€ ë‚˜ê°”ëŠ”ì§€ ì¶”ì 

    def produce_chunks():
        nonlocal last_sent
        idx = 0  # ì¸ë±ìŠ¤ ì¶”ê°€
        try:
            for is_last, chunk in lookahead_iter(stream):
                delta = chunk.choices[0].delta
                content = getattr(delta, "content", None)

                if content:
                    data = {
                        "client_id": payload.client_id,
                        "content": content,
                        "is_last": is_last,
                        "index": idx,  # ì¸ë±ìŠ¤ ë¶€ì—¬
                    }
                    idx += 1  # ì¸ë±ìŠ¤ ì¦ê°€
                    if is_last:
                        last_sent = True

                    send_to_redis_async(data)
                    msg = f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                    asyncio.run_coroutine_threadsafe(queue.put(msg), loop)
        finally:
            # í˜¹ì‹œ ë§ˆì§€ë§‰ is_lastê°€ ì•ˆ ë‚˜ê°”ìœ¼ë©´ ê°•ì œë¡œ ì „ì†¡
            if not last_sent:
                data = {
                    "client_id": payload.client_id,
                    "content": "",  # ë˜ëŠ” None
                    "is_last": True,
                    "index": idx,  # ë§ˆì§€ë§‰ ì¸ë±ìŠ¤
                }
                send_to_redis_async(data)
                msg = f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                asyncio.run_coroutine_threadsafe(queue.put(msg), loop)

            asyncio.run_coroutine_threadsafe(queue.put(None), loop)

    # ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰
    loop.run_in_executor(None, produce_chunks)

    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¼
    async def event_stream():
        while True:
            item = await queue.get()
            if item is None:
                break
            yield item

    return StreamingResponse(event_stream(), media_type="text/event-stream")


# AE ì¸ì½”ë”© í•¨ìˆ˜
def run_ae(ae_sess, embedding):
    input_name = ae_sess.get_inputs()[0].name
    output_name = ae_sess.get_outputs()[0].name
    return ae_sess.run([output_name], {input_name: embedding.astype(np.float32)})[0]


# ê·¸ë£¹ ë‹¨ìœ„ ìŠ¤ì¼€ì¼ë§ í•¨ìˆ˜ (ê·¸ë£¹ë³„ scaler ì ìš©)
def scale_ext_grouped(
    ext: list, col_names: list, prefix: str, scalers: dict, group_key_map: dict
):
    grouped_data = {}
    grouped_indices = {}
    for idx, (col, val) in enumerate(zip(col_names, ext)):
        group = group_key_map.get(col, None)
        if group:
            key = f"{prefix}_{group}"
            grouped_data.setdefault(key, []).append(val)
            grouped_indices.setdefault(key, []).append(idx)

    scaled = ext.copy()
    for key in grouped_data:
        if key in scalers:
            try:
                values = np.array(grouped_data[key], dtype=np.float32).reshape(1, -1)
                # transformed = scalers[key].transform(values)[0]

                columns = scalers[key].feature_names_in_  # sklearn >=1.0
                values_df = pd.DataFrame(values, columns=columns)
                transformed = scalers[key].transform(values_df)[0]

                for idx, val in zip(grouped_indices[key], transformed):
                    scaled[idx] = val
            except Exception as e:
                print(f"âŒ {key} ìŠ¤ì¼€ì¼ ì‹¤íŒ¨: {e}")
                raise
        else:
            print(f"âš ï¸ {key} ìŠ¤ì¼€ì¼ëŸ¬ ì—†ìŒ â†’ ì›ë³¸ ì‚¬ìš©")

    return np.array(scaled, dtype=np.float32)


# íšŒê·€ ê¸°ë°˜ ìœ ì‚¬ ë‰´ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜
async def compute_similarity(
    db: Session,
    summary: str,
    extA: list,
    topicA: list,
    similar_summaries: list,
    extBs: list,
    topicBs: list,
    scalers,
    ae_sess,
    regressor_sess,
    embedding_api_func,
    ext_col_names: list,
    topic_col_names: list,
    news_topk_ids: list,
):

    # group_key_map ìƒì„± (ê¸°ì¤€ + ìœ ì‚¬ ë‰´ìŠ¤)
    group_key_map = {}
    for col in ext_col_names + topic_col_names:
        if "date_close" in col:
            group_key_map[col] = "price_close"
        elif "date_volume" in col:
            group_key_map[col] = "volume"
        elif "date_foreign" in col:
            group_key_map[col] = "foreign"
        elif "date_institution" in col:
            group_key_map[col] = "institution"
        elif "date_individual" in col:
            group_key_map[col] = "individual"
        elif col in ["fx", "bond10y", "base_rate"]:
            group_key_map[col] = "macro"
        elif "í† í”½" in col:
            group_key_map[col] = "topic"

    for col in ext_col_names + topic_col_names:
        col_sim = f"similar_{col}"
        if "date_close" in col:
            group_key_map[col_sim] = "price_close"
        elif "date_volume" in col:
            group_key_map[col_sim] = "volume"
        elif "date_foreign" in col:
            group_key_map[col_sim] = "foreign"
        elif "date_institution" in col:
            group_key_map[col_sim] = "institution"
        elif "date_individual" in col:
            group_key_map[col_sim] = "individual"
        elif col in ["fx", "bond10y", "base_rate"]:
            group_key_map[col_sim] = "macro"
        elif "í† í”½" in col:
            group_key_map[col_sim] = "topic"

    # í…ìŠ¤íŠ¸ ì„ë² ë”© + AE ì¸ì½”ë”©
    all_texts = [summary] + similar_summaries
    embeddings = np.array(await embedding_api_func(all_texts))

    embA, embBs = embeddings[0], embeddings[1:]
    latentA = run_ae(ae_sess, embA.reshape(1, -1))[0]
    latentBs = [run_ae(ae_sess, e.reshape(1, -1))[0] for e in embBs]

    # ìŠ¤ì¼€ì¼ë§
    extA_total = extA + topicA
    extA_col_names = ext_col_names + topic_col_names
    extA_scaled = scale_ext_grouped(
        extA_total, extA_col_names, "extA", scalers, group_key_map
    )

    extB_total = [ext + topic for ext, topic in zip(extBs, topicBs)]
    extB_col_names = [f"similar_{col}" for col in ext_col_names + topic_col_names]
    extBs_scaled = [
        scale_ext_grouped(extB, extB_col_names, "extB_similar", scalers, group_key_map)
        for extB in extB_total
    ]

    # íšŒê·€ ì˜ˆì¸¡
    inputA_name = regressor_sess.get_inputs()[0].name
    inputB_name = regressor_sess.get_inputs()[1].name
    output_name = regressor_sess.get_outputs()[0].name

    scores = []
    for i, (latentB, extB_scaled) in enumerate(zip(latentBs, extBs_scaled)):
        if extB_scaled.shape[0] != 42:
            raise ValueError(
                f"extB_scaled ê¸¸ì´ ì´ìƒí•¨! ê¸°ëŒ€: 42, ì‹¤ì œ: {extB_scaled.shape[0]} | index: {i}"
            )

        featA = np.concatenate([latentA, extA_scaled]).reshape(1, -1).astype(np.float32)
        featB = np.concatenate([latentB, extB_scaled]).reshape(1, -1).astype(np.float32)

        score = regressor_sess.run(
            [output_name], {inputA_name: featA, inputB_name: featB}
        )[0][0][0]
        scores.append(score)

    # ê²°ê³¼ ë°˜í™˜
    results = list(zip(similar_summaries, scores, news_topk_ids))
    results.sort(key=lambda x: -x[1])  # score ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬

    return [
        {
            "news_id": nid,
            "summary": summ,
            "wdate": "",
            "score": float(score),
            "rank": i + 1,
        }
        for i, (summ, score, nid) in enumerate(results)
    ]


async def get_embedding_batch(article_list, request):
    """ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”© ì²˜ë¦¬"""
    batch_size = 20

    all_embeddings = []
    for i in range(0, len(article_list), batch_size):
        batch = article_list[i : i + batch_size]
        try:
            batch_news_embeddings = await get_news_embeddings(batch, request)
            batch_embeddings = batch_news_embeddings if batch_news_embeddings else []
            all_embeddings.extend(batch_embeddings)
            print(f"ë°°ì¹˜ {i}-{i+len(batch)}: {len(batch_embeddings)}ê°œ ì„ë² ë”© ì„±ê³µ")

        except Exception as e:
            print(f"ë°°ì¹˜ {i}-{i+len(batch)} ì‹¤íŒ¨: {str(e)}")
            all_embeddings.extend([None] * len(batch))

    return all_embeddings


def get_news_metadata_df(news_ids: list) -> pd.DataFrame:
    """
    ë‰´ìŠ¤ ID ëª©ë¡ì„ ë°›ì•„ ê° ë‰´ìŠ¤ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜
    :param news_ids: ë‰´ìŠ¤ ID ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['20250513_0092', '20250618_28735495'])
    :return: ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° DataFrame
    """
    API_BASE_URL = "http://3.37.207.16:8000"
    METADATA_ENDPOINT = "/news/v2/{news_id}/metadata"

    records = []

    # news_idsê°€ ìœ íš¨í•œ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
    if not isinstance(news_ids, list) or not all(isinstance(i, str) for i in news_ids):
        print("ì˜ëª»ëœ news_ids í˜•ì‹: ë°˜ë“œì‹œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤")
        return pd.DataFrame()

    for news_id in news_ids:
        # URL ì •ìƒ ìƒì„± í™•ì¸
        url = API_BASE_URL + METADATA_ENDPOINT.format(news_id=news_id)
        try:
            response = requests.get(url)
            response.raise_for_status()  # 4xx/5xx ì˜¤ë¥˜ ì‹œ ì˜ˆì™¸ ë°œìƒ
            meta = response.json()
            records.append(meta)
            print(f"{news_id} ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì„±ê³µ")
        except requests.exceptions.HTTPError as e:
            print(f"{news_id} ì²˜ë¦¬ ì¤‘ HTTP ì˜¤ë¥˜ ({e.response.status_code}): {e}")
        except Exception as e:
            print(f"{news_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    return pd.DataFrame(records)


async def get_news_recommended(payload, request):
    """
    ë‰´ìŠ¤ í›„ë³´êµ° ì¶”ì²œ
    """
    news_clicked_ids = payload.news_clicked_ids
    news_candidate_ids = payload.news_candidate_ids

    # ì„ë² ë”©
    news_clicked_metadata = get_news_metadata_df(news_clicked_ids)
    news_candidate_medatata = get_news_metadata_df(news_candidate_ids)

    news_clicked_summaries = news_clicked_metadata["summary"].tolist()
    news_candidate_summaries = news_candidate_medatata["summary"].tolist()

    news_clicked_embeddings = await get_embedding_batch(news_clicked_summaries, request)
    news_candidate_embeddings = await get_embedding_batch(
        news_candidate_summaries, request
    )

    # ì„ë² ë”© ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬
    clicked_embeddings_np = np.array(news_clicked_embeddings, dtype=np.float32)
    candi_embeddings_np = np.array(news_candidate_embeddings, dtype=np.float32)

    # ì…ë ¥ ë°ì´í„° í˜•ì‹ ì¡°ì •
    num_clicked = len(news_clicked_embeddings)
    num_candidates = len(news_candidate_embeddings)

    clicked_input = clicked_embeddings_np[:num_clicked].reshape(1, num_clicked, 768)
    candidate_input = candi_embeddings_np[:num_candidates].reshape(
        1, num_candidates, 768
    )

    # ì¶”ë¡  Step 1
    top_k = 5  # í´ë¦­ ë‰´ìŠ¤ ë³„ í›„ë³´ ë‰´ìŠ¤ ìœ ì‚¬ í™•ë¥  top_k

    # ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
    inputs = {"clicked": clicked_input, "candidates": candidate_input}
    model_recommend = request.app.state.model_recommend
    outputs = model_recommend.run(None, inputs)[0]

    scores = outputs  # [1, 5]
    top_indices = np.argsort(-scores, axis=2)[:, :, :top_k]
    flattened = top_indices.squeeze(0).flatten()

    # ìœ ë‹ˆí¬ í›„ë³´êµ° ë° ë“±ì¥ ë¹ˆë„ ê³„ì‚°
    unique_candidates = np.unique(flattened)
    counts = Counter(flattened)

    # [0, 2, 6, 4, ...]
    candidate_indices = unique_candidates.tolist()

    # ì‹¤ì œ ë‰´ìŠ¤ ID ë§¤í•‘
    top_k_news_ids = [news_candidate_ids[i] for i in candidate_indices]

    # ìµœì¢… ë°˜í™˜
    # ['20250513_0089', '20250513_0148', '20250513_0085', ... ]
    return top_k_news_ids


async def get_news_recommended_ranked(payload, request, db):
    """
    ë‰´ìŠ¤ í›„ë³´êµ° ì¶”ì²œ
    """
    model_recommend_ranker = request.app.state.model_recommend_ranker
    user_id = payload.user_id

    # ê¸°ë³¸ê°’
    user_data = {"userPnl": 0, "asset": 0, "investScore": 0}

    # 1ì°¨ API ìš”ì²­ (43.200.17.139)
    try:
        url1 = f"http://43.200.17.139:8080/api/v1/userinfo/{user_id}"
        response = requests.get(url1, timeout=3)
        response.raise_for_status()
        user_data = response.json()["data"]

        print(f"âœ… ì‚¬ìš©ì {user_id} ì •ë³´ ì¡°íšŒ ì„±ê³µ (1ì°¨): {url1}")
    except Exception as e:
        print(f"âŒ 1ì°¨ ì‚¬ìš©ì API ì‹¤íŒ¨: {str(e)}")

        # 2ì°¨ API ìš”ì²­ (3.37.207.16)
        try:
            url2 = f"http://3.37.207.16:8000/users/{user_id}"
            response = requests.get(url2, timeout=3)
            response.raise_for_status()
            user_data = response.json()

            print(f"âœ… ì‚¬ìš©ì {user_id} ì •ë³´ ì¡°íšŒ ì„±ê³µ (2ì°¨): {url2}")
        except Exception as e:
            print(f"âŒ 2ì°¨ ì‚¬ìš©ì API ì‹¤íŒ¨: {str(e)}")
            print(f"âš ï¸ ê¸°ë³¸ ì‚¬ìš©ì ë°ì´í„°ë¡œ ëŒ€ì²´: {user_data}")

    # ë‰´ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    news_ids = payload.news_ids

    # 1. ê°ê° í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¡°íšŒ
    news_list = db.query(NewsModel_v2).filter(NewsModel_v2.news_id.in_(news_ids)).all()
    metadata_list = (
        db.query(NewsModel_v2_Metadata)
        .filter(NewsModel_v2_Metadata.news_id.in_(news_ids))
        .all()
    )
    external_list = (
        db.query(NewsModel_v2_External)
        .filter(NewsModel_v2_External.news_id.in_(news_ids))
        .all()
    )
    topic_list = (
        db.query(NewsModel_v2_Topic)
        .filter(NewsModel_v2_Topic.news_id.in_(news_ids))
        .all()
    )

    # 2. ê° í…Œì´ë¸” ê²°ê³¼ë¥¼ news_id ê¸°ì¤€ dictë¡œ ë§¤í•‘
    news_map = {row.news_id: row.__dict__ for row in news_list}
    metadata_map = {row.news_id: row.__dict__ for row in metadata_list}
    external_map = {row.news_id: row.__dict__ for row in external_list}
    topic_map = {row.news_id: row.__dict__ for row in topic_list}

    # 3. SQLAlchemy ë‚´ë¶€ í‚¤ ì œê±° (_sa_instance_state)
    def clean_sqlalchemy_dict(d):
        return {k: v for k, v in d.items() if k != "_sa_instance_state"}

    # 4. ë³‘í•©
    merged_results = []

    for news_id in news_ids:
        row = {}

        if news_id in news_map:
            row.update(clean_sqlalchemy_dict(news_map[news_id]))
        if news_id in metadata_map:
            row.update(clean_sqlalchemy_dict(metadata_map[news_id]))
        if news_id in external_map:
            row.update(clean_sqlalchemy_dict(external_map[news_id]))
        if news_id in topic_map:
            row.update(clean_sqlalchemy_dict(topic_map[news_id]))

        # [2] ì‚¬ìš©ì ë°ì´í„° ë³‘í•©
        row.update(user_data)  # user_dataëŠ” ê° rowì— ë™ì¼í•˜ê²Œ ì ìš©ëœë‹¤ê³  ê°€ì •

        # [3] main_topic ê³„ì‚°
        topic_columns = [f"topic_{i}" for i in range(1, 10)]
        topic_values = [row.get(col) for col in topic_columns]

        if any(pd.notna(val) for val in topic_values):
            try:
                max_topic_index = int(np.argmax(topic_values)) + 1  # 1ë¶€í„° ì‹œì‘
                row["main_topic"] = max_topic_index
            except Exception:
                row["main_topic"] = None
        else:
            row["main_topic"] = None

        # [4] is_same_stock ê³„ì‚°
        try:
            news_stocks = {
                stock["stock_id"] for stock in row.get("stock_list_view", [])
            }
            user_stocks = {stock["stockCode"] for stock in row.get("memberStocks", [])}
            row["is_same_stock"] = 1 if news_stocks & user_stocks else 0
        except Exception:
            row["is_same_stock"] = 0

        # [5] ë³‘í•© ê²°ê³¼ ì €ì¥
        merged_results.append(row)

    columns_to_keep = [
        "userPnl",
        "asset",
        "investScore",
        "topic_1",
        "topic_2",
        "topic_3",
        "topic_4",
        "topic_5",
        "topic_6",
        "topic_7",
        "topic_8",
        "topic_9",
        "main_topic",
        "is_same_stock",
    ]

    # ë³‘í•©ëœ ê²°ê³¼ì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê¸°
    filtered_results = [
        {k: row.get(k, None) for k in columns_to_keep} for row in merged_results
    ]

    df_input = pd.DataFrame(filtered_results)

    try:
        # í´ë¦­ í™•ë¥  ì˜ˆì¸¡ (í´ë˜ìŠ¤ 1ì— ëŒ€í•œ í™•ë¥ )
        proba = model_recommend_ranker.predict_proba(df_input)[:, 1]

        # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ë³‘í•©
        df_pred = pd.DataFrame(merged_results)
        df_pred["click_score"] = proba
        df_pred = df_pred[
            [
                "news_id",
                "wdate",
                "title",
                "summary",
                "image",
                "press",
                "url",
                "click_score",
            ]
        ]

        # 7. í™•ë¥  ê¸°ì¤€ ì •ë ¬
        df_pred = df_pred.sort_values("click_score", ascending=False).reset_index(
            drop=True
        )

        # 5. SHAP value ê³„ì‚°
        explainer = shap.TreeExplainer(model_recommend_ranker)
        shap_values = explainer.shap_values(df_input)  # shape: [n_samples, n_features]

        # 6. ìƒìœ„ 3ê°œ ì¤‘ìš” í”¼ì²˜ëª… ì¶”ì¶œ
        feature_kor_map = {
            "userPnl": "íˆ¬ì ìˆ˜ìµë¥  ê³ ë ¤",
            "asset": "ìì‚° ë³´ìœ ëŸ‰ ê³ ë ¤",
            "investScore": "íˆ¬ì ì„±í–¥ ê³ ë ¤",
            "topic_1": "ê¸€ë¡œë²Œ ì‹œì¥ ë™í–¥",
            "topic_2": "ì¦ê¶Œì‚¬ ì£¼ê°€ ì „ë§",
            "topic_3": "ê¸°ì—… ì‹¤ì  ê°œì„ ",
            "topic_4": "ì¦ê¶Œì‚¬ ì‚¬ì—… ë¶„ì„",
            "topic_5": "ëŒ€ê¸°ì—… ì‚¬ì—… ì „ëµ",
            "topic_6": "ê¸°ì—… ê¸°ìˆ  ê°œë°œ",
            "topic_7": "ë°˜ë„ì²´ ë° AI",
            "topic_8": "ê¸ˆìœµ ì„œë¹„ìŠ¤",
            "topic_9": "ì£¼ì£¼ ë° ê²½ì˜ ì´ìŠˆ",
            "is_same_stock": "ê´€ì‹¬ ìœ ì‚¬ ì¢…ëª©",
        }

        top_features_list = []
        feature_names = df_input.columns.tolist()

        for i, shap_row in enumerate(shap_values):
            abs_values = [abs(val) for val in shap_row]
            top_indices = sorted(range(len(abs_values)), key=lambda i: -abs_values[i])[
                :3
            ]
            top_feature_names = [feature_names[i] for i in top_indices]

            main_topic_value = df_input.iloc[i]["main_topic"]
            converted = []

            for f in top_feature_names:
                if f == "main_topic" and 1 <= int(main_topic_value) <= 9:
                    topic_key = f"topic_{int(main_topic_value)}"
                    converted.append(feature_kor_map.get(topic_key, topic_key))
                else:
                    converted.append(feature_kor_map.get(f, f))

            unique_converted = list(
                dict.fromkeys(converted)
            )  # âœ… ìˆœì„œ ìœ ì§€ + ì¤‘ë³µ ì œê±°
            top_features_list.append(unique_converted)

        # 7. SHAP ìƒìœ„ í”¼ì²˜ ì»¬ëŸ¼ ì¶”ê°€
        df_pred["recommend_reasons"] = top_features_list

        news_recommend_ranked_list = df_pred[:10].to_dict(orient="records")

        return news_recommend_ranked_list

    except Exception as e:
        print(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []
