from tokenizers import Tokenizer
from pathlib import Path
import onnxruntime as ort
from langchain_chroma import Chroma
from langchain.embeddings.base import Embeddings
import numpy as np
import joblib
import os
import requests
import openai
from dotenv import load_dotenv
import onnxruntime as ort

load_dotenv()


def get_summarize_model():
    """
    ONNX ìš”ì•½ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    base_path = Path("models/kobart_summary_onnx")

    encoder_sess = ort.InferenceSession(str(base_path / "encoder_model.onnx"))
    decoder_sess = ort.InferenceSession(str(base_path / "decoder_model.onnx"))
    tokenizer = Tokenizer.from_file(str(base_path / "tokenizer.json"))

    return encoder_sess, decoder_sess, tokenizer


def get_ner_tokenizer():
    """
    ONNX NER ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    base_path = Path("models/ner_onnx")

    tokenizer = Tokenizer.from_file(str(base_path / "tokenizer.json"))
    session = ort.InferenceSession(str(base_path / "model.onnx"))

    return tokenizer, session


def get_embedding_tokenizer():
    """
    ONNX NER ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    base_path = Path("models/kr_sbert_mean_onnx")

    tokenizer = Tokenizer.from_file(str(base_path / "tokenizer.json"))
    session = ort.InferenceSession(str(base_path / "kr_sbert.onnx"))

    return tokenizer, session


def get_vectordb():
    """
    vectordb ë¡œë”©
    """

    class OnnxEmbedder(Embeddings):
        def __init__(self, model_path: str, tokenizer_path: str):
            self.session = ort.InferenceSession(model_path)
            self.tokenizer = Tokenizer.from_file(tokenizer_path)

        def _embed(self, text: str):
            encoding = self.tokenizer.encode(text)
            input_ids = np.array([encoding.ids], dtype=np.int64)
            attention_mask = np.array([[1] * len(encoding.ids)], dtype=np.int64)

            outputs = self.session.run(
                None, {"input_ids": input_ids, "attention_mask": attention_mask}
            )

            raw_vector = outputs[0][0]
            norm_vector = raw_vector / (np.linalg.norm(raw_vector) + 1e-10)
            return norm_vector.tolist()

        def embed_documents(self, texts: list[str]) -> list[list[float]]:
            return [self._embed(text) for text in texts]

        def embed_query(self, text: str) -> list[float]:
            return self._embed(text)

    model_base_path = Path("models")

    embedding = OnnxEmbedder(
        model_path=str(model_base_path / "kr_sbert_mean_onnx/kr_sbert.onnx"),
        tokenizer_path=str(model_base_path / "kr_sbert_mean_onnx/tokenizer.json"),
    )

    db_base_path = Path("db")

    vectordb = Chroma(
        persist_directory=str(db_base_path / "chroma_store"),
        embedding_function=embedding,
    )

    return vectordb


def get_lda_model():
    """
    LDA ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    model_base_path = Path("models")

    count_vectorizer = joblib.load(str(model_base_path / "count_vectorizer.pkl"))
    lda_model = joblib.load(str(model_base_path / "best_lda_model.pkl"))

    db_base_path = Path("db")

    with open(str(db_base_path / "stopwords-ko.txt"), "r", encoding="utf-8") as f:
        stopwords = [word.strip() for word in f.readlines()]

    return lda_model, count_vectorizer, stopwords


class NewsTossChatbot:
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY í•„ìš”)
        self.client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def get_client(self):
        return self.client

    def search_similar_news(self, query_text, top_k=10):
        url = "http://15.165.211.100:8000/news/similar"
        payload = {"article": query_text, "top_k": top_k}

        response = requests.post(url, json=payload)
        response.raise_for_status()
        similar_news = response.json()["similar_news_list"]

        return similar_news

    def build_prompt(self, context, question, has_news=True):
        if has_news:
            return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
            ë‹¹ì‹ ì´ ëˆ„êµ¬ëƒ ë¬»ëŠ”ë‹¤ë©´, "ì €ëŠ” ê³¼ê±° ë‰´ìŠ¤ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦¬ëŠ” ì±—ë´‡ ë‰´ìŠ¤í† ìŠ¤ì…ë‹ˆë‹¤.ğŸ˜„" ë¼ê³  ë‹µí•˜ì„¸ìš”.
            ì–´ë– í•œ ì§ˆë¬¸ì´ë“  ë°˜ë“œì‹œ ì•„ë˜ ì˜ˆì‹œ í¬ë§·ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì¶”ê°€ ì˜ê²¬, ì¢…í•© ì •ë³´ ë“±ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

            [ë‹µë³€ ì‘ì„± ì‹œ ë°˜ë“œì‹œ ë‹¤ìŒì„ ì§€ì¼œì£¼ì„¸ìš”]
            1. ë‹µë³€ì—ëŠ” ë°˜ë“œì‹œ ê³¼ê±° ìœ ì‚¬ì‚¬ê±´ ë‰´ìŠ¤ ì •ë³´ë¥¼ ì•„ë˜ì™€ ê°™ì€ ì¹´ë“œ í˜•íƒœë¡œ ì •ë¦¬í•´ ë³´ì—¬ì£¼ì„¸ìš”:
                - ë‚ ì§œ, ì œëª©(í•˜ì´í¼ë§í¬), ì–¸ë¡ ì‚¬, ìš”ì•½, ê´€ë ¨ ì´ë¯¸ì§€(ì•„ë˜ ì˜ˆì‹œ ì°¸ê³ )
                - ì˜ˆì‹œ:
                â–  [2024-11-28] "SKí•˜ì´ë‹‰ìŠ¤, ì‹ ê·œ ì£¼ì£¼í™˜ì›ì±…ìœ¼ë¡œ ì¬ë¬´êµ¬ì¡° ê°œì„  ê¸°ëŒ€"
                (https://n.news.naver.com/mnews/article/008/0005120417)
                â–¶ ì–¸ë¡ ì‚¬: ë¨¸ë‹ˆíˆ¬ë°ì´
                â–¶ ìœ ì‚¬ë„: 0.56
                â–¶ ìš”ì•½: NHíˆ¬ìì¦ê¶Œì´ ì‹ ê·œ ì£¼ì£¼í™˜ì› ì •ì±…ì„ ê³µì‹œí•œ SKí•˜ì´ë‹‰ìŠ¤ì— ëŒ€í•´...
                â–¶ ê´€ë ¨ ì´ë¯¸ì§€: <img src="https://imgnews.pstatic.net/image/008/2024/11/28/0005120417_001_20241128085813446.jpg?type=w800" alt="ë‰´ìŠ¤ ì´ë¯¸ì§€">
            2. ìœ ì‚¬ ì‚¬ê±´ ë‰´ìŠ¤ ì •ë³´ ì™¸ ë‹¤ë¥¸ ì˜ê²¬, ì¢…í•© ì•ˆë‚´ ì •ë³´ ë“±ì€ ì ˆëŒ€ë¡œ ì œì‹œí•˜ì§€ ë§ˆì„¸ìš”. 

            [ì œê³µëœ ìœ ì‚¬ ë‰´ìŠ¤ ì¹´ë“œ]
            {context}

            [ì‚¬ìš©ì ì§ˆë¬¸]
            {question}
            <|eot_id|><|start_header_id|>assistant<|end_header_id|>"""
        else:
            return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
            ìœ ì‚¬ ë‰´ìŠ¤ê°€ ì—†ë‹¤ë©´, ì ˆëŒ€ë¡œ ì˜ê²¬ì´ë‚˜ ì¢…í•© ì•ˆë‚´ ì •ë³´ ë“±ì„ ì œì‹œí•˜ì§€ ë§ê³  ì•„ë˜ 3ê°€ì§€ ë‹µë³€ ì˜ˆì‹œ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
            - "í˜„ì¬ ì œê³µëœ ë‰´ìŠ¤ ì¹´ë“œ ì¤‘ì—ì„œëŠ” ì´ë²ˆ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ëœ ì‚¬ë¡€ëŠ” í™•ì¸ë˜ì§€ ì•Šì§€ë§Œ, ë‰´ìŠ¤í† ìŠ¤ëŠ” í•­ìƒ ìµœì‹  ì´ìŠˆì™€ ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì„ ì„ ë‹¤í•´ ì•ˆë‚´í•´ë“œë¦¬ê³  ìˆìŠµë‹ˆë‹¤. ê¶ê¸ˆí•˜ì‹  ì ì´ë‚˜ ë” êµ¬ì²´ì ì¸ ê´€ì‹¬ ë¶„ì•¼ê°€ ìˆë‹¤ë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!"
            - "ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ì‚¬ë¡€ë¥¼ ì°¾ê¸° ìœ„í•´ ë…¸ë ¥í–ˆì§€ë§Œ, ì´ë²ˆì—ëŠ” ì œê³µëœ ë‰´ìŠ¤ ì¹´ë“œ ë‚´ì—ì„œ ì§ì ‘ì ì¸ ì—°ê´€ ì‚¬ë¡€ë¥¼ í™•ì¸í•˜ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œë„ ë” ì •í™•í•˜ê³  í’ë¶€í•œ ì •ë³´ë¥¼ ë“œë¦´ ìˆ˜ ìˆë„ë¡ ê³„ì† ì—…ë°ì´íŠ¸í•˜ê³  ìˆìœ¼ë‹ˆ, ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"
            - "ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´, ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"

            [ì‚¬ìš©ì ì§ˆë¬¸]
            {question}

            <|eot_id|><|start_header_id|>assistant<|end_header_id|>"""


    def make_stream_prompt(self, question, top_k=10):
        similar_news = self.search_similar_news(question, top_k=top_k)
        # 0.1 ì´ìƒë§Œ í•„í„°ë§
        filtered_news = [row for row in similar_news if row.get('similarity', 0) >= 0.1]
        retrieved_infos = []
        for row in filtered_news:
            info = (
                f"{row['title']} ({row['url']})\n"
                f"<img src=\"{row['image']}\" alt=\"ë‰´ìŠ¤ ì´ë¯¸ì§€\">\n"
                f"{row['summary']}\n"
                f"{row['wdate'][:10]} {row.get('press', 'ì •ë³´ì—†ìŒ')}\n"
                f"(ìœ ì‚¬ë„: {row.get('similarity', 0):.2f})"
            )
            retrieved_infos.append(info)
        context = "\n\n".join(retrieved_infos)
        return self.build_prompt(context, question, has_news=bool(filtered_news))


    def answer(self, question, top_k=10):
        prompt = self.make_stream_prompt(question, top_k)

        response = self.client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4,
            max_tokens=1024,
        )

        return response.choices[0].message.content


def get_recommend_model():
    model_base_path = Path("models")

    # ONNX ì„¸ì…˜ ìƒì„±
    model_recommend = ort.InferenceSession(
        str(model_base_path / "two_tower_model.onnx")
    )

    return model_recommend
