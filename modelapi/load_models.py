from tokenizers import Tokenizer
from pathlib import Path
import onnxruntime as ort
from langchain_chroma import Chroma
from langchain.embeddings.base import Embeddings
import numpy as np
import joblib
import os
import requests
import openai
from dotenv import load_dotenv
import pickle

load_dotenv()


def get_summarize_model():
    """
    ONNX ìš”ì•½ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    base_path = Path("models/kobart_summary_onnx")

    encoder_sess = ort.InferenceSession(str(base_path / "encoder_model.onnx"))
    decoder_sess = ort.InferenceSession(str(base_path / "decoder_model.onnx"))
    tokenizer = Tokenizer.from_file(str(base_path / "tokenizer.json"))

    return encoder_sess, decoder_sess, tokenizer


def get_ner_tokenizer():
    """
    ONNX NER ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    base_path = Path("models/ner_onnx")

    tokenizer = Tokenizer.from_file(str(base_path / "tokenizer.json"))
    session = ort.InferenceSession(str(base_path / "model.onnx"))

    return tokenizer, session


def get_embedding_tokenizer():
    """
    ONNX NER ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    base_path = Path("models/kr_sbert_mean_onnx")

    tokenizer = Tokenizer.from_file(str(base_path / "tokenizer.json"))
    session = ort.InferenceSession(str(base_path / "kr_sbert.onnx"))

    return tokenizer, session


def get_vectordb():
    """
    vectordb ë¡œë”©
    """

    class OnnxEmbedder(Embeddings):
        def __init__(self, model_path: str, tokenizer_path: str):
            self.session = ort.InferenceSession(model_path)
            self.tokenizer = Tokenizer.from_file(tokenizer_path)

        def _embed(self, text: str):
            encoding = self.tokenizer.encode(text)
            input_ids = np.array([encoding.ids], dtype=np.int64)
            attention_mask = np.array([[1] * len(encoding.ids)], dtype=np.int64)

            outputs = self.session.run(
                None, {"input_ids": input_ids, "attention_mask": attention_mask}
            )

            raw_vector = outputs[0][0]
            norm_vector = raw_vector / (np.linalg.norm(raw_vector) + 1e-10)
            return norm_vector.tolist()

        def embed_documents(self, texts: list[str]) -> list[list[float]]:
            return [self._embed(text) for text in texts]

        def embed_query(self, text: str) -> list[float]:
            return self._embed(text)

    model_base_path = Path("models")

    embedding = OnnxEmbedder(
        model_path=str(model_base_path / "kr_sbert_mean_onnx/kr_sbert.onnx"),
        tokenizer_path=str(model_base_path / "kr_sbert_mean_onnx/tokenizer.json"),
    )

    db_base_path = Path("db")

    vectordb = Chroma(
        persist_directory=str(db_base_path / "chroma_store"),
        embedding_function=embedding,
    )

    return vectordb


def get_lda_model():
    """
    LDA ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    model_base_path = Path("models")

    count_vectorizer = joblib.load(str(model_base_path / "count_vectorizer.pkl"))
    lda_model = joblib.load(str(model_base_path / "best_lda_model.pkl"))

    db_base_path = Path("db")

    with open(str(db_base_path / "stopwords-ko.txt"), "r", encoding="utf-8") as f:
        stopwords = [word.strip() for word in f.readlines()]

    return lda_model, count_vectorizer, stopwords


def get_prediction_models():
    """
    ì˜ˆì¸¡ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”©
    """
    model_base_path = Path("models/saved_models")

    # ONNX ì¶”ë¡  ì„¸ì…˜ ìƒì„±
    sess = ort.InferenceSession(
        str(model_base_path / "predictor.onnx"), providers=["CPUExecutionProvider"]
    )

    # íƒ€ê²Ÿ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    target_scaler = joblib.load(str(model_base_path / "target_scaler.joblib"))

    # ê·¸ë£¹ë³„ ìŠ¤ì¼€ì¼ëŸ¬ ë™ì  ë¡œë”©
    fitted_scalers = {
        i: joblib.load(str(model_base_path / f"scaler_group_{i}.joblib"))
        for i in range(9)
    }

    return sess, target_scaler, fitted_scalers


class NewsTossChatbot:
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY í•„ìš”)
        self.client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def get_client(self):
        return self.client

    def search_similar_news(self, query_text, top_k=2):
        # Step 1: ì²« ë²ˆì§¸ APIë¡œ query_text ê¸°ë°˜ ê°€ì¥ ìœ ì‚¬í•œ ë‰´ìŠ¤ 1ê°œ ì°¾ê¸°
        first_url = "http://15.165.211.100:8000/news/similar"
        response = requests.post(first_url, json={"article": query_text, "top_k": 1})
        response.raise_for_status()
        top_news = response.json()["similar_news_list"][0]
        news_id = top_news["news_id"]

        # Step 2: í•´ë‹¹ news_idë¥¼ ë‘ ë²ˆì§¸ APIì— ë„£ì–´ì„œ ìœ ì‚¬ ë‰´ìŠ¤ top_kê°œ ê°€ì ¸ì˜¤ê¸°
        second_url = f"http://3.37.207.16:8000/news/v2/{news_id}/similar?top_n=2"
        response = requests.get(second_url)
        response.raise_for_status()
        similar_news = response.json()

        return similar_news


    def build_prompt(self, context, question, has_news=True):
        if has_news:
            system_prompt = """ë‹¹ì‹ ì€ ê³¼ê±° ë‰´ìŠ¤ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œë§Œ ë‹µë³€í•˜ëŠ” ì „ë¬¸ AI ì±—ë´‡, 'ë‰´ìŠ¤í† ìŠ¤'ì…ë‹ˆë‹¤.
                    - "ë„ˆ ëˆ„êµ¬ì•¼?"ë¼ê³  ë¬»ëŠ”ë‹¤ë©´: "ì €ëŠ” ê³¼ê±° ë‰´ìŠ¤ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦¬ëŠ” ì±—ë´‡ ë‰´ìŠ¤í† ìŠ¤ì…ë‹ˆë‹¤.ğŸ˜„" ë¼ê³  ëŒ€ë‹µí•˜ì„¸ìš”.
                    - ì•„ë˜ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ë”°ë¥´ë©°, **ì ˆëŒ€ ì¢…í•© ì •ë³´ë‚˜ ì¶”ê°€ í•´ì„ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**

                    ## [ë‹µë³€ ì‘ì„± ì§€ì¹¨]

                    1. ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ **ë‰´ìŠ¤ ì¹´ë“œ í˜•íƒœ**ë¡œ ìœ ì‚¬ ë‰´ìŠ¤ ì •ë³´ë¥¼ ìš”ì•½í•´ ì œì‹œí•˜ì„¸ìš” (ìˆœì„œ ê³ ì •):
                        - ì´ë¯¸ì§€ URLì˜ `type=w800`ì„ `type=w200`ìœ¼ë¡œ ë°˜ë“œì‹œ ìˆ˜ì •í•˜ì„¸ìš”.
                        - ì¹´ë“œ ë‚´ìš©ì€ ë‹¤ìŒ ë§ˆí¬ë‹¤ìš´ ì˜ˆì‹œì²˜ëŸ¼ ì‘ì„±í•˜ì„¸ìš”:
                        ```markdown
                        <img src="https://imgnews.pstatic.net/image/008/2024/11/28/0005120417_001_20241128085813446.jpg?type=w200" alt="ë‰´ìŠ¤ ì´ë¯¸ì§€">
                        â–¶ **ì œëª©**: <a href="https://n.news.naver.com/mnews/article/008/0005120417" target="_blank">SKí•˜ì´ë‹‰ìŠ¤, ì‹ ê·œ ì£¼ì£¼í™˜ì›ì±…ìœ¼ë¡œ ì¬ë¬´êµ¬ì¡° ê°œì„  ê¸°ëŒ€</a><br>  
                        â–¶ **ìœ ì‚¬ë„**: 0.56   
                        â–¶ **ë‚ ì§œ**: 2024-11-28  
                        â–¶ **ìš”ì•½**: NHíˆ¬ìì¦ê¶Œì´ ì‹ ê·œ ì£¼ì£¼í™˜ì› ì •ì±…ì„ ê³µì‹œí•œ SKí•˜ì´ë‹‰ìŠ¤ì— ëŒ€í•´...

                        ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ ì£¼ì„¸ìš”ğŸ˜‰
                        ```

                    2. ë‰´ìŠ¤ ì¹´ë“œ ì™¸ì˜ í•´ì„, ì˜ˆì¸¡, ì¢…í•©ì  ì˜ê²¬ì€ **ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**

                    3. ë‹µë³€ ë§ˆì§€ë§‰ì— ë‹¤ìŒ ë¬¸ì¥ì„ ë°˜ë“œì‹œ ì¶”ê°€í•˜ê³ , **ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì§ˆë¬¸ì„ ë§ˆí¬ë‹¤ìš´ ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸ë¡œ 2~3ê°œ ìƒì„±**í•˜ì„¸ìš”:

                    **ì•„ë˜ì™€ ê°™ì€ ì§ˆë¬¸ë„ í•¨ê»˜ ì°¸ê³ í•´ ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”.**
                    - ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì£¼ì œë¥¼ ìœ ì§€í•œ ì±„, ì±—ë´‡ì´ ì˜ ëŒ€ë‹µí•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
                    - ì˜ë¬¸ë¬¸ êµ¬ì¡°ë¥¼ ë”°ë¼ì•¼ í•˜ë©°, ì£¼ì œë¥¼ ë²—ì–´ë‚˜ì§€ ë§ˆì„¸ìš”.
                    """

            user_prompt = f"""## [ì œê³µëœ ìœ ì‚¬ ë‰´ìŠ¤ ì¹´ë“œ]
                    {context}

                    ## [ì‚¬ìš©ì ì§ˆë¬¸]
                    {question}"""

            return [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                    ]

        else:
            system_prompt = """ë‹¹ì‹ ì€ ê³¼ê±° ë‰´ìŠ¤ ê¸°ë°˜ AI ì±—ë´‡ì…ë‹ˆë‹¤.  
                    ìœ ì‚¬ ë‰´ìŠ¤ê°€ ì—†ì„ ê²½ìš°, ì•„ë˜ 3ê°€ì§€ ì˜ˆì‹œ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•´ ì •ì¤‘íˆ ì•ˆë‚´í•˜ì„¸ìš”.  
                    **ì ˆëŒ€ ì¢…í•© ì •ë³´, ë°°ê²½ ì„¤ëª…, ê°œì¸ ì˜ê²¬ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.**

                    - "ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´, ìµœì‹  ì´ìŠˆì™€ ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•´ë“œë¦´ê²Œìš”!"
                    - "ì§ì ‘ì ì¸ ì—°ê´€ ì‚¬ë¡€ë¥¼ í™•ì¸í•˜ë ¤ë©´, ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”. â˜ºï¸"
                    - "ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´, ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
                """
            user_prompt = f"""## [ì‚¬ìš©ì ì§ˆë¬¸]{question}"""

            return [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]


    def make_stream_prompt(self, question, top_k=2):
        similar_news = self.search_similar_news(question, top_k=top_k)
        filtered_news = [row for row in similar_news if row.get('similarity', 0) >= 0.1]
        retrieved_infos = []
        for row in filtered_news:
            info = (
                f"{row['title']} ({row['url']})\n"
                f"<img src=\"{row['image']}\" alt=\"ë‰´ìŠ¤ ì´ë¯¸ì§€\">\n"
                f"{row['summary']}\n"
                f"{row['wdate'][:10]}\n"
                f"(ìœ ì‚¬ë„: {row.get('similarity', 0):.2f})"
            )
            retrieved_infos.append(info)

        context = "\n\n".join(retrieved_infos)
        return self.build_prompt(context, question, has_news=bool(filtered_news))

    def answer(self, question, top_k=2):
        messages = self.make_stream_prompt(question, top_k)

        response = self.client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=messages,
            temperature=0.4,
            max_tokens=1024,
        )

        return response.choices[0].message.content


# ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”: ê° íŒŒì¼ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì½ì–´ ë“¤ì—¬ì„œ, ìŠ¤ì¼€ì¼ë§ ì‹œ ë³€ìˆ˜ëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ êµ¬ì„±
def load_scalers_by_group(folder_path):
    scalers = {}

    for filename in os.listdir(folder_path):
        if filename.endswith(".joblib"):
            key = filename.replace(".joblib", "")
            full_path = os.path.join(folder_path, filename)
            obj = joblib.load(full_path)

            # ë²„ì „ ì •ë³´ í¬í•¨ëœ dictì¼ ê²½ìš° ëŒ€ì‘
            if isinstance(obj, dict) and "scaler" in obj:
                scalers[key] = obj["scaler"]
            else:
                scalers[key] = obj

    return scalers


# ê³¼ê±° ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ì„ ìœ„í•œ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜
def get_similarity_model():
    model_dir = "models/"
    scaler_dir = os.path.join(model_dir, "scalers_grouped")
    ae_path = os.path.join(model_dir, "ae_encoder.onnx")
    regressor_path = os.path.join(model_dir, "regressor_model.onnx")

    # ONNX ëª¨ë¸ ë¡œë”©
    ae_sess = ort.InferenceSession(ae_path)
    regressor_sess = ort.InferenceSession(regressor_path)

    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”©
    scalers = load_scalers_by_group(scaler_dir)

    return scalers, ae_sess, regressor_sess


def get_recommend_model():
    model_base_path = Path("models")

    # ONNX ì„¸ì…˜ ìƒì„±
    model_recommend = ort.InferenceSession(
        str(model_base_path / "two_tower_model.onnx")
    )

    return model_recommend
