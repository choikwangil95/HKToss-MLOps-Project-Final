from tokenizers import Tokenizer
from pathlib import Path
import onnxruntime as ort
from langchain_chroma import Chroma
from langchain.embeddings.base import Embeddings
import numpy as np
import joblib
import os
import requests
import openai
from dotenv import load_dotenv
import pickle
import re

load_dotenv()


def get_summarize_model():
    """
    ONNX ìš”ì•½ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    base_path = Path("models/kobart_summary_onnx")

    encoder_sess = ort.InferenceSession(str(base_path / "encoder_model.onnx"))
    decoder_sess = ort.InferenceSession(str(base_path / "decoder_model.onnx"))
    tokenizer = Tokenizer.from_file(str(base_path / "tokenizer.json"))

    return encoder_sess, decoder_sess, tokenizer


def get_ner_tokenizer():
    """
    ONNX NER ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    base_path = Path("models/ner_onnx")

    tokenizer = Tokenizer.from_file(str(base_path / "tokenizer.json"))
    session = ort.InferenceSession(str(base_path / "model.onnx"))

    return tokenizer, session


def get_embedding_tokenizer():
    """
    ONNX NER ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    base_path = Path("models/kr_sbert_mean_onnx")

    tokenizer = Tokenizer.from_file(str(base_path / "tokenizer.json"))
    session = ort.InferenceSession(str(base_path / "kr_sbert.onnx"))

    return tokenizer, session


def get_vectordb():
    """
    vectordb ë¡œë”©
    """

    class OnnxEmbedder(Embeddings):
        def __init__(self, model_path: str, tokenizer_path: str):
            self.session = ort.InferenceSession(model_path)
            self.tokenizer = Tokenizer.from_file(tokenizer_path)

        def _embed(self, text: str):
            encoding = self.tokenizer.encode(text)
            input_ids = np.array([encoding.ids], dtype=np.int64)
            attention_mask = np.array([[1] * len(encoding.ids)], dtype=np.int64)

            outputs = self.session.run(
                None, {"input_ids": input_ids, "attention_mask": attention_mask}
            )

            raw_vector = outputs[0][0]
            norm_vector = raw_vector / (np.linalg.norm(raw_vector) + 1e-10)
            return norm_vector.tolist()

        def embed_documents(self, texts: list[str]) -> list[list[float]]:
            return [self._embed(text) for text in texts]

        def embed_query(self, text: str) -> list[float]:
            return self._embed(text)

    model_base_path = Path("models")

    embedding = OnnxEmbedder(
        model_path=str(model_base_path / "kr_sbert_mean_onnx/kr_sbert.onnx"),
        tokenizer_path=str(model_base_path / "kr_sbert_mean_onnx/tokenizer.json"),
    )

    db_base_path = Path("db")

    vectordb = Chroma(
        persist_directory=str(db_base_path / "chroma_store"),
        embedding_function=embedding,
    )

    return vectordb


def get_lda_model():
    """
    LDA ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
    """
    model_base_path = Path("models")

    count_vectorizer = joblib.load(str(model_base_path / "count_vectorizer.pkl"))
    lda_model = joblib.load(str(model_base_path / "best_lda_model.pkl"))

    db_base_path = Path("db")

    with open(str(db_base_path / "stopwords-ko.txt"), "r", encoding="utf-8") as f:
        stopwords = [word.strip() for word in f.readlines()]

    return lda_model, count_vectorizer, stopwords


def get_prediction_models():
    """
    ì˜ˆì¸¡ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”©
    """
    model_base_path = Path("models/saved_models")

    # ONNX ì¶”ë¡  ì„¸ì…˜ ìƒì„±
    sess = ort.InferenceSession(
        str(model_base_path / "predictor.onnx"), providers=["CPUExecutionProvider"]
    )

    # íƒ€ê²Ÿ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    target_scaler = joblib.load(str(model_base_path / "target_scaler.joblib"))

    # ê·¸ë£¹ë³„ ìŠ¤ì¼€ì¼ëŸ¬ ë™ì  ë¡œë”©
    fitted_scalers = {
        i: joblib.load(str(model_base_path / f"scaler_group_{i}.joblib"))
        for i in range(9)
    }

    return sess, target_scaler, fitted_scalers


class NewsTossChatbot:
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY í•„ìš”)
        self.client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def get_client(self):
        return self.client

    def search_similar_news(self, query_text, top_k=2):
        # Step 1: ì²« ë²ˆì§¸ APIë¡œ query_text ê¸°ë°˜ ê°€ì¥ ìœ ì‚¬í•œ ë‰´ìŠ¤ 1ê°œ ì°¾ê¸°
        first_url = "http://15.165.211.100:8000/news/similar"
        response = requests.post(first_url, json={"article": query_text, "top_k": 1})
        response.raise_for_status()
        top_news = response.json()["similar_news_list"][0]
        news_id = top_news["news_id"]

        # Step 2: í•´ë‹¹ news_idë¥¼ ë‘ ë²ˆì§¸ APIì— ë„£ì–´ì„œ ìœ ì‚¬ ë‰´ìŠ¤ top_kê°œ ê°€ì ¸ì˜¤ê¸°
        second_url = f"http://3.37.207.16:8000/news/v2/{news_id}/similar?top_n=2"
        response = requests.get(second_url)
        response.raise_for_status()
        similar_news = response.json()

        return similar_news

    def build_prompt(self, context, question, has_news=True):
        if has_news:
            system_prompt = """
                ë‹¹ì‹ ì€ ê³¼ê±° ë‰´ìŠ¤ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œë§Œ ë‹µë³€í•˜ëŠ” ì „ë¬¸ AI ì±—ë´‡, 'ë‰´ìŠ¤í† ìŠ¤'ì…ë‹ˆë‹¤.  
                ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ ê¸°ì¤€ì— ë”°ë¼ ì‘ì„±ë˜ì–´ì•¼ í•˜ë©°, ë‹¤ìŒ ì‚¬í•­ì„ ì² ì €íˆ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

                ---

                **ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ì•„ì•¼ í•  ë‚´ìš©:**

                - ì¢…í•© ì •ë³´  
                - ë°°ê²½ ì„¤ëª…  
                - ê°œì¸ ì˜ê²¬  
                - ë¯¸ë˜ ì˜ˆì¸¡  
                - ì¶”ê°€ í•´ì„  

                ---

                ## [ì§ˆë¬¸ ë¶„ë¥˜ ê¸°ì¤€]

                ### 1. ì •ì²´ì„± ê´€ë ¨ ì§ˆë¬¸
                - ì˜ˆì‹œ: "ë„ˆ ëˆ„êµ¬ì•¼", "ì •ì²´ê°€ ë­ì•¼", "ë‹ˆ ì—­í• ì€ ë­ì•¼", "ë„ˆ ë­í•˜ëŠ” ì• ì•¼"
                - **ì´ ê²½ìš°, ìœ ì‚¬ ë‰´ìŠ¤ ì¹´ë“œ ì—†ì´ ì•„ë˜ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”.**

                ì €ëŠ” ê³¼ê±° ë‰´ìŠ¤ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦¬ëŠ” AI ì±—ë´‡ ë‰´ìŠ¤í† ìŠ¤ì…ë‹ˆë‹¤. ğŸ˜„

                ---

                ### 2. ê²½ì œÂ·ì‚°ì—…Â·ì£¼ì‹Â·ì •ì±… ë“± ì‹œì‚¬ ì´ìŠˆ ì§ˆë¬¸
                - ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë‰´ìŠ¤ ì¹´ë“œ HTMLì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

                ---

                ## [ë‰´ìŠ¤ ì¹´ë“œ ì¶œë ¥ í˜•ì‹]

                ** ì£¼ì˜: ì½”ë“œë¸”ë¡(```html ... ```) ì‚¬ìš© ì ˆëŒ€ ê¸ˆì§€!  
                HTML íƒœê·¸ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤ì œ ë Œë”ë§ë˜ë„ë¡ í•´ì•¼ í•¨.**

                ### HTML ì¶œë ¥ ì˜ˆì‹œ:

                <div style="margin-bottom: 24px;">
                <h3 style="margin: 0 0 8px 0;">
                    <a href="https://n.news.naver.com/mnews/article/015/0005063326" target="_blank" style="text-decoration: none; color: #0070f3;">
                    í•˜ì´ë¸Œ ìƒì¥ ë•Œ 4000ì–µ ë”°ë¡œ ì±™ê¸´ ë°©ì‹œí˜â€¦ë‹¹êµ­, ì œì¬ ì—¬ë¶€ ê²€í† 
                    </a>
                </h3>

                <img src="https://imgnews.pstatic.net/image/015/2024/11/29/0005063326_001_20241129155613852.jpg?type=w200"
                    alt="ë‰´ìŠ¤ ì´ë¯¸ì§€"
                    style="width: 200px; border-radius: 8px; margin-bottom: 12px;">

                <ul style="margin: 0 0 12px 20px; padding: 0;">
                    <li><strong>ìœ ì‚¬ë„</strong>: 0.58</li>
                    <li><strong>ë‚ ì§œ</strong>: 2024-11-29</li>
                    <li><strong>ìš”ì•½</strong>: ë°©ì‹œí˜ í•˜ì´ë¸Œ ì˜ì¥ì€ 2020ë…„ í•˜ì´ë¸Œ ìƒì¥ ì „ ìŠ¤í‹±ì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸ ë“±ê³¼ ì£¼ì£¼ ê°„ ê³„ì•½ì„ ë§ºê³ ...</li>
                </ul>

                <p style="margin-top: 12px;">ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ ì£¼ì„¸ìš” ğŸ˜‰</p>
                </div>

                ---

                ## [ìœ ì‚¬ ì§ˆë¬¸ ì¶”ì²œ]

                ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ **ì˜ë¬¸ë¬¸ í˜•íƒœì˜ ìœ ì‚¬ ì§ˆë¬¸** 2~3ê°œë¥¼ ì¶œë ¥í•˜ì„¸ìš”.  
                **ë§ˆí¬ë‹¤ìš´ ëŒ€ì‹  ì•„ë˜ HTML êµ¬ì¡°ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.**

                <h3>ì•„ë˜ì™€ ê°™ì€ ì§ˆë¬¸ë„ í•¨ê»˜ ì°¸ê³ í•´ë³´ì„¸ìš”</h3>
                <ul>
                    <li>í•˜ì´ë¸Œ ìƒì¥ ë‹¹ì‹œ ë°©ì‹œí˜ ì˜ì¥ì˜ ê³„ì•½ ë‚´ìš©ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?</li>
                    <li>ê³¼ê±° IPO ì£¼ê´€ì‚¬ ì„ ì • ê³¼ì •ì—ì„œ ì–´ë–¤ ì´ìŠˆë“¤ì´ ìˆì—ˆë‚˜ìš”?</li>
                    <li>IPO ì‹¤íŒ¨ ì‹œ ì§€ë¶„ ë°˜í™˜ ì¡°ê±´ì´ ì ìš©ëœ ì‚¬ë¡€ê°€ ìˆë‚˜ìš”?</li>
                </ul>
            """

            user_prompt = f"""## [ì œê³µëœ ìœ ì‚¬ ë‰´ìŠ¤ ì¹´ë“œ]
                    {context}

                    ## [ì‚¬ìš©ì ì§ˆë¬¸]
                    {question}"""

            return [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]

        else:
            system_prompt = """ë‹¹ì‹ ì€ ê³¼ê±° ë‰´ìŠ¤ ê¸°ë°˜ AI ì±—ë´‡ì…ë‹ˆë‹¤.  
                    ìœ ì‚¬ ë‰´ìŠ¤ê°€ ì—†ì„ ê²½ìš°, ì•„ë˜ 3ê°€ì§€ ì˜ˆì‹œ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•´ ì •ì¤‘íˆ ì•ˆë‚´í•˜ì„¸ìš”.  
                    **ì ˆëŒ€ ì¢…í•© ì •ë³´, ë°°ê²½ ì„¤ëª…, ê°œì¸ ì˜ê²¬ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.**

                    - "ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´, ìµœì‹  ì´ìŠˆì™€ ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•´ë“œë¦´ê²Œìš”!"
                    - "ì§ì ‘ì ì¸ ì—°ê´€ ì‚¬ë¡€ë¥¼ í™•ì¸í•˜ë ¤ë©´, ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”. â˜ºï¸"
                    - "ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´, ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
                """
            user_prompt = f"""## [ì‚¬ìš©ì ì§ˆë¬¸]{question}"""

            return [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]

    def make_stream_prompt(self, question, top_k=2):
        similar_news = self.search_similar_news(question, top_k=top_k)
        filtered_news = [row for row in similar_news if row.get("similarity", 0) >= 0.1]
        retrieved_infos = []
        for row in filtered_news:
            info = (
                f"{row['title']} ({row['url']})\n"
                f"<img src=\"{row['image']}\" alt=\"ë‰´ìŠ¤ ì´ë¯¸ì§€\">\n"
                f"{row['summary']}\n"
                f"{row['wdate'][:10]}\n"
                f"(ìœ ì‚¬ë„: {row.get('similarity', 0):.2f})"
            )
            retrieved_infos.append(info)

        context = "\n\n".join(retrieved_infos)
        return self.build_prompt(context, question, has_news=bool(filtered_news))

    def answer(self, question, top_k=2):
        messages = self.make_stream_prompt(question, top_k)

        response = self.client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=messages,
            temperature=0.4,
            max_tokens=1024,
        )

        return response.choices[0].message.content


# ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”: ê° íŒŒì¼ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì½ì–´ ë“¤ì—¬ì„œ, ìŠ¤ì¼€ì¼ë§ ì‹œ ë³€ìˆ˜ëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ êµ¬ì„±
def load_scalers_by_group(folder_path):
    scalers = {}

    for filename in os.listdir(folder_path):
        if filename.endswith(".joblib"):
            key = filename.replace(".joblib", "")
            full_path = os.path.join(folder_path, filename)
            obj = joblib.load(full_path)

            # ë²„ì „ ì •ë³´ í¬í•¨ëœ dictì¼ ê²½ìš° ëŒ€ì‘
            if isinstance(obj, dict) and "scaler" in obj:
                scalers[key] = obj["scaler"]
            else:
                scalers[key] = obj

    return scalers


# ê³¼ê±° ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ì„ ìœ„í•œ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜
def get_similarity_model():
    model_dir = "models/"
    scaler_dir = os.path.join(model_dir, "scalers_grouped")
    ae_path = os.path.join(model_dir, "ae_encoder.onnx")
    regressor_path = os.path.join(model_dir, "similarity_ranker.onnx")

    # ONNX ëª¨ë¸ ë¡œë”©
    ae_sess = ort.InferenceSession(ae_path)
    regressor_sess = ort.InferenceSession(regressor_path)

    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”©
    scalers = load_scalers_by_group(scaler_dir)

    return scalers, ae_sess, regressor_sess


def get_recommend_model():
    model_base_path = Path("models")

    # ONNX ì„¸ì…˜ ìƒì„±
    model_recommend = ort.InferenceSession(
        str(model_base_path / "two_tower_model.onnx")
    )

    return model_recommend


def get_recommend_ranker_model():
    model_base_path = Path("models")

    # ONNX ì„¸ì…˜ ìƒì„±
    model_recommend_ranker = joblib.load(str(model_base_path / "lgbm_model2.pkl"))

    return model_recommend_ranker
